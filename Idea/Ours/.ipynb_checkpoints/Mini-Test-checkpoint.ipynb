{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9611f9df-cf11-4016-ac34-b6af20367c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() )\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL\")\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Dataset\")\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Idea\")\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Network\")\n",
    "from Network.utils_network import init_nets\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from Dataset.init_dataset import Cifar10FL,Cifar100FL,FashionMNISTData,MNISTData,USPSTData,SVHNData\n",
    "#from Dataset.utils_dataset import init_logs, get_dataloader,generate_public_data_idxs\n",
    "#from Idea.utils_idea import update_model_via_private_data_with_two_model,evaluate_network,mkdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fd8e64-b8a7-44d2-a8eb-115627a728c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global Parameters\n",
    "'''\n",
    "Method_Name = 'Ours'\n",
    "Ablation_Name='FCCL'\n",
    "\n",
    "\n",
    "Private_Dataset_Name_List=['mnist', 'usps', 'svhn', 'syn']\n",
    "Private_Data_Total_Len_List=[60000, 7291, 73257, 10000]\n",
    "Private_Data_Len_List=[150, 80, 5000, 1800]\n",
    "Private_Training_Epoch=[40,35,3,4]\n",
    "Private_Dataset_Classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "Private_Net_Feature_Dim_List=[512,512,320,1280]\n",
    "TrainBatchSize=512\n",
    "Local_TrainBatchSize=256\n",
    "TestBatchSize=512\n",
    "Public_Training_Epoch=1\n",
    "Public_Dataset_Length=5000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Temperature = 1\n",
    "Scenario = \"Digits\"\n",
    "Seed = 42\n",
    "N_Participants = 4\n",
    "CommunicationEpoch = 40\n",
    "TrainBatchSize = 512\n",
    "TestBatchSize = 512\n",
    "Dataset_Dir = ''\n",
    "Project_Dir = ''\n",
    "Idea_Ours_Dir = Project_Dir + 'Idea/Ours/'\n",
    "Private_Net_Name_List = ['ResNet10', 'ResNet12', 'Efficientnet', 'Mobilenetv2']\n",
    "Pariticpant_Params = {\n",
    "    'loss_funnction' : 'KLDivLoss',\n",
    "    'optimizer_name' : 'Adam',\n",
    "    'learning_rate'  : 0.001\n",
    "}\n",
    "\n",
    "'''\n",
    "Scenario for large domain gap\n",
    "'''\n",
    "Private_Training_Epoch = [40,35,3,4]\n",
    "Private_Dataset_Classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "Output_Channel = len(Private_Dataset_Classes)\n",
    "'''\n",
    "Public data parameters\n",
    "'''\n",
    "\n",
    "#['mnist', 'usps', 'svhn', 'syn']\n",
    "Public_Dataset_Name = \"cifar_100\"\n",
    "Public_Dataset_Length = 5000\n",
    "Public_Dataset_Dir = Dataset_Dir+Public_Dataset_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a39c031-55cd-40bb-b8c8-21a9df2bdaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "Project_Path = Project_Dir\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "def init_logs(log_level=logging.INFO,log_path = Project_Path+'Logs/',sub_name=None):\n",
    "    # logging：https://www.cnblogs.com/CJOKER/p/8295272.html\n",
    "    # 第一步，创建一个logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(log_level)  # Log等级总开关\n",
    "    # 第二步，创建一个handler，用于写入日志文件\n",
    "    log_path = log_path\n",
    "    mkdirs(log_path)\n",
    "    filename = os.path.basename(sys.argv[0][0:-3])\n",
    "    if sub_name == None:\n",
    "        log_name = log_path + filename + '.log'\n",
    "    else:\n",
    "        log_name = log_path + filename + '_' + sub_name +'.log'\n",
    "    logfile = log_name\n",
    "    fh = logging.FileHandler(logfile, mode='w')\n",
    "    fh.setLevel(log_level)  # 输出到file的log等级的开关\n",
    "    # 第三步，定义handler的输出格式\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "    console  = logging.StreamHandler()\n",
    "    console.setLevel(log_level)\n",
    "    console.setFormatter(formatter)\n",
    "    # 第四步，将logger添加到handler里面\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(console)\n",
    "    # 日志\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48e47ab-ee1c-4fd1-849b-9a10534fbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdirs(dirpath):\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except Exception as _:\n",
    "        pass\n",
    "\n",
    "def load_cifar10_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    cifar10_train_ds = Cifar10FL(datadir, train=True, download=False, transform=transform)\n",
    "    cifar10_test_ds = Cifar10FL(datadir, train=False, download=False, transform=transform)\n",
    "    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n",
    "    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_cifar100_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    cifar100_train_ds = Cifar100FL(datadir, train=True, download=True, transform=transform)\n",
    "    cifar100_test_ds = Cifar100FL(datadir, train=False, download=True, transform=transform)\n",
    "    X_train, y_train = cifar100_train_ds.data, cifar100_train_ds.target\n",
    "    X_test, y_test = cifar100_train_ds.data, cifar100_test_ds.target\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def load_fashionmnist_data(datadir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    fashionmnist_train_ds = FashionMNISTData(datadir, train=True, download=False, transform=transform)\n",
    "    fashionmnist_test_ds = FashionMNISTData(datadir, train=False, download=False, transform=transform)\n",
    "    X_train, y_train = fashionmnist_train_ds.data, fashionmnist_train_ds.target\n",
    "    X_test, y_test = fashionmnist_test_ds.data, fashionmnist_test_ds.target\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def generate_public_data_idxs(dataset,datadir,size,epoch=None):\n",
    "    if dataset =='cifar_100':\n",
    "        _, y_train, _, _ = load_cifar100_data(datadir)\n",
    "        n_train = y_train.shape[0]\n",
    "    if dataset =='tiny_imagenet':\n",
    "        train_ds = datasets.ImageFolder(datadir+'/train', transform=None)\n",
    "        n_train = len(train_ds)\n",
    "    if dataset =='FashionMNIST':\n",
    "        _, y_train, _, _ = load_fashionmnist_data(datadir)\n",
    "        n_train = y_train.shape[0]\n",
    "    idxs = np.random.permutation(n_train) # 打乱顺序\n",
    "    if epoch == None:\n",
    "        idxs = idxs[0:size] # 获取前size个\n",
    "        return idxs\n",
    "    else:\n",
    "        idx_list = []\n",
    "        for epoch_index in range(epoch):\n",
    "            idx_list.append(np.random.choice(idxs, size, replace=False))\n",
    "        return idx_list\n",
    "\n",
    "def get_dataloader(dataset, datadir, train_bs, test_bs, dataidxs=None, noise_level=0,num_workers=4):\n",
    "    if dataset in ('cifar_10', 'cifar_100'):\n",
    "        if dataset == 'cifar_10':\n",
    "            dl_obj = Cifar10FL\n",
    "            normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                             std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: F.pad(\n",
    "                    Variable(x.unsqueeze(0), requires_grad=False),\n",
    "                    (4, 4, 4, 4), mode='reflect').data.squeeze()),\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ColorJitter(brightness=noise_level),\n",
    "                transforms.RandomCrop(32),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            # data prep for test set\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "        if dataset =='cifar_100':\n",
    "            dl_obj=Cifar100FL\n",
    "            normalize = transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                             std=[0.2673342858792401, 0.2564384629170883, 0.27615047132568404])\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize])\n",
    "        train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
    "        test_ds = dl_obj(datadir, train=False, transform=transform_test, download=True)\n",
    "        train_dl = torch.utils.data.DataLoader(dataset=train_ds, drop_last=True, batch_size=train_bs, shuffle=True,num_workers=num_workers)\n",
    "        test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=test_bs, shuffle=False,num_workers=num_workers)\n",
    "        return train_dl, test_dl, train_ds, test_ds\n",
    "    if dataset == 'tiny_imagenet':\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "        if dataidxs is None:\n",
    "            train_ds = datasets.ImageFolder(datadir+'/train', transform=transform_train)\n",
    "        else:\n",
    "            train_ds = datasets.ImageFolder(datadir+'/train', transform=transform_train)\n",
    "            train_ds = torch.utils.data.Subset(train_ds, dataidxs)\n",
    "        test_ds = datasets.ImageFolder(datadir+'/test', transform=transform_test)\n",
    "        train_dl = torch.utils.data.DataLoader(dataset=train_ds, drop_last=True, batch_size=train_bs, shuffle=True,num_workers=num_workers)\n",
    "        test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=test_bs, shuffle=False,num_workers=num_workers)\n",
    "        return train_dl, test_dl, train_ds, test_ds\n",
    "    if dataset =='FashionMNIST':\n",
    "        dl_obj = FashionMNISTData\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
    "        test_ds = dl_obj(datadir, train=False, transform=transform_test, download=True)\n",
    "    if dataset == 'syn':\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        if dataidxs is None:\n",
    "            train_ds = datasets.ImageFolder(Dataset_Dir + 'syn/imgs_train', transform=transform_train)\n",
    "        else:\n",
    "            train_ds = datasets.ImageFolder(Dataset_Dir + 'syn/imgs_train', transform=transform_train)\n",
    "            train_ds = torch.utils.data.Subset(train_ds, dataidxs)\n",
    "        test_ds = datasets.ImageFolder(Dataset_Dir + 'syn/imgs_valid', transform=transform_test)\n",
    "    if dataset  == 'mnist':\n",
    "        dl_obj = MNISTData\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
    "        test_ds = dl_obj(datadir, train=False, transform=transform_test, download=True)\n",
    "    if dataset == 'usps':\n",
    "        dl_obj = USPSTData\n",
    "        normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            normalize\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            normalize])\n",
    "        train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
    "        test_ds = dl_obj(datadir, train=False, transform=transform_test, download=True)\n",
    "    if dataset =='svhn':\n",
    "        dl_obj = SVHNData\n",
    "        normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        train_ds = dl_obj(datadir, dataidxs=dataidxs, train='train', transform=transform_train, download=True)\n",
    "        test_ds = dl_obj(datadir, train='test', transform=transform_test, download=True)\n",
    "    if dataset in ('amazon', 'caltech','dslr','webcam'):\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        transform_traintest = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        ds = datasets.ImageFolder(Dataset_Dir + dataset+'/', transform=transform_traintest)\n",
    "        dataset_len = len(ds)\n",
    "        full_idx = range(dataset_len)\n",
    "        test_idx = []\n",
    "        for class_index in range(len(ds.classes)):\n",
    "            target_class_list = [index for (index,value) in enumerate(ds.targets) if value == class_index]\n",
    "            target_class_test_length = int(0.3*len(target_class_list))\n",
    "            test_idx.extend(np.random.permutation(target_class_list)[0:target_class_test_length])\n",
    "        if dataidxs is None:\n",
    "            train_idx = list(set(full_idx).difference(set(test_idx)))\n",
    "        else:\n",
    "            train_idx = list(set(full_idx).difference(set(test_idx)))[0:len(dataidxs)]\n",
    "        train_ds = torch.utils.data.Subset(ds,train_idx)\n",
    "        test_ds = torch.utils.data.Subset(ds,full_idx)\n",
    "    if dataset in ('Art', 'Clipart','Product','Real World'):\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        transform_traintest = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        ds = datasets.ImageFolder(Dataset_Dir + dataset+'/', transform=transform_traintest)\n",
    "        dataset_len = len(ds)\n",
    "        full_idx = range(dataset_len)\n",
    "        test_idx = []\n",
    "        for class_index in range(len(ds.classes)):\n",
    "            target_class_list = [index for (index,value) in enumerate(ds.targets) if value == class_index]\n",
    "            target_class_test_length = int(0.3*len(target_class_list))\n",
    "            test_idx.extend(np.random.permutation(target_class_list)[0:target_class_test_length])\n",
    "        if dataidxs is None:\n",
    "            train_idx = list(set(full_idx).difference(set(test_idx)))\n",
    "        else:\n",
    "            train_idx = list(set(full_idx).difference(set(test_idx)))[0:len(dataidxs)]\n",
    "        train_ds = torch.utils.data.Subset(ds,train_idx)\n",
    "        test_ds = torch.utils.data.Subset(ds,full_idx)\n",
    "    train_dl = torch.utils.data.DataLoader(dataset=train_ds,batch_size=train_bs,shuffle=True,num_workers=num_workers)\n",
    "    test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=test_bs, shuffle=False,num_workers=num_workers)\n",
    "    return train_dl, test_dl, train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f029266-ff3b-44e1-b0d9-9089a85c4712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:09,276 - 3847571766.py[line:2] - INFO: Method Name : Ours Ablation Name : FCCL\n",
      "2023-03-01 10:24:09,277 - 3847571766.py[line:3] - INFO: Random Seed and Server Config\n"
     ]
    }
   ],
   "source": [
    "logger = init_logs(sub_name=Ablation_Name)\n",
    "logger.info('Method Name : '+Method_Name + ' Ablation Name : '+Ablation_Name)\n",
    "logger.info(\"Random Seed and Server Config\")\n",
    "seed = Seed\n",
    "np.random.seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288b2fec-62b3-48e2-a347-43adb2a40893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:12,066 - 4165827351.py[line:1] - INFO: Initialize Participants' Data idxs and Model\n",
      "2023-03-01 10:24:12,069 - 4165827351.py[line:8] - INFO: {'mnist': array([12628, 37730, 39991,  8525,  8279, 51012, 14871, 15127,  9366,\n",
      "       33322, 53390, 21819,  5026, 23428, 45297, 26354, 30195, 47038,\n",
      "       20731, 34047, 26064, 42469, 29746, 14522, 31572, 54949, 19368,\n",
      "        3803, 53325, 14300, 51301,  9008, 47521, 25224, 48921, 37978,\n",
      "       44171, 26303, 19458,  5369, 50291, 25951, 54908, 56362, 32218,\n",
      "        2885, 36559,  8966, 46574, 10530, 44628,   273, 19269, 36911,\n",
      "       10121, 13290, 57606, 47189, 29209, 42187, 25386, 17005, 10981,\n",
      "       47313, 27070,  6685, 54960, 58125, 40700, 13902, 31539, 49716,\n",
      "       49519, 51923,  3502, 39336,  2218, 18505, 10689, 21377,  1866,\n",
      "       20192, 28870, 52203,  3867,  3222, 21785, 20984, 48539, 40694,\n",
      "        8440,  9951,  1334, 32572, 28344, 46503, 34482, 36271, 36874,\n",
      "       11512, 23847,  9937, 52122,  9189, 55990, 11744, 34545, 13152,\n",
      "       13059, 49858, 52310,  8914, 42178,  9199, 37940, 54184, 48596,\n",
      "       58585,  8063, 21052, 30487,  7528,  9935, 31354, 48001, 49675,\n",
      "       49979, 34356, 38895, 46280,  9073, 24961,   476, 16358, 24839,\n",
      "       24915, 15359, 16242, 33821, 31935, 48297, 25311,  4402, 52611,\n",
      "       27692, 36474, 33603, 24219, 41145, 31168]), 'usps': array([1537, 1824, 3493, 1822, 6429,  956, 4053, 5297, 3337, 2869, 2879,\n",
      "        807, 6711, 3781, 4404, 2151, 5139, 3253, 2821, 4750, 1961, 5754,\n",
      "       2435, 4782, 2118,  372, 5312, 2661, 5918,  553, 1137,  218, 4825,\n",
      "       2130, 3266, 3246, 5012,  830, 6231, 2635, 1261, 6797, 5141, 7231,\n",
      "       4726,  777, 5634, 2259, 1995,  478, 2826, 4678, 2440,  939, 2319,\n",
      "       2883, 5827, 5734, 2983, 4202, 4546, 2320, 3973, 3441,   10, 6605,\n",
      "       2401, 1223, 1916, 6647, 6092, 5979, 4358, 3408, 2647, 5263, 4401,\n",
      "       6820, 6120, 1674]), 'svhn': array([23850, 68371, 58083, ..., 51621, 60914, 34123]), 'syn': array([1962, 1460, 2240, ..., 1855, 3468, 1604])}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initialize Participants' Data idxs and Model\")\n",
    "# For Digits scenario\n",
    "private_dataset_idxs_dict = {}\n",
    "for index in range(N_Participants):\n",
    "    idxes = np.random.permutation(Private_Data_Total_Len_List[index])\n",
    "    idxes = idxes[0:Private_Data_Len_List[index]]\n",
    "    private_dataset_idxs_dict[Private_Dataset_Name_List[index]]= idxes\n",
    "logger.info(private_dataset_idxs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c15a137-8ca3-4465-84f2-bd7baee653fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:15,621 - 3557118194.py[line:2] - INFO: Load Participants' Models\n"
     ]
    }
   ],
   "source": [
    "net_list = init_nets(n_parties=N_Participants,nets_name_list=Private_Net_Name_List,num_classes=Output_Channel)\n",
    "logger.info(\"Load Participants' Models\")\n",
    "for i in range(N_Participants):\n",
    "    network = net_list[i]\n",
    "    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "    network = network.to(device)\n",
    "    netname = Private_Net_Name_List[i]\n",
    "    private_dataset_name = Private_Dataset_Name_List[i]\n",
    "    private_model_path = Project_Dir + 'Network/Model_Storage/' + netname + '_' + str(i) + '_' + private_dataset_name + '.ckpt'\n",
    "    #network.load_state_dict(torch.load(private_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44e9696-c391-41f9-bd25-088f4a0b81a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:17,880 - 11831659.py[line:2] - INFO: Load Frozen Participants' Models\n"
     ]
    }
   ],
   "source": [
    "frozen_net_list = init_nets(n_parties=N_Participants,nets_name_list=Private_Net_Name_List,num_classes=Output_Channel)\n",
    "logger.info(\"Load Frozen Participants' Models\")\n",
    "for i in range(N_Participants):\n",
    "    network = frozen_net_list[i]\n",
    "    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "    network = network.to(device)\n",
    "    netname = Private_Net_Name_List[i]\n",
    "    private_dataset_name = Private_Dataset_Name_List[i]\n",
    "    private_model_path = Project_Dir + 'Network/Model_Storage/' + netname + '_' + str(i) + '_' + private_dataset_name + '.ckpt'\n",
    "    #network.load_state_dict(torch.load(private_model_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82b398c-75cc-4887-8e0f-896153568607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:22,378 - 3788695022.py[line:2] - INFO: Load Progressive Participants' Models\n"
     ]
    }
   ],
   "source": [
    "progressive_net_list = init_nets(n_parties=N_Participants,nets_name_list=Private_Net_Name_List,num_classes=Output_Channel)\n",
    "logger.info(\"Load Progressive Participants' Models\")\n",
    "for i in range(N_Participants):\n",
    "    network = progressive_net_list[i]\n",
    "    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "    network = network.to(device)\n",
    "    netname = Private_Net_Name_List[i]\n",
    "    private_dataset_name = Private_Dataset_Name_List[i]\n",
    "    private_model_path = Project_Dir + 'Network/Model_Storage/' + netname + '_' + str(i) + '_' + private_dataset_name + '.ckpt'\n",
    "    #network.load_state_dict(torch.load(private_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68aeb9a-fc90-40d6-af6a-af88f29ca00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:27,593 - 41792046.py[line:1] - INFO: Initialize Public Data Parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digitscifar_100\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initialize Public Data Parameters\")\n",
    "print(Scenario+Public_Dataset_Name)\n",
    "public_data_indexs = generate_public_data_idxs(dataset=Public_Dataset_Name,datadir=Public_Dataset_Dir,size=Public_Dataset_Length)\n",
    "\n",
    "public_train_dl, _, _, _ = get_dataloader(dataset=Public_Dataset_Name, datadir=Public_Dataset_Dir,\n",
    "                                                            train_bs=TrainBatchSize, test_bs=TestBatchSize,\n",
    "                                                            dataidxs=public_data_indexs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a7d460-781d-4ea6-a8bf-cefe0d7a4864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:24:35,066 - 4106949018.py[line:1] - INFO: Initialize Private Data Loader\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: svhn/train_32x32.mat\n",
      "Using downloaded and verified file: svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "logger.info('Initialize Private Data Loader')\n",
    "private_train_data_loader_list = []\n",
    "private_test_data_loader_list = []\n",
    "for participant_index in range(N_Participants):\n",
    "    private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "    private_dataidx = private_dataset_idxs_dict[private_dataset_name]\n",
    "    private_dataset_dir = Dataset_Dir + private_dataset_name\n",
    "    train_dl_local, test_dl_local, _, _ = get_dataloader(dataset=private_dataset_name,\n",
    "                                                 datadir=private_dataset_dir,\n",
    "                                                 train_bs=TrainBatchSize, test_bs=TestBatchSize,\n",
    "                                                 dataidxs=private_dataidx)\n",
    "    private_train_data_loader_list.append(train_dl_local)\n",
    "    private_test_data_loader_list.append(test_dl_local)\n",
    "\n",
    "col_loss_list = []\n",
    "local_loss_list = []\n",
    "acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262272e1-8fb9-4dbc-93af-84d414d6322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(CommunicationEpoch):\n",
    "\n",
    "        logger.info(\"The \"+str(epoch_index)+\" th Communication Epoch\")\n",
    "        logger.info('Evaluate Models')\n",
    "        acc_epoch_list = []\n",
    "        for participant_index in range(N_Participants):\n",
    "            netname = Private_Net_Name_List[participant_index]\n",
    "            private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "            private_dataset_dir = Dataset_Dir + private_dataset_name\n",
    "            print(netname + '_' + private_dataset_name + '_' + private_dataset_dir)\n",
    "            _, test_dl, _, _ = get_dataloader(dataset=private_dataset_name, datadir=private_dataset_dir,\n",
    "                                              train_bs=TrainBatchSize,\n",
    "                                              test_bs=TestBatchSize, dataidxs=None)\n",
    "            network = net_list[participant_index]\n",
    "            #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "            network = network.to(device)\n",
    "            acc_epoch_list.append(evaluate_network(network=network, dataloader=test_dl, logger=logger))\n",
    "        acc_list.append(acc_epoch_list)\n",
    "\n",
    "        a = datetime.now()\n",
    "        for _ in range(Public_Training_Epoch):\n",
    "            for batch_idx, (images, _) in enumerate(public_train_dl):\n",
    "                linear_output_list = []\n",
    "                linear_output_target_list = [] # Save other participants' linear output\n",
    "                linear_output_progressive_list = [] # Save itself progressive model's linear output\n",
    "                col_loss_batch_list = []\n",
    "                '''\n",
    "                Calculate Linear Output\n",
    "                '''\n",
    "                for participant_index in range(N_Participants):\n",
    "                    network = net_list[participant_index]\n",
    "                    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "                    network = network.to(device)\n",
    "                    network.train()\n",
    "                    images = images.to(device)\n",
    "                    linear_output = network(x=images)\n",
    "                    linear_output_target_list.append(linear_output.clone().detach())\n",
    "                    linear_output_list.append(linear_output)\n",
    "                '''\n",
    "                Calculate Progressive Linear Output\n",
    "                '''\n",
    "                for progressive_participant_index in range(N_Participants):\n",
    "                    progressive_network = progressive_net_list[progressive_participant_index]\n",
    "                    #progressive_network = nn.DataParallel(progressive_network,device_ids=device_ids).to(device)\n",
    "                    progressive_network = progressive_network.to(device)\n",
    "                    progressive_network.eval()\n",
    "                    with torch.no_grad():\n",
    "                        images = images.to(device)\n",
    "                        progressive_linear_output = progressive_network(images)\n",
    "                        linear_output_progressive_list.append(progressive_linear_output)\n",
    "                '''\n",
    "                Update Participants' Models via Col Loss\n",
    "                '''\n",
    "                for participant_index in range(N_Participants):\n",
    "                    '''\n",
    "                    Calculate the Loss with others\n",
    "                    '''\n",
    "                    network = net_list[participant_index]\n",
    "                    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "                    network = network.to(device)\n",
    "                    network.train()\n",
    "                    optimizer = optim.Adam(network.parameters(), lr=Pariticpant_Params['learning_rate'])\n",
    "                    optimizer.zero_grad()\n",
    "                    linear_output_target_avg_list = []\n",
    "                    for i in range(N_Participants):\n",
    "                        if i != participant_index:\n",
    "                            linear_output_target_avg_list.append(linear_output_target_list[i])\n",
    "                        if i ==participant_index:\n",
    "                            linear_output_target_avg_list.append(linear_output_progressive_list[i])\n",
    "\n",
    "                    linear_output_target_avg = torch.mean(torch.stack(linear_output_target_avg_list), 0)\n",
    "                    linear_output = linear_output_list[participant_index]\n",
    "                    z_1_bn = (linear_output-linear_output.mean(0))/linear_output.std(0)\n",
    "                    z_2_bn = (linear_output_target_avg-linear_output_target_avg.mean(0))/linear_output_target_avg.std(0)\n",
    "                    # empirical cross-correlation matrix\n",
    "                    c = z_1_bn.T @ z_2_bn\n",
    "                    # sum the cross-correlation matrix between all gpus\n",
    "                    c.div_(len(images))\n",
    "\n",
    "                    on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "                    off_diag = off_diagonal(c).add_(1).pow_(2).sum()\n",
    "                    col_loss = on_diag + 0.0051 * off_diag\n",
    "                    col_loss_batch_list.append(col_loss.item())\n",
    "                    col_loss.backward()\n",
    "                    optimizer.step()\n",
    "                col_loss_list.append(col_loss_batch_list)\n",
    "\n",
    "        '''\n",
    "        Update Participants' Models via Private Data\n",
    "        '''\n",
    "        local_loss_batch_list = []\n",
    "        for participant_index in range(N_Participants):\n",
    "            network = net_list[participant_index]\n",
    "            #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "            network = network.to(device)\n",
    "            network.train()\n",
    "\n",
    "            frozen_network = frozen_net_list[participant_index]\n",
    "            #frozen_network = nn.DataParallel(frozen_network,device_ids=device_ids).to(device)\n",
    "            frozen_network = frozen_network.to(device)\n",
    "            frozen_network.eval()\n",
    "\n",
    "            progressive_network = progressive_net_list[participant_index]\n",
    "            #progressive_network = nn.DataParallel(progressive_network,device_ids=device_ids).to(device)\n",
    "            progressive_network = progressive_network.to(device)\n",
    "            progressive_network.eval()\n",
    "\n",
    "            private_dataset_name=  Private_Dataset_Name_List[participant_index]\n",
    "            private_dataidx = private_dataset_idxs_dict[private_dataset_name]\n",
    "            private_dataset_dir = Dataset_Dir+private_dataset_name\n",
    "            train_dl_local, _, train_ds_local, _ = get_dataloader(dataset=private_dataset_name,\n",
    "                                                                  datadir=private_dataset_dir,\n",
    "                                                                  train_bs=TrainBatchSize, test_bs=TestBatchSize,\n",
    "                                                                  dataidxs=private_dataidx)\n",
    "\n",
    "            private_epoch = max(int(Public_Dataset_Length / len(train_ds_local)), 1)\n",
    "            private_epoch = Private_Training_Epoch[participant_index]\n",
    "\n",
    "            network, private_loss_batch_list = update_model_via_private_data_with_two_model(network=network,\n",
    "            frozen_network=frozen_network,progressive_network=progressive_network,\n",
    "            temperature = Temperature,private_epoch=private_epoch,private_dataloader=train_dl_local,\n",
    "            loss_function=Pariticpant_Params['loss_funnction'],optimizer_method=Pariticpant_Params['optimizer_name'],\n",
    "            learing_rate=Pariticpant_Params['learning_rate'],logger=logger)\n",
    "            mean_private_loss_batch = mean(private_loss_batch_list)\n",
    "            local_loss_batch_list.append(mean_private_loss_batch)\n",
    "        local_loss_list.append(local_loss_batch_list)\n",
    "\n",
    "        b = datetime.now()\n",
    "        temp = b-a\n",
    "        print(temp)\n",
    "        '''\n",
    "        用于迭代 Progressive 模型\n",
    "        '''\n",
    "        for j in range(N_Participants):\n",
    "            progressive_net_list[j] = copy.deepcopy(net_list[j])\n",
    "\n",
    "        if epoch_index ==CommunicationEpoch-1:\n",
    "            acc_epoch_list = []\n",
    "            logger.info('Final Evaluate Models')\n",
    "            for participant_index in range(N_Participants):\n",
    "                netname = Private_Net_Name_List[participant_index]\n",
    "                private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "                private_dataset_dir = Dataset_Dir + private_dataset_name\n",
    "                print(netname+'_'+private_dataset_name+'_'+private_dataset_dir)\n",
    "                _, test_dl, _, _ = get_dataloader(dataset=private_dataset_name, datadir=private_dataset_dir,\n",
    "                                                  train_bs=TrainBatchSize,\n",
    "                                                  test_bs=TestBatchSize, dataidxs=None)\n",
    "                \n",
    "                network = net_list[participant_index]\n",
    "                #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "                network = network.to(device)\n",
    "                acc_epoch_list.append(evaluate_network(network=network, dataloader=test_dl, logger=logger))\n",
    "            acc_list.append(acc_epoch_list)\n",
    "\n",
    "        if epoch_index % 5 == 3 or epoch_index == CommunicationEpoch - 1:\n",
    "            mkdirs(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario)\n",
    "            mkdirs(Idea_Ours_Dir + '/Model_Storage/' + Scenario)\n",
    "            mkdirs(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name)\n",
    "            mkdirs(Idea_Ours_Dir + '/Model_Storage/' + Scenario + '/' + Ablation_Name)\n",
    "            mkdirs(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name)\n",
    "            mkdirs(Idea_Ours_Dir + '/Model_Storage/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name)\n",
    "\n",
    "            logger.info('Save Loss')\n",
    "            col_loss_array = np.array(col_loss_list)\n",
    "            np.save(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                    + '/collaborative_loss.npy', col_loss_array)\n",
    "            local_loss_array = np.array(local_loss_list)\n",
    "            np.save(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                    + '/local_loss.npy', local_loss_array)\n",
    "            logger.info('Save Acc')\n",
    "            acc_array = np.array(acc_list)\n",
    "            np.save(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                    + '/acc.npy', acc_array)\n",
    "\n",
    "            logger.info('Save Models')\n",
    "            for participant_index in range(N_Participants):\n",
    "                netname = Private_Net_Name_List[participant_index]\n",
    "                private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "                network = net_list[participant_index]\n",
    "                #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "                network = network.to(device)\n",
    "                torch.save(network.state_dict(),\n",
    "                           Idea_Ours_Dir + '/Model_Storage/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                           + '/' + netname + '_' + str(participant_index) + '_' + private_dataset_name + '.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
