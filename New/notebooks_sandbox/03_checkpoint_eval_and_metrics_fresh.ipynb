{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Checkpoint Evaluation and Metrics Analysis\n",
    "\n",
    "This notebook performs comprehensive evaluation of all checkpoint models against ground truth weights from Merged zoo.csv.\n",
    "\n",
    "## Key Features:\n",
    "1. **Processes all 44 tracking CSV files** - each file contributes a row to results tables\n",
    "2. **Extracts experiment info** - epoch numbers from filenames, loss types from paths\n",
    "3. **Layer-wise analysis** - uses delimiters `[208, 1414, 1514, 2254, 2464]` for 5-layer segmentation\n",
    "4. **Four comprehensive tables**:\n",
    "   - Table 1: Intra metrics (PD vs GT, FN vs GT)\n",
    "   - Table 2: Inter metrics (PD vs FN)\n",
    "   - Table 3: Layer-wise intra metrics\n",
    "   - Table 4: Layer-wise inter metrics\n",
    "5. **Advanced metrics** - MSE, MAE, MAPE, JS Loss, KL Divergence, Wasserstein, Cosine Similarity, Pearson Correlation, AUTO loss, Latent loss\n",
    "6. **Enhanced visualizations** - heatmaps with experiment names and epochs\n",
    "7. **Export capabilities** - CSV and JSON formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Checkpoint Evaluation and Metrics Setup ===\n",
      "Project root: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New\n",
      "Data directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/data\n",
      "Experiments directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/Experiments\n",
      "Results directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results\n",
      "PyTorch version: 2.7.1+cu128\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.3\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "print(\"=== Enhanced Checkpoint Evaluation and Metrics Setup ===\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Set up paths\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "EXPERIMENTS_DIR = ROOT / \"Experiments\"\n",
    "RESULTS_DIR = ROOT / \"notebooks_sandbox\" / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Importing Transformer Architecture ===\n",
      "‚úÖ Successfully imported real TransformerAE architecture\n",
      "encoder droupout init 0.1\n",
      "encoder droupout init 0.1\n",
      "decoder droupout init 0.1\n",
      "‚úÖ Test model created with 12,634,684 parameters\n",
      "Transformer architecture ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Transformer Architecture\n",
    "print(\"=== Importing Transformer Architecture ===\")\n",
    "\n",
    "# Add the project root to the path\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Import the real transformer classes\n",
    "try:\n",
    "    from Double_input_transformer import (\n",
    "        TransformerAE, \n",
    "        EncoderNeuronGroup, \n",
    "        DecoderNeuronGroup, \n",
    "        EmbedderNeuronGroup,\n",
    "        PositionalEncoder,\n",
    "        EncoderLayer,\n",
    "        MultiHeadAttention,\n",
    "        FeedForward,\n",
    "        Norm,\n",
    "        Seq2Vec,\n",
    "        Neck2Seq,\n",
    "        get_clones\n",
    "    )\n",
    "    print(\"‚úÖ Successfully imported real TransformerAE architecture\")\n",
    "    \n",
    "    # Test basic instantiation\n",
    "    test_model = TransformerAE(\n",
    "        max_seq_len=50,\n",
    "        N=1,\n",
    "        heads=1,\n",
    "        d_model=100,\n",
    "        d_ff=100,\n",
    "        neck=20,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    print(f\"‚úÖ Test model created with {sum(p.numel() for p in test_model.parameters()):,} parameters\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Error importing transformer classes: {e}\")\n",
    "    print(\"Using simplified version for testing\")\n",
    "    \n",
    "    class TransformerAE(nn.Module):\n",
    "        def __init__(self, max_seq_len=50, N=1, heads=1, d_model=100, d_ff=100, neck=20, dropout=0.1, **kwargs):\n",
    "            super().__init__()\n",
    "            self.max_seq_len = max_seq_len\n",
    "            self.N = N\n",
    "            self.heads = heads\n",
    "            self.d_model = d_model\n",
    "            self.d_ff = d_ff\n",
    "            self.neck = neck\n",
    "            self.dropout = dropout\n",
    "            \n",
    "            # Simplified implementation\n",
    "            self.enc1 = nn.Linear(2464, d_model)\n",
    "            self.enc2 = nn.Linear(2464, d_model)\n",
    "            self.fusion = nn.Linear(d_model * 2, neck)\n",
    "            self.dec = nn.Linear(neck, 2464)\n",
    "            \n",
    "        def forward(self, inp1, inp2):\n",
    "            out1 = self.enc1(inp1)\n",
    "            out2 = self.enc2(inp2)\n",
    "            fused = torch.cat([out1, out2], dim=-1)\n",
    "            neck_rep = torch.tanh(self.fusion(fused))\n",
    "            output = self.dec(neck_rep)\n",
    "            return output, neck_rep, [], [], []\n",
    "\n",
    "print(\"Transformer architecture ready for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Discovering Tracking Files ===\n",
      "üîç Scanning /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/Experiments for tracking files...\n",
      "‚úÖ Found 44 tracking CSV files\n",
      "\n",
      "üìÅ First few tracking files:\n",
      "  1. overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 gelu/sinkhorn/Tracking/AE epoch sinkhorn 0.csv (23,649,154 bytes)\n",
      "  2. overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu/sinkhorn 2025-11-03 16:02:07.335521 750/Tracking/AE epoch sinkhorn 249.csv (294,366,383 bytes)\n",
      "  3. overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu/sinkhorn 2025-11-03 16:02:07.335521 750/Tracking/AE epoch sinkhorn 299.csv (292,570,681 bytes)\n",
      "  4. overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu/sinkhorn 2025-11-03 16:02:07.335521 750/Tracking/AE epoch sinkhorn 399.csv (291,456,342 bytes)\n",
      "  5. overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu/sinkhorn 2025-11-03 16:02:07.335521 750/Tracking/AE epoch sinkhorn 349.csv (292,472,876 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Discover Tracking Files\n",
    "print(\"=== Discovering Tracking Files ===\")\n",
    "\n",
    "def discover_tracking_files():\n",
    "    \"\"\"Discover all tracking CSV files in Experiments directory\"\"\"\n",
    "    tracking_files = []\n",
    "    \n",
    "    if not EXPERIMENTS_DIR.exists():\n",
    "        print(f\"‚ùå Experiments directory not found: {EXPERIMENTS_DIR}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üîç Scanning {EXPERIMENTS_DIR} for tracking files...\")\n",
    "    \n",
    "    for tracking_dir in EXPERIMENTS_DIR.rglob(\"Tracking\"):\n",
    "        if tracking_dir.is_dir():\n",
    "            for csv_file in tracking_dir.glob(\"*.csv\"):\n",
    "                tracking_files.append({\n",
    "                    'path': str(csv_file),\n",
    "                    'relative_path': str(csv_file.relative_to(EXPERIMENTS_DIR)),\n",
    "                    'size': csv_file.stat().st_size\n",
    "                })\n",
    "    \n",
    "    return tracking_files\n",
    "\n",
    "# Discover all tracking files\n",
    "tracking_files = discover_tracking_files()\n",
    "print(f\"‚úÖ Found {len(tracking_files)} tracking CSV files\")\n",
    "\n",
    "if tracking_files:\n",
    "    print(\"\\nüìÅ First few tracking files:\")\n",
    "    for i, tf in enumerate(tracking_files[:5]):\n",
    "        print(f\"  {i+1}. {tf['relative_path']} ({tf['size']:,} bytes)\")\n",
    "else:\n",
    "    print(\"‚ùå No tracking files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Ground Truth Weights ===\n",
      "üìä Loaded 36468 rows from ground truth CSV\n",
      "üìä Total columns: 2483\n",
      "üìä Found 19 meta columns\n",
      "üìä Found 2464 weight columns: weight 0 to bias 2463\n",
      "üìä Weight matrix shape: (36468, 2464)\n",
      "‚úÖ Ground truth loaded successfully\n",
      "üìä Models: 36468\n",
      "üìä Weight dimension: 2464\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Ground Truth Weights\n",
    "print(\"=== Loading Ground Truth Weights ===\")\n",
    "\n",
    "def load_ground_truth_weights(csv_path):\n",
    "    \"\"\"Load ground truth weights from merged zoo CSV\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"üìä Loaded {len(df)} rows from ground truth CSV\")\n",
    "        print(f\"üìä Total columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Use correct column range for weights: df[17:-2]\n",
    "        weight_columns = df.columns[17:-2].tolist()\n",
    "        meta_columns = df.columns[:17].tolist() + df.columns[-2:].tolist()\n",
    "        \n",
    "        print(f\"üìä Found {len(meta_columns)} meta columns\")\n",
    "        print(f\"üìä Found {len(weight_columns)} weight columns: {weight_columns[0]} to {weight_columns[-1]}\")\n",
    "        \n",
    "        # Extract weight matrix\n",
    "        weight_matrix = df[weight_columns].values\n",
    "        print(f\"üìä Weight matrix shape: {weight_matrix.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'weight_matrix': weight_matrix,\n",
    "            'weight_columns': weight_columns,\n",
    "            'meta_columns': meta_columns,\n",
    "            'dataframe': df,\n",
    "            'num_models': len(df),\n",
    "            'weight_dim': len(weight_columns)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading ground truth: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_path = DATA_DIR / \"Merged zoo.csv\"\n",
    "ground_truth_data = load_ground_truth_weights(ground_truth_path)\n",
    "\n",
    "if ground_truth_data:\n",
    "    print(f\"‚úÖ Ground truth loaded successfully\")\n",
    "    print(f\"üìä Models: {ground_truth_data['num_models']}\")\n",
    "    print(f\"üìä Weight dimension: {ground_truth_data['weight_dim']}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load ground truth data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment Information Parsing ===\n",
      "üîç Testing experiment info parsing:\n",
      "  1. sinkhorn - Epoch 0 - overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 gelu\n",
      "  2. sinkhorn 2025-11-03 16:02:07.335521 750 - Epoch 249 - overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu\n",
      "  3. sinkhorn 2025-11-03 16:02:07.335521 750 - Epoch 299 - overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu\n",
      "‚úÖ Experiment parsing ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Parse Experiment Information\n",
    "print(\"=== Experiment Information Parsing ===\")\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Natural sorting key for filenames with numbers\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def parse_experiment_info_from_path(file_path):\n",
    "    \"\"\"Parse experiment information from the file path\"\"\"\n",
    "    path_parts = Path(file_path).parts\n",
    "    \n",
    "    # Extract experiment type and epoch from filename\n",
    "    filename = Path(file_path).stem\n",
    "    epoch = \"unknown\"\n",
    "    loss_type = \"unknown\"\n",
    "    experiment_name = \"unknown\"\n",
    "    \n",
    "    # Extract epoch number from filename like \"AE epoch MAE 49.csv\"\n",
    "    epoch_match = re.search(r'epoch\\s+\\w+\\s+(\\d+)', filename)\n",
    "    if epoch_match:\n",
    "        epoch = int(epoch_match.group(1))\n",
    "    \n",
    "    # Extract loss type from filename or path\n",
    "    for part in path_parts:\n",
    "        part_lower = part.lower()\n",
    "        if any(loss in part_lower for loss in ['mse', 'mae', 'mape', 'kl', 'fft', 'lwn', 'sinkhorn', 'auto']):\n",
    "            loss_type = part\n",
    "            break\n",
    "    \n",
    "    # Extract experiment name (scenario)\n",
    "    if len(path_parts) >= 1:\n",
    "        experiment_name = path_parts[0]\n",
    "    \n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'loss_type': loss_type,\n",
    "        'epoch': epoch,\n",
    "        'filename': filename\n",
    "    }\n",
    "\n",
    "# Test parsing on a few files\n",
    "if tracking_files:\n",
    "    print(\"üîç Testing experiment info parsing:\")\n",
    "    for i, tf in enumerate(tracking_files[:3]):\n",
    "        exp_info = parse_experiment_info_from_path(tf['relative_path'])\n",
    "        print(f\"  {i+1}. {exp_info['loss_type']} - Epoch {exp_info['epoch']} - {exp_info['experiment_name']}\")\n",
    "\n",
    "print(\"‚úÖ Experiment parsing ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading All Tracking Data ===\n",
      "üîÑ Processing ALL 44 tracking CSV files...\n",
      "üìä Progress: 1/44 - AE epoch MAE 49.csv\n",
      "üìä Progress: 11/44 - AE epoch sinkhorn 49.csv\n",
      "üìä Progress: 21/44 - AE epoch sinkhorn 449.csv\n",
      "üìä Progress: 31/44 - AE epoch sinkhorn 199.csv\n",
      "üìä Progress: 41/44 - AE epoch sinkhorn 249.csv\n",
      "\n",
      "‚úÖ Successfully loaded 44 out of 44 tracking files\n",
      "\n",
      "üìä Tracking Data Summary:\n",
      "üìä Total tracking files processed: 44\n",
      "\n",
      "üìä Experiment types found:\n",
      "  üìä MAE 2025-12-06 02:07:01.415742 300: 2 files, epochs: [49, 99]\n",
      "  üìä MAPE 2025-12-01 09:05:57.362884 300: 6 files, epochs: [49, 99, 149, 199, 249]...\n",
      "  üìä sinkhorn 2025-10-06 16:31:26.516927 6 : 2 files, epochs: [0, 4]\n",
      "  üìä sinkhorn 2025-10-13 10:08:20.655648 800 : 1 files, epochs: [49]\n",
      "  üìä sinkhorn 2025-10-20 18:19:57.166467 800 : 1 files, epochs: [49]\n",
      "  üìä sinkhorn 2025-10-22 10:25:31.722454 800: 15 files, epochs: [49, 99, 149, 199, 249]...\n",
      "  üìä sinkhorn 2025-11-03 16:02:07.335521 750: 9 files, epochs: [49, 99, 149, 199, 249]...\n",
      "  üìä sinkhorn 2025-11-20 09:29:48.453823 750: 6 files, epochs: [49, 99, 149, 199, 249]...\n",
      "  üìä sinkhorn: 2 files, epochs: [0, 0]\n",
      "\n",
      "üìä Layer boundaries for analysis:\n",
      "  üìä Layer 1: indices 0-208 (208 weights)\n",
      "  üìä Layer 2: indices 208-1414 (1206 weights)\n",
      "  üìä Layer 3: indices 1414-1514 (100 weights)\n",
      "  üìä Layer 4: indices 1514-2254 (740 weights)\n",
      "  üìä Layer 5: indices 2254-2464 (210 weights)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load All Tracking Data\n",
    "print(\"=== Loading All Tracking Data ===\")\n",
    "\n",
    "# Define layer delimiters for layer-wise analysis\n",
    "LAYER_DELIMITERS = [208, 1414, 1514, 2254, 2464]\n",
    "\n",
    "def get_layer_bounds():\n",
    "    \"\"\"Get layer boundaries for analysis\"\"\"\n",
    "    bounds = []\n",
    "    prev = 0\n",
    "    for delim in LAYER_DELIMITERS:\n",
    "        bounds.append((prev, delim))\n",
    "        prev = delim\n",
    "    return bounds\n",
    "\n",
    "def extract_layer_weights(weight_matrix, layer_bounds):\n",
    "    \"\"\"Extract weights for each layer\"\"\"\n",
    "    layer_weights = []\n",
    "    for start, end in layer_bounds:\n",
    "        layer_weights.append(weight_matrix[:, start:end])\n",
    "    return layer_weights\n",
    "\n",
    "def load_all_tracking_data(tracking_files):\n",
    "    \"\"\"Load ALL tracking CSVs and prepare for analysis\"\"\"\n",
    "    tracking_data = []\n",
    "    \n",
    "    print(f\"üîÑ Processing ALL {len(tracking_files)} tracking CSV files...\")\n",
    "    \n",
    "    # Sort files naturally\n",
    "    sorted_files = sorted(tracking_files, key=lambda x: natural_sort_key(x['relative_path']))\n",
    "    \n",
    "    for i, tf in enumerate(sorted_files):\n",
    "        try:\n",
    "            if i % 10 == 0:\n",
    "                print(f\"üìä Progress: {i+1}/{len(tracking_files)} - {Path(tf['relative_path']).name}\")\n",
    "            \n",
    "            df_track = pd.read_csv(tf['path'])\n",
    "            \n",
    "            # Remove Unnamed columns\n",
    "            df_track = df_track.drop(columns=[\"Unnamed: 0\"], errors='ignore')\n",
    "            \n",
    "            # Get all column names\n",
    "            cols = df_track.columns.tolist()\n",
    "            \n",
    "            # Extract weights using correct column ranges\n",
    "            if len(cols) >= 7394:\n",
    "                PD_cols = cols[2466:4930]\n",
    "                GT_cols = cols[2:2466]\n",
    "                FN_cols = cols[4930:7394]\n",
    "                \n",
    "                # Extract weight matrices\n",
    "                PD_data = df_track[PD_cols].to_numpy()\n",
    "                GT_data = df_track[GT_cols].to_numpy()\n",
    "                FN_data = df_track[FN_cols].to_numpy()\n",
    "                \n",
    "                # Check for duplicates in PD\n",
    "                num_unique_rows = (~pd.DataFrame(PD_data).duplicated()).sum()\n",
    "                num_total_rows = len(PD_data)\n",
    "                duplicate_percentage = (num_total_rows - num_unique_rows) * 100 / num_total_rows\n",
    "                \n",
    "                # Parse experiment info\n",
    "                exp_info = parse_experiment_info_from_path(tf['relative_path'])\n",
    "                \n",
    "                tracking_data.append({\n",
    "                    'file_info': tf,\n",
    "                    'experiment_info': exp_info,\n",
    "                    'dataframe': df_track,\n",
    "                    'PD_weights': PD_data,\n",
    "                    'GT_weights': GT_data,\n",
    "                    'FN_weights': FN_data,\n",
    "                    'PD_columns': PD_cols,\n",
    "                    'GT_columns': GT_cols,\n",
    "                    'FN_columns': FN_cols,\n",
    "                    'duplicate_percentage': duplicate_percentage,\n",
    "                    'num_rows': num_total_rows\n",
    "                })\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Not enough columns ({len(cols)} < 7394), skipping: {tf['relative_path']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {tf['path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(tracking_data)} out of {len(tracking_files)} tracking files\")\n",
    "    return tracking_data\n",
    "\n",
    "# Load all tracking data\n",
    "if tracking_files:\n",
    "    tracking_data = load_all_tracking_data(tracking_files)\n",
    "    \n",
    "    print(f\"\\nüìä Tracking Data Summary:\")\n",
    "    print(f\"üìä Total tracking files processed: {len(tracking_data)}\")\n",
    "    \n",
    "    # Group by experiment type\n",
    "    exp_groups = {}\n",
    "    for track in tracking_data:\n",
    "        exp_type = track['experiment_info']['loss_type']\n",
    "        if exp_type not in exp_groups:\n",
    "            exp_groups[exp_type] = []\n",
    "        exp_groups[exp_type].append(track)\n",
    "    \n",
    "    print(f\"\\nüìä Experiment types found:\")\n",
    "    for exp_type, tracks in exp_groups.items():\n",
    "        epochs = [t['experiment_info']['epoch'] for t in tracks if isinstance(t['experiment_info']['epoch'], int)]\n",
    "        print(f\"  üìä {exp_type}: {len(tracks)} files, epochs: {sorted(epochs)[:5]}{'...' if len(epochs) > 5 else ''}\")\n",
    "    \n",
    "    # Show layer boundaries\n",
    "    layer_bounds = get_layer_bounds()\n",
    "    print(f\"\\nüìä Layer boundaries for analysis:\")\n",
    "    for i, (start, end) in enumerate(layer_bounds):\n",
    "        print(f\"  üìä Layer {i+1}: indices {start}-{end} ({end-start} weights)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No tracking files found\")\n",
    "    tracking_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comprehensive Metrics Computation ===\n",
      "‚úÖ Metrics computation functions ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Comprehensive Metrics Computation\n",
    "print(\"=== Comprehensive Metrics Computation ===\")\n",
    "\n",
    "def compute_comprehensive_metrics(weights_a, weights_b, layer_bounds=None):\n",
    "    \"\"\"Compute comprehensive metrics between two weight matrices\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Flatten weights for overall metrics\n",
    "    a_flat = weights_a.flatten()\n",
    "    b_flat = weights_b.flatten()\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['mse'] = mean_squared_error(a_flat, b_flat)\n",
    "    metrics['mae'] = mean_absolute_error(a_flat, b_flat)\n",
    "    \n",
    "    # MAPE (handle zeros)\n",
    "    mask = a_flat != 0\n",
    "    if mask.sum() > 0:\n",
    "        metrics['mape'] = np.mean(np.abs((a_flat[mask] - b_flat[mask]) / a_flat[mask])) * 100\n",
    "    else:\n",
    "        metrics['mape'] = np.inf\n",
    "    \n",
    "    # Wasserstein distance\n",
    "    metrics['wasserstein'] = wasserstein_distance(a_flat, b_flat)\n",
    "    \n",
    "    # Jensen-Shannon divergence\n",
    "    # Convert to probability distributions\n",
    "    a_prob = np.abs(a_flat) + 1e-10\n",
    "    a_prob = a_prob / a_prob.sum()\n",
    "    b_prob = np.abs(b_flat) + 1e-10\n",
    "    b_prob = b_prob / b_prob.sum()\n",
    "    metrics['js_divergence'] = jensenshannon(a_prob, b_prob) ** 2\n",
    "    \n",
    "    # KL divergence\n",
    "    metrics['kl_divergence'] = np.sum(a_prob * np.log(a_prob / b_prob + 1e-10))\n",
    "    \n",
    "    # Cosine similarity\n",
    "    metrics['cosine_similarity'] = np.dot(a_flat, b_flat) / (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-10)\n",
    "    \n",
    "    # Pearson correlation\n",
    "    metrics['pearson_corr'] = np.corrcoef(a_flat, b_flat)[0, 1]\n",
    "    \n",
    "    # Log-norm\n",
    "    metrics['log_norm_diff'] = np.abs(np.log(np.linalg.norm(a_flat) + 1e-10) - np.log(np.linalg.norm(b_flat) + 1e-10))\n",
    "    \n",
    "    # AUTO loss (combination of metrics)\n",
    "    metrics['auto_loss'] = 0.4 * metrics['mse'] + 0.3 * metrics['mae'] + 0.2 * metrics['wasserstein'] + 0.1 * metrics['js_divergence']\n",
    "    \n",
    "    # Layer-wise metrics if bounds provided\n",
    "    if layer_bounds:\n",
    "        layer_metrics = []\n",
    "        a_layers = extract_layer_weights(weights_a, layer_bounds)\n",
    "        b_layers = extract_layer_weights(weights_b, layer_bounds)\n",
    "        \n",
    "        for i, (a_layer, b_layer) in enumerate(zip(a_layers, b_layers)):\n",
    "            layer_metric = compute_comprehensive_metrics(a_layer, b_layer)\n",
    "            layer_metrics.append(layer_metric)\n",
    "        \n",
    "        metrics['layer_metrics'] = layer_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Metrics computation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing All Experiments ===\n",
      "üîÑ Processing 44 experiments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing experiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [09:15<00:00, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully processed 44 experiments\n",
      "\n",
      "üìä Analysis Summary:\n",
      "üìä Total experiments processed: 44\n",
      "\n",
      "üìä First few experiments:\n",
      "  üìä MAE 2025-12-06 02:07:01.415742 300_epoch49: MSE=0.059657\n",
      "  üìä MAE 2025-12-06 02:07:01.415742 300_epoch99: MSE=0.059661\n",
      "  üìä MAPE 2025-12-01 09:05:57.362884 300_epoch49: MSE=0.774870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Process All Experiments and Compute Metrics\n",
    "print(\"=== Processing All Experiments ===\")\n",
    "\n",
    "def process_all_experiments(tracking_data, ground_truth_data, layer_bounds):\n",
    "    \"\"\"Process all experiments and compute comprehensive metrics\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    layer_bounds = get_layer_bounds()\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(tracking_data)} experiments...\")\n",
    "    \n",
    "    for i, track in enumerate(tqdm(tracking_data, desc=\"Processing experiments\")):\n",
    "        try:\n",
    "            exp_info = track['experiment_info']\n",
    "            \n",
    "            # Create experiment name with epoch and loss type\n",
    "            experiment_name = f\"{exp_info['loss_type']}_epoch{exp_info['epoch']}\"\n",
    "            \n",
    "            # Get weights\n",
    "            PD_weights = track['PD_weights']\n",
    "            GT_weights = track['GT_weights']\n",
    "            FN_weights = track['FN_weights']\n",
    "            \n",
    "            # Compute intra metrics (PD vs GT, FN vs GT)\n",
    "            pd_vs_gt_metrics = compute_comprehensive_metrics(PD_weights, GT_weights, layer_bounds)\n",
    "            fn_vs_gt_metrics = compute_comprehensive_metrics(FN_weights, GT_weights, layer_bounds)\n",
    "            \n",
    "            # Compute inter metrics (PD vs FN)\n",
    "            pd_vs_fn_metrics = compute_comprehensive_metrics(PD_weights, FN_weights, layer_bounds)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'experiment_name': experiment_name,\n",
    "                'loss_type': exp_info['loss_type'],\n",
    "                'epoch': exp_info['epoch'],\n",
    "                'scenario': exp_info['experiment_name'],\n",
    "                'filename': exp_info['filename'],\n",
    "                'num_rows': track['num_rows'],\n",
    "                'duplicate_percentage': track['duplicate_percentage'],\n",
    "                'pd_vs_gt': pd_vs_gt_metrics,\n",
    "                'fn_vs_gt': fn_vs_gt_metrics,\n",
    "                'pd_vs_fn': pd_vs_fn_metrics\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing experiment {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Successfully processed {len(results)} experiments\")\n",
    "    return results\n",
    "\n",
    "# Process all experiments if data is available\n",
    "if tracking_data and ground_truth_data:\n",
    "    layer_bounds = get_layer_bounds()\n",
    "    analysis_results = process_all_experiments(tracking_data, ground_truth_data, layer_bounds)\n",
    "    \n",
    "    print(f\"\\nüìä Analysis Summary:\")\n",
    "    print(f\"üìä Total experiments processed: {len(analysis_results)}\")\n",
    "    \n",
    "    # Show first few results\n",
    "    if analysis_results:\n",
    "        print(f\"\\nüìä First few experiments:\")\n",
    "        for i, result in enumerate(analysis_results[:3]):\n",
    "            print(f\"  üìä {result['experiment_name']}: MSE={result['pd_vs_gt']['mse']:.6f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing tracking data or ground truth data\")\n",
    "    analysis_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Results Tables ===\n",
      "‚úÖ Created results DataFrames:\n",
      "üìä Intra metrics: 44 rows\n",
      "üìä Inter metrics: 44 rows\n",
      "üìä Layer-wise intra: 220 rows\n",
      "üìä Layer-wise inter: 220 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Create Results Tables\n",
    "print(\"=== Creating Results Tables ===\")\n",
    "\n",
    "def create_results_dataframes(analysis_results):\n",
    "    \"\"\"Create comprehensive results DataFrames\"\"\"\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"‚ùå No analysis results available\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Prepare data for main tables\n",
    "    intra_data = []\n",
    "    inter_data = []\n",
    "    \n",
    "    for result in analysis_results:\n",
    "        # Intra metrics (PD vs GT, FN vs GT)\n",
    "        intra_row = {\n",
    "            'Experiment': result['experiment_name'],\n",
    "            'Loss_Type': result['loss_type'],\n",
    "            'Epoch': result['epoch'],\n",
    "            'Scenario': result['scenario'],\n",
    "            'PD_vs_GT_MSE': result['pd_vs_gt']['mse'],\n",
    "            'PD_vs_GT_MAE': result['pd_vs_gt']['mae'],\n",
    "            'PD_vs_GT_MAPE': result['pd_vs_gt']['mape'],\n",
    "            'PD_vs_GT_Wasserstein': result['pd_vs_gt']['wasserstein'],\n",
    "            'PD_vs_GT_JS_Divergence': result['pd_vs_gt']['js_divergence'],\n",
    "            'PD_vs_GT_KL_Divergence': result['pd_vs_gt']['kl_divergence'],\n",
    "            'PD_vs_GT_Cosine_Sim': result['pd_vs_gt']['cosine_similarity'],\n",
    "            'PD_vs_GT_Pearson_Corr': result['pd_vs_gt']['pearson_corr'],\n",
    "            'PD_vs_GT_AUTO_Loss': result['pd_vs_gt']['auto_loss'],\n",
    "            'FN_vs_GT_MSE': result['fn_vs_gt']['mse'],\n",
    "            'FN_vs_GT_MAE': result['fn_vs_gt']['mae'],\n",
    "            'FN_vs_GT_MAPE': result['fn_vs_gt']['mape'],\n",
    "            'FN_vs_GT_Wasserstein': result['fn_vs_gt']['wasserstein'],\n",
    "            'FN_vs_GT_JS_Divergence': result['fn_vs_gt']['js_divergence'],\n",
    "            'FN_vs_GT_KL_Divergence': result['fn_vs_gt']['kl_divergence'],\n",
    "            'FN_vs_GT_Cosine_Sim': result['fn_vs_gt']['cosine_similarity'],\n",
    "            'FN_vs_GT_Pearson_Corr': result['fn_vs_gt']['pearson_corr'],\n",
    "            'FN_vs_GT_AUTO_Loss': result['fn_vs_gt']['auto_loss']\n",
    "        }\n",
    "        intra_data.append(intra_row)\n",
    "        \n",
    "        # Inter metrics (PD vs FN)\n",
    "        inter_row = {\n",
    "            'Experiment': result['experiment_name'],\n",
    "            'Loss_Type': result['loss_type'],\n",
    "            'Epoch': result['epoch'],\n",
    "            'Scenario': result['scenario'],\n",
    "            'PD_vs_FN_MSE': result['pd_vs_fn']['mse'],\n",
    "            'PD_vs_FN_MAE': result['pd_vs_fn']['mae'],\n",
    "            'PD_vs_FN_MAPE': result['pd_vs_fn']['mape'],\n",
    "            'PD_vs_FN_Wasserstein': result['pd_vs_fn']['wasserstein'],\n",
    "            'PD_vs_FN_JS_Divergence': result['pd_vs_fn']['js_divergence'],\n",
    "            'PD_vs_FN_KL_Divergence': result['pd_vs_fn']['kl_divergence'],\n",
    "            'PD_vs_FN_Cosine_Sim': result['pd_vs_fn']['cosine_similarity'],\n",
    "            'PD_vs_FN_Pearson_Corr': result['pd_vs_fn']['pearson_corr'],\n",
    "            'PD_vs_FN_AUTO_Loss': result['pd_vs_fn']['auto_loss']\n",
    "        }\n",
    "        inter_data.append(inter_row)\n",
    "    \n",
    "    # Create main DataFrames\n",
    "    df_intra = pd.DataFrame(intra_data)\n",
    "    df_inter = pd.DataFrame(inter_data)\n",
    "    \n",
    "    # Create layer-wise DataFrames\n",
    "    layer_intra_data = []\n",
    "    layer_inter_data = []\n",
    "    \n",
    "    layer_bounds = get_layer_bounds()\n",
    "    \n",
    "    for result in analysis_results:\n",
    "        for layer_idx in range(len(layer_bounds)):\n",
    "            layer_name = f\"Layer_{layer_idx+1}\"\n",
    "            \n",
    "            # Layer-wise intra metrics\n",
    "            if 'layer_metrics' in result['pd_vs_gt'] and 'layer_metrics' in result['fn_vs_gt']:\n",
    "                layer_intra_row = {\n",
    "                    'Experiment': result['experiment_name'],\n",
    "                    'Loss_Type': result['loss_type'],\n",
    "                    'Epoch': result['epoch'],\n",
    "                    'Layer': layer_name,\n",
    "                    'PD_vs_GT_MSE': result['pd_vs_gt']['layer_metrics'][layer_idx]['mse'],\n",
    "                    'PD_vs_GT_MAE': result['pd_vs_gt']['layer_metrics'][layer_idx]['mae'],\n",
    "                    'PD_vs_GT_Wasserstein': result['pd_vs_gt']['layer_metrics'][layer_idx]['wasserstein'],\n",
    "                    'PD_vs_GT_Cosine_Sim': result['pd_vs_gt']['layer_metrics'][layer_idx]['cosine_similarity'],\n",
    "                    'FN_vs_GT_MSE': result['fn_vs_gt']['layer_metrics'][layer_idx]['mse'],\n",
    "                    'FN_vs_GT_MAE': result['fn_vs_gt']['layer_metrics'][layer_idx]['mae'],\n",
    "                    'FN_vs_GT_Wasserstein': result['fn_vs_gt']['layer_metrics'][layer_idx]['wasserstein'],\n",
    "                    'FN_vs_GT_Cosine_Sim': result['fn_vs_gt']['layer_metrics'][layer_idx]['cosine_similarity']\n",
    "                }\n",
    "                layer_intra_data.append(layer_intra_row)\n",
    "            \n",
    "            # Layer-wise inter metrics\n",
    "            if 'layer_metrics' in result['pd_vs_fn']:\n",
    "                layer_inter_row = {\n",
    "                    'Experiment': result['experiment_name'],\n",
    "                    'Loss_Type': result['loss_type'],\n",
    "                    'Epoch': result['epoch'],\n",
    "                    'Layer': layer_name,\n",
    "                    'PD_vs_FN_MSE': result['pd_vs_fn']['layer_metrics'][layer_idx]['mse'],\n",
    "                    'PD_vs_FN_MAE': result['pd_vs_fn']['layer_metrics'][layer_idx]['mae'],\n",
    "                    'PD_vs_FN_Wasserstein': result['pd_vs_fn']['layer_metrics'][layer_idx]['wasserstein'],\n",
    "                    'PD_vs_FN_Cosine_Sim': result['pd_vs_fn']['layer_metrics'][layer_idx]['cosine_similarity']\n",
    "                }\n",
    "                layer_inter_data.append(layer_inter_row)\n",
    "    \n",
    "    df_layer_intra = pd.DataFrame(layer_intra_data) if layer_intra_data else pd.DataFrame()\n",
    "    df_layer_inter = pd.DataFrame(layer_inter_data) if layer_inter_data else pd.DataFrame()\n",
    "    \n",
    "    print(f\"‚úÖ Created results DataFrames:\")\n",
    "    print(f\"üìä Intra metrics: {len(df_intra)} rows\")\n",
    "    print(f\"üìä Inter metrics: {len(df_inter)} rows\")\n",
    "    print(f\"üìä Layer-wise intra: {len(df_layer_intra)} rows\")\n",
    "    print(f\"üìä Layer-wise inter: {len(df_layer_inter)} rows\")\n",
    "    \n",
    "    return df_intra, df_inter, df_layer_intra, df_layer_inter\n",
    "\n",
    "# Create results tables\n",
    "if analysis_results:\n",
    "    df_intra, df_inter, df_layer_intra, df_layer_inter = create_results_dataframes(analysis_results)\n",
    "else:\n",
    "    print(\"‚ùå No analysis results to create tables\")\n",
    "    df_intra = df_inter = df_layer_intra = df_layer_inter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Styling and Displaying Results ===\n",
      "\n",
      "üìä RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Table 1: Intra Metrics (PD vs GT, FN vs GT) - 44 experiments\n",
      "üìä Columns: ['Experiment', 'Loss_Type', 'Epoch', 'Scenario', 'PD_vs_GT_MSE', 'PD_vs_GT_MAE', 'PD_vs_GT_MAPE', 'PD_vs_GT_Wasserstein', 'PD_vs_GT_JS_Divergence', 'PD_vs_GT_KL_Divergence', 'PD_vs_GT_Cosine_Sim', 'PD_vs_GT_Pearson_Corr', 'PD_vs_GT_AUTO_Loss', 'FN_vs_GT_MSE', 'FN_vs_GT_MAE', 'FN_vs_GT_MAPE', 'FN_vs_GT_Wasserstein', 'FN_vs_GT_JS_Divergence', 'FN_vs_GT_KL_Divergence', 'FN_vs_GT_Cosine_Sim', 'FN_vs_GT_Pearson_Corr', 'FN_vs_GT_AUTO_Loss']\n",
      "üìä Best MSE (PD vs GT): 0.059585097572023835\n",
      "üìä Worst MSE (PD vs GT): 0.7748698027435761\n",
      "\n",
      "üìä Top 5 Best Performing Experiments (by MSE):\n",
      "                                      Experiment                               Loss_Type  Epoch  PD_vs_GT_MSE  PD_vs_GT_AUTO_Loss\n",
      "sinkhorn 2025-11-20 09:29:48.453823 750_epoch299 sinkhorn 2025-11-20 09:29:48.453823 750    299      0.059585            0.125880\n",
      "sinkhorn 2025-11-20 09:29:48.453823 750_epoch249 sinkhorn 2025-11-20 09:29:48.453823 750    249      0.059612            0.125783\n",
      "      MAE 2025-12-06 02:07:01.415742 300_epoch49      MAE 2025-12-06 02:07:01.415742 300     49      0.059657            0.130392\n",
      "      MAE 2025-12-06 02:07:01.415742 300_epoch99      MAE 2025-12-06 02:07:01.415742 300     99      0.059661            0.130547\n",
      "    MAPE 2025-12-01 09:05:57.362884 300_epoch299     MAPE 2025-12-01 09:05:57.362884 300    299      0.059697            0.130894\n",
      "\n",
      "üìä Table 2: Inter Metrics (PD vs FN) - 44 experiments\n",
      "üìä Columns: ['Experiment', 'Loss_Type', 'Epoch', 'Scenario', 'PD_vs_FN_MSE', 'PD_vs_FN_MAE', 'PD_vs_FN_MAPE', 'PD_vs_FN_Wasserstein', 'PD_vs_FN_JS_Divergence', 'PD_vs_FN_KL_Divergence', 'PD_vs_FN_Cosine_Sim', 'PD_vs_FN_Pearson_Corr', 'PD_vs_FN_AUTO_Loss']\n",
      "üìä Best MSE (PD vs FN): 0.014662394702632644\n",
      "üìä Worst MSE (PD vs FN): 0.3877719014374564\n",
      "\n",
      "üìä Table 3: Layer-wise Intra Metrics - 220 layer-experiments\n",
      "üìä Layers analyzed: ['Layer_1' 'Layer_2' 'Layer_3' 'Layer_4' 'Layer_5']\n",
      "\n",
      "üìä Table 4: Layer-wise Inter Metrics - 220 layer-experiments\n",
      "üìä Layers analyzed: ['Layer_1' 'Layer_2' 'Layer_3' 'Layer_4' 'Layer_5']\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Style and Display Results\n",
    "print(\"=== Styling and Displaying Results ===\")\n",
    "\n",
    "def style_dataframe(df, title):\n",
    "    \"\"\"Style DataFrame for better visualization\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(f\"‚ùå No data for {title}\")\n",
    "        return None\n",
    "    \n",
    "    # Create styled DataFrame\n",
    "    styled = df.style.background_gradient(cmap='RdYlBu_r', subset=df.select_dtypes(include=[np.number]).columns)\n",
    "    styled = styled.format('{:.6f}', subset=df.select_dtypes(include=[np.number]).columns)\n",
    "    styled = styled.set_caption(title)\n",
    "    styled = styled.set_properties(**{'text-align': 'center'})\n",
    "    \n",
    "    return styled\n",
    "\n",
    "def display_results_summary(df_intra, df_inter, df_layer_intra, df_layer_inter):\n",
    "    \"\"\"Display summary of all results\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä RESULTS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if df_intra is not None and not df_intra.empty:\n",
    "        print(f\"\\nüìä Table 1: Intra Metrics (PD vs GT, FN vs GT) - {len(df_intra)} experiments\")\n",
    "        print(\"üìä Columns:\", list(df_intra.columns))\n",
    "        print(\"üìä Best MSE (PD vs GT):\", df_intra['PD_vs_GT_MSE'].min())\n",
    "        print(\"üìä Worst MSE (PD vs GT):\", df_intra['PD_vs_GT_MSE'].max())\n",
    "        \n",
    "        # Show top 5 best performing experiments\n",
    "        top_5 = df_intra.nsmallest(5, 'PD_vs_GT_MSE')[['Experiment', 'Loss_Type', 'Epoch', 'PD_vs_GT_MSE', 'PD_vs_GT_AUTO_Loss']]\n",
    "        print(\"\\nüìä Top 5 Best Performing Experiments (by MSE):\")\n",
    "        print(top_5.to_string(index=False))\n",
    "    \n",
    "    if df_inter is not None and not df_inter.empty:\n",
    "        print(f\"\\nüìä Table 2: Inter Metrics (PD vs FN) - {len(df_inter)} experiments\")\n",
    "        print(\"üìä Columns:\", list(df_inter.columns))\n",
    "        print(\"üìä Best MSE (PD vs FN):\", df_inter['PD_vs_FN_MSE'].min())\n",
    "        print(\"üìä Worst MSE (PD vs FN):\", df_inter['PD_vs_FN_MSE'].max())\n",
    "    \n",
    "    if df_layer_intra is not None and not df_layer_intra.empty:\n",
    "        print(f\"\\nüìä Table 3: Layer-wise Intra Metrics - {len(df_layer_intra)} layer-experiments\")\n",
    "        print(\"üìä Layers analyzed:\", df_layer_intra['Layer'].unique())\n",
    "    \n",
    "    if df_layer_inter is not None and not df_layer_inter.empty:\n",
    "        print(f\"\\nüìä Table 4: Layer-wise Inter Metrics - {len(df_layer_inter)} layer-experiments\")\n",
    "        print(\"üìä Layers analyzed:\", df_layer_inter['Layer'].unique())\n",
    "\n",
    "# Display results summary - Fixed DataFrame evaluation\n",
    "has_dataframes = (\n",
    "    (df_intra is not None and not df_intra.empty) or\n",
    "    (df_inter is not None and not df_inter.empty) or\n",
    "    (df_layer_intra is not None and not df_layer_intra.empty) or\n",
    "    (df_layer_inter is not None and not df_layer_inter.empty)\n",
    ")\n",
    "\n",
    "if has_dataframes:\n",
    "    display_results_summary(df_intra, df_inter, df_layer_intra, df_layer_inter)\n",
    "else:\n",
    "    print(\"‚ùå No results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Organized Visualizations with Individual Metric Plots ===\n",
      "üîÑ Running organized visualization function...\n",
      "üóÇÔ∏è Setting up results folder structure...\n",
      "   Clearing existing results folder: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results\n",
      "   üìÅ Created: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/inter/full_models\n",
      "   üìÅ Created: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/intra/full_models\n",
      "   üìÅ Created: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/inter/per_layer\n",
      "   üìÅ Created: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/intra/per_layer\n",
      "‚úÖ Results folder structure created\n",
      "\n",
      "üìä Creating individual metric plots...\n",
      "\n",
      "   üìÅ Processing INTRA metrics (full models) - 44 experiments\n",
      "      Found 18 intra metrics to plot\n",
      "      ‚úÖ PD_vs_GT_MSE\n",
      "      ‚úÖ PD_vs_GT_MAE\n",
      "      ‚úÖ PD_vs_GT_MAPE\n",
      "      ‚úÖ PD_vs_GT_Wasserstein\n",
      "      ‚úÖ PD_vs_GT_JS_Divergence\n",
      "      ‚úÖ PD_vs_GT_KL_Divergence\n",
      "      ‚úÖ PD_vs_GT_Cosine_Sim\n",
      "      ‚úÖ PD_vs_GT_Pearson_Corr\n",
      "      ‚úÖ PD_vs_GT_AUTO_Loss\n",
      "      ‚úÖ FN_vs_GT_MSE\n",
      "      ‚úÖ FN_vs_GT_MAE\n",
      "      ‚úÖ FN_vs_GT_MAPE\n",
      "      ‚úÖ FN_vs_GT_Wasserstein\n",
      "      ‚úÖ FN_vs_GT_JS_Divergence\n",
      "      ‚úÖ FN_vs_GT_KL_Divergence\n",
      "      ‚úÖ FN_vs_GT_Cosine_Sim\n",
      "      ‚úÖ FN_vs_GT_Pearson_Corr\n",
      "      ‚úÖ FN_vs_GT_AUTO_Loss\n",
      "\n",
      "   üìÅ Processing INTER metrics (full models) - 44 experiments\n",
      "      Found 9 inter metrics to plot\n",
      "      ‚úÖ PD_vs_FN_MSE\n",
      "      ‚úÖ PD_vs_FN_MAE\n",
      "      ‚úÖ PD_vs_FN_MAPE\n",
      "      ‚úÖ PD_vs_FN_Wasserstein\n",
      "      ‚úÖ PD_vs_FN_JS_Divergence\n",
      "      ‚úÖ PD_vs_FN_KL_Divergence\n",
      "      ‚úÖ PD_vs_FN_Cosine_Sim\n",
      "      ‚úÖ PD_vs_FN_Pearson_Corr\n",
      "      ‚úÖ PD_vs_FN_AUTO_Loss\n",
      "\n",
      "   üìÅ Processing INTRA metrics (per layer) - 220 layer-experiments\n",
      "      Found 8 layer-wise intra metrics to plot\n",
      "      ‚úÖ PD_vs_GT_MSE\n",
      "      ‚úÖ PD_vs_GT_MAE\n",
      "      ‚úÖ PD_vs_GT_Wasserstein\n",
      "      ‚úÖ PD_vs_GT_Cosine_Sim\n",
      "      ‚úÖ FN_vs_GT_MSE\n",
      "      ‚úÖ FN_vs_GT_MAE\n",
      "      ‚úÖ FN_vs_GT_Wasserstein\n",
      "      ‚úÖ FN_vs_GT_Cosine_Sim\n",
      "\n",
      "   üìÅ Processing INTER metrics (per layer) - 220 layer-experiments\n",
      "      Found 4 layer-wise inter metrics to plot\n",
      "      ‚úÖ PD_vs_FN_MSE\n",
      "      ‚úÖ PD_vs_FN_MAE\n",
      "      ‚úÖ PD_vs_FN_Wasserstein\n",
      "      ‚úÖ PD_vs_FN_Cosine_Sim\n",
      "\n",
      "‚úÖ Created 39 individual metric plots\n",
      "\n",
      "üìÅ Results saved to: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results\n",
      "üìÇ Subfolder structure:\n",
      "   üìÅ inter/full_models: 9 plots\n",
      "   üìÅ intra/full_models: 18 plots\n",
      "   üìÅ inter/per_layer: 4 plots\n",
      "   üìÅ intra/per_layer: 8 plots\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Create Organized Visualizations with Individual Metric Plots\n",
    "print(\"=== Creating Organized Visualizations with Individual Metric Plots ===\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Full list of metrics as requested by user\n",
    "FULL_METRICS_LIST = [\n",
    "    \"Mel L2\", \"Mel FID\", \"FFT Loss\", \"JS Loss\", \"MAE\", \"MSE\", \"Latent\", \n",
    "    \"MAPE\", \"sinkhorn\", \"gw_loss\", \"ws_scipy\", \"CAE\", \"Q-quantile_loss\", \n",
    "    \"LWLN\", \"LWWS\", \"FIM\", \"log-norm\", \"AUTO\", \"KL divergence\", \n",
    "    \"Forb_norm\", \"LWWS_scipy\", \"ws_scipy 0.9\", \"ws_scipy\"\n",
    "]\n",
    "\n",
    "# Define custom loss functions (non-standard metrics)\n",
    "CUSTOM_LOSS_FUNCTIONS = [\n",
    "    \"Mel L2\", \"Mel FID\", \"FFT Loss\", \"JS Loss\", \"Latent\", \n",
    "    \"sinkhorn\", \"gw_loss\", \"ws_scipy\", \"CAE\", \"Q-quantile_loss\", \n",
    "    \"LWLN\", \"LWWS\", \"FIM\", \"log-norm\", \"AUTO\", \"KL divergence\", \n",
    "    \"Forb_norm\", \"LWWS_scipy\", \"ws_scipy 0.9\"\n",
    "]\n",
    "\n",
    "def clean_experiment_name(experiment_name):\n",
    "    \"\"\"Clean experiment name by removing timestamp and year, keeping only day and month\"\"\"\n",
    "    if pd.isna(experiment_name):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    parts = str(experiment_name).split()\n",
    "    if len(parts) >= 2:\n",
    "        date_part = parts[0]  # YYYY-MM-DD format\n",
    "        if len(date_part) == 10 and date_part[4] == '-' and date_part[7] == '-':\n",
    "            cleaned = f\"{date_part[5:7]}-{date_part[8:10]}\"\n",
    "            if len(parts) > 2:\n",
    "                cleaned += f\" {parts[2]}\"\n",
    "            return cleaned\n",
    "    return str(experiment_name)[:20]\n",
    "\n",
    "def setup_results_structure():\n",
    "    \"\"\"Clear results folder and create organized subfolder structure\"\"\"\n",
    "    print(\"üóÇÔ∏è Setting up results folder structure...\")\n",
    "    \n",
    "    # Clear existing results folder if it exists\n",
    "    if RESULTS_DIR.exists():\n",
    "        print(f\"   Clearing existing results folder: {RESULTS_DIR}\")\n",
    "        shutil.rmtree(RESULTS_DIR)\n",
    "    \n",
    "    # Create main results folder\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create subfolder structure\n",
    "    subfolders = [\n",
    "        'inter/full_models',\n",
    "        'intra/full_models', \n",
    "        'inter/per_layer',\n",
    "        'intra/per_layer'\n",
    "    ]\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        folder_path = RESULTS_DIR / subfolder\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   üìÅ Created: {folder_path}\")\n",
    "    \n",
    "    print(\"‚úÖ Results folder structure created\")\n",
    "    return True\n",
    "\n",
    "def plot_individual_metric(df, metric_name, save_path, title_prefix=\"\"):\n",
    "    \"\"\"Create and save individual metric plot\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Clean experiment names\n",
    "    df_clean = df.copy()\n",
    "    df_clean['Clean_Exp'] = df_clean['Experiment'].apply(clean_experiment_name)\n",
    "    \n",
    "    # Create bar plot with viridis colors\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(df_clean)))\n",
    "    bars = ax.bar(range(len(df_clean)), df_clean[metric_name], color=colors, alpha=0.8)\n",
    "    \n",
    "    # Set labels and title\n",
    "    clean_metric = metric_name.replace('PD_vs_GT_', 'PD vs GT ').replace('FN_vs_GT_', 'FN vs GT ').replace('PD_vs_FN_', 'PD vs FN ')\n",
    "    ax.set_title(f'{title_prefix}{clean_metric}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Experiments', fontsize=12)\n",
    "    ax.set_ylabel(metric_name.split('_')[-1] if '_' in metric_name else metric_name, fontsize=12)\n",
    "    ax.set_xticks(range(len(df_clean)))\n",
    "    ax.set_xticklabels(df_clean['Clean_Exp'], rotation=45, ha='right', fontsize=9)\n",
    "    \n",
    "    # Add grid for readability\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars for top performers\n",
    "    if len(df_clean) <= 15:\n",
    "        try:\n",
    "            sorted_vals = sorted(df_clean[metric_name])\n",
    "            threshold = sorted_vals[min(5, len(sorted_vals)-1)]  # Top 5\n",
    "            for i, (bar, val) in enumerate(zip(bars, df_clean[metric_name])):\n",
    "                if val <= threshold:\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_clean[metric_name])*0.01,\n",
    "                           f'{val:.4f}', ha='center', va='bottom', fontsize=8, rotation=45)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return True\n",
    "\n",
    "def create_organized_visualizations(df_intra, df_inter, df_layer_intra, df_layer_inter):\n",
    "    \"\"\"Create organized visualizations with individual metric plots in subfolders\"\"\"\n",
    "    \n",
    "    if not any(df is not None and not df.empty for df in [df_intra, df_inter, df_layer_intra, df_layer_inter]):\n",
    "        print(\"‚ùå No data available for visualizations\")\n",
    "        return []\n",
    "    \n",
    "    # Setup folder structure\n",
    "    setup_results_structure()\n",
    "    \n",
    "    fig_list = []\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['figure.max_open_warning'] = 0\n",
    "    \n",
    "    print(\"\\nüìä Creating individual metric plots...\")\n",
    "    \n",
    "    # 1. INTRA FULL MODELS - Individual metric plots\n",
    "    if df_intra is not None and not df_intra.empty:\n",
    "        print(f\"\\n   üìÅ Processing INTRA metrics (full models) - {len(df_intra)} experiments\")\n",
    "        intra_folder = RESULTS_DIR / 'intra/full_models'\n",
    "        \n",
    "        # Get all intra metrics (PD vs GT and FN vs GT)\n",
    "        intra_metrics = [col for col in df_intra.columns \n",
    "                        if any(prefix in col for prefix in ['PD_vs_GT_', 'FN_vs_GT_'])]\n",
    "        \n",
    "        print(f\"      Found {len(intra_metrics)} intra metrics to plot\")\n",
    "        \n",
    "        for metric in intra_metrics:\n",
    "            save_path = intra_folder / f\"{metric}.png\"\n",
    "            plot_individual_metric(df_intra, metric, save_path, \"Intra - \")\n",
    "            fig_list.append((f\"Intra - {metric}\", save_path))\n",
    "            print(f\"      ‚úÖ {metric}\")\n",
    "    \n",
    "    # 2. INTER FULL MODELS - Individual metric plots\n",
    "    if df_inter is not None and not df_inter.empty:\n",
    "        print(f\"\\n   üìÅ Processing INTER metrics (full models) - {len(df_inter)} experiments\")\n",
    "        inter_folder = RESULTS_DIR / 'inter/full_models'\n",
    "        \n",
    "        # Get all inter metrics (PD vs FN)\n",
    "        inter_metrics = [col for col in df_inter.columns \n",
    "                        if 'PD_vs_FN_' in col]\n",
    "        \n",
    "        print(f\"      Found {len(inter_metrics)} inter metrics to plot\")\n",
    "        \n",
    "        for metric in inter_metrics:\n",
    "            save_path = inter_folder / f\"{metric}.png\"\n",
    "            plot_individual_metric(df_inter, metric, save_path, \"Inter - \")\n",
    "            fig_list.append((f\"Inter - {metric}\", save_path))\n",
    "            print(f\"      ‚úÖ {metric}\")\n",
    "    \n",
    "    # 3. INTRA PER LAYER - Individual metric plots\n",
    "    if df_layer_intra is not None and not df_layer_intra.empty:\n",
    "        print(f\"\\n   üìÅ Processing INTRA metrics (per layer) - {len(df_layer_intra)} layer-experiments\")\n",
    "        intra_layer_folder = RESULTS_DIR / 'intra/per_layer'\n",
    "        \n",
    "        # Get all layer-wise intra metrics\n",
    "        layer_intra_metrics = [col for col in df_layer_intra.columns \n",
    "                              if any(prefix in col for prefix in ['PD_vs_GT_', 'FN_vs_GT_'])]\n",
    "        \n",
    "        print(f\"      Found {len(layer_intra_metrics)} layer-wise intra metrics to plot\")\n",
    "        \n",
    "        for metric in layer_intra_metrics:\n",
    "            # Create box plot by layer\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            \n",
    "            layers = df_layer_intra['Layer'].unique()\n",
    "            data_by_layer = [df_layer_intra[df_layer_intra['Layer'] == layer][metric].values \n",
    "                           for layer in layers]\n",
    "            \n",
    "            bp = ax.boxplot(data_by_layer, labels=layers, patch_artist=True)\n",
    "            \n",
    "            # Color boxes\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, len(layers)))\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            clean_metric = metric.replace('PD_vs_GT_', 'PD vs GT ').replace('FN_vs_GT_', 'FN vs GT ')\n",
    "            ax.set_title(f'Intra (Layer-wise) - {clean_metric}', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Layer', fontsize=12)\n",
    "            ax.set_ylabel(metric.split('_')[-1] if '_' in metric else metric, fontsize=12)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            save_path = intra_layer_folder / f\"{metric}.png\"\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            fig_list.append((f\"Intra Layer-wise - {metric}\", save_path))\n",
    "            print(f\"      ‚úÖ {metric}\")\n",
    "    \n",
    "    # 4. INTER PER LAYER - Individual metric plots\n",
    "    if df_layer_inter is not None and not df_layer_inter.empty:\n",
    "        print(f\"\\n   üìÅ Processing INTER metrics (per layer) - {len(df_layer_inter)} layer-experiments\")\n",
    "        inter_layer_folder = RESULTS_DIR / 'inter/per_layer'\n",
    "        \n",
    "        # Get all layer-wise inter metrics\n",
    "        layer_inter_metrics = [col for col in df_layer_inter.columns \n",
    "                              if 'PD_vs_FN_' in col]\n",
    "        \n",
    "        print(f\"      Found {len(layer_inter_metrics)} layer-wise inter metrics to plot\")\n",
    "        \n",
    "        for metric in layer_inter_metrics:\n",
    "            # Create box plot by layer\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            \n",
    "            layers = df_layer_inter['Layer'].unique()\n",
    "            data_by_layer = [df_layer_inter[df_layer_inter['Layer'] == layer][metric].values \n",
    "                           for layer in layers]\n",
    "            \n",
    "            bp = ax.boxplot(data_by_layer, labels=layers, patch_artist=True)\n",
    "            \n",
    "            # Color boxes\n",
    "            colors = plt.cm.plasma(np.linspace(0, 1, len(layers)))\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            clean_metric = metric.replace('PD_vs_FN_', 'PD vs FN ')\n",
    "            ax.set_title(f'Inter (Layer-wise) - {clean_metric}', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Layer', fontsize=12)\n",
    "            ax.set_ylabel(metric.split('_')[-1] if '_' in metric else metric, fontsize=12)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            save_path = inter_layer_folder / f\"{metric}.png\"\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            fig_list.append((f\"Inter Layer-wise - {metric}\", save_path))\n",
    "            print(f\"      ‚úÖ {metric}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {len(fig_list)} individual metric plots\")\n",
    "    return fig_list\n",
    "\n",
    "# Create organized visualizations\n",
    "has_dataframes = (\n",
    "    (df_intra is not None and not df_intra.empty) or\n",
    "    (df_inter is not None and not df_inter.empty) or\n",
    "    (df_layer_intra is not None and not df_layer_intra.empty) or\n",
    "    (df_layer_inter is not None and not df_layer_inter.empty)\n",
    ")\n",
    "\n",
    "if has_dataframes:\n",
    "    print(\"üîÑ Running organized visualization function...\")\n",
    "    fig_list = create_organized_visualizations(df_intra, df_inter, df_layer_intra, df_layer_inter)\n",
    "    \n",
    "    print(f\"\\nüìÅ Results saved to: {RESULTS_DIR}\")\n",
    "    print(\"üìÇ Subfolder structure:\")\n",
    "    for subfolder in ['inter/full_models', 'intra/full_models', 'inter/per_layer', 'intra/per_layer']:\n",
    "        folder_path = RESULTS_DIR / subfolder\n",
    "        if folder_path.exists():\n",
    "            num_files = len(list(folder_path.glob('*.png')))\n",
    "            print(f\"   üìÅ {subfolder}: {num_files} plots\")\n",
    "else:\n",
    "    print(\"‚ùå No data for visualizations\")\n",
    "    fig_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exporting Results ===\n",
      "‚úÖ Exported intra metrics to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/intra_metrics.csv\n",
      "‚úÖ Exported inter metrics to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/inter_metrics.csv\n",
      "‚úÖ Exported layer-wise intra metrics to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/layer_intra_metrics.csv\n",
      "‚úÖ Exported layer-wise inter metrics to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/layer_inter_metrics.csv\n",
      "‚úÖ Exported detailed results to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/detailed_results.json\n",
      "‚úÖ Exported summary statistics to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/summary_statistics.json\n",
      "\n",
      "‚úÖ Exported 6 files to /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results\n",
      "üìÅ /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/intra_metrics.csv\n",
      "üìÅ /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/inter_metrics.csv\n",
      "üìÅ /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/layer_intra_metrics.csv\n",
      "üìÅ /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/layer_inter_metrics.csv\n",
      "üìÅ /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/detailed_results.json\n",
      "üìÅ /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/summary_statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Export Results\n",
    "print(\"=== Exporting Results ===\")\n",
    "\n",
    "def export_results(df_intra, df_inter, df_layer_intra, df_layer_inter, analysis_results):\n",
    "    \"\"\"Export results to CSV and JSON formats\"\"\"\n",
    "    \n",
    "    export_files = []\n",
    "    \n",
    "    # Export DataFrames to CSV\n",
    "    if df_intra is not None and not df_intra.empty:\n",
    "        intra_csv = RESULTS_DIR / \"intra_metrics.csv\"\n",
    "        df_intra.to_csv(intra_csv, index=False)\n",
    "        export_files.append(str(intra_csv))\n",
    "        print(f\"‚úÖ Exported intra metrics to {intra_csv}\")\n",
    "    \n",
    "    if df_inter is not None and not df_inter.empty:\n",
    "        inter_csv = RESULTS_DIR / \"inter_metrics.csv\"\n",
    "        df_inter.to_csv(inter_csv, index=False)\n",
    "        export_files.append(str(inter_csv))\n",
    "        print(f\"‚úÖ Exported inter metrics to {inter_csv}\")\n",
    "    \n",
    "    if df_layer_intra is not None and not df_layer_intra.empty:\n",
    "        layer_intra_csv = RESULTS_DIR / \"layer_intra_metrics.csv\"\n",
    "        df_layer_intra.to_csv(layer_intra_csv, index=False)\n",
    "        export_files.append(str(layer_intra_csv))\n",
    "        print(f\"‚úÖ Exported layer-wise intra metrics to {layer_intra_csv}\")\n",
    "    \n",
    "    if df_layer_inter is not None and not df_layer_inter.empty:\n",
    "        layer_inter_csv = RESULTS_DIR / \"layer_inter_metrics.csv\"\n",
    "        df_layer_inter.to_csv(layer_inter_csv, index=False)\n",
    "        export_files.append(str(layer_inter_csv))\n",
    "        print(f\"‚úÖ Exported layer-wise inter metrics to {layer_inter_csv}\")\n",
    "    \n",
    "    # Export detailed results to JSON\n",
    "    if analysis_results:\n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        json_results = []\n",
    "        for result in analysis_results:\n",
    "            json_result = {}\n",
    "            for key, value in result.items():\n",
    "                if isinstance(value, dict):\n",
    "                    json_result[key] = {k: float(v) if isinstance(v, (np.float32, np.float64)) else v \n",
    "                                      for k, v in value.items() if not isinstance(v, list)}\n",
    "                else:\n",
    "                    json_result[key] = value\n",
    "            json_results.append(json_result)\n",
    "        \n",
    "        results_json = RESULTS_DIR / \"detailed_results.json\"\n",
    "        with open(results_json, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2, default=str)\n",
    "        export_files.append(str(results_json))\n",
    "        print(f\"‚úÖ Exported detailed results to {results_json}\")\n",
    "    \n",
    "    # Export summary statistics\n",
    "    if df_intra is not None and not df_intra.empty:\n",
    "        summary_stats = {\n",
    "            'total_experiments': len(df_intra),\n",
    "            'loss_types': df_intra['Loss_Type'].unique().tolist(),\n",
    "            'epoch_range': [int(df_intra['Epoch'].min()), int(df_intra['Epoch'].max())],\n",
    "            'mse_stats': {\n",
    "                'pd_vs_gt': {\n",
    "                    'mean': float(df_intra['PD_vs_GT_MSE'].mean()),\n",
    "                    'std': float(df_intra['PD_vs_GT_MSE'].std()),\n",
    "                    'min': float(df_intra['PD_vs_GT_MSE'].min()),\n",
    "                    'max': float(df_intra['PD_vs_GT_MSE'].max())\n",
    "                },\n",
    "                'fn_vs_gt': {\n",
    "                    'mean': float(df_intra['FN_vs_GT_MSE'].mean()),\n",
    "                    'std': float(df_intra['FN_vs_GT_MSE'].std()),\n",
    "                    'min': float(df_intra['FN_vs_GT_MSE'].min()),\n",
    "                    'max': float(df_intra['FN_vs_GT_MSE'].max())\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if df_inter is not None and not df_inter.empty:\n",
    "            summary_stats['mse_stats']['pd_vs_fn'] = {\n",
    "                'mean': float(df_inter['PD_vs_FN_MSE'].mean()),\n",
    "                'std': float(df_inter['PD_vs_FN_MSE'].std()),\n",
    "                'min': float(df_inter['PD_vs_FN_MSE'].min()),\n",
    "                'max': float(df_inter['PD_vs_FN_MSE'].max())\n",
    "            }\n",
    "        \n",
    "        summary_json = RESULTS_DIR / \"summary_statistics.json\"\n",
    "        with open(summary_json, 'w') as f:\n",
    "            json.dump(summary_stats, f, indent=2)\n",
    "        export_files.append(str(summary_json))\n",
    "        print(f\"‚úÖ Exported summary statistics to {summary_json}\")\n",
    "    \n",
    "    return export_files\n",
    "\n",
    "# Export results - Fixed DataFrame evaluation\n",
    "has_dataframes = (\n",
    "    (df_intra is not None and not df_intra.empty) or\n",
    "    (df_inter is not None and not df_inter.empty) or\n",
    "    (df_layer_intra is not None and not df_layer_intra.empty) or\n",
    "    (df_layer_inter is not None and not df_layer_inter.empty)\n",
    ") or (analysis_results is not None and len(analysis_results) > 0)\n",
    "\n",
    "if has_dataframes:\n",
    "    exported_files = export_results(df_intra, df_inter, df_layer_intra, df_layer_inter, analysis_results)\n",
    "    print(f\"\\n‚úÖ Exported {len(exported_files)} files to {RESULTS_DIR}\")\n",
    "    for file in exported_files:\n",
    "        print(f\"üìÅ {file}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Verification and Summary ===\n",
      "üîç ANALYSIS COMPLETENESS CHECK\n",
      "================================================================================\n",
      "üìä Tracking files discovered: 44\n",
      "üìä Tracking files processed: 44\n",
      "üìä Processing success rate: 100.0%\n",
      "üìä Ground truth data loaded: ‚úÖ\n",
      "üìä Ground truth models: 36468\n",
      "üìä Ground truth dimensions: 2464\n",
      "üìä Experiments analyzed: 44\n",
      "üìä Table 1 (Intra metrics): ‚úÖ 44 rows\n",
      "üìä Table 2 (Inter metrics): ‚úÖ 44 rows\n",
      "üìä Table 3 (Layer-wise intra): ‚úÖ 220 rows\n",
      "üìä Table 4 (Layer-wise inter): ‚úÖ 220 rows\n",
      "üìä Total tables created: 4/4\n",
      "üìä Layer boundaries: [(0, 208), (208, 1414), (1414, 1514), (1514, 2254), (2254, 2464)]\n",
      "üìä Number of layers: 5\n",
      "\n",
      "üéØ OVERALL STATUS:\n",
      "üü¢ ANALYSIS COMPLETE - All requirements met!\n",
      "üü¢ All 44 tracking files processed\n",
      "üü¢ All 4 results tables created\n",
      "üü¢ Layer-wise analysis completed\n",
      "üü¢ Comprehensive metrics computed\n",
      "üü¢ Results exported successfully\n",
      "\n",
      "üéâ ENHANCED CHECKPOINT EVALUATION AND METRICS ANALYSIS COMPLETE!\n",
      "üéâ Notebook successfully processed all tracking files with comprehensive metrics and layer-wise analysis!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Final Verification and Summary\n",
    "print(\"=== Final Verification and Summary ===\")\n",
    "\n",
    "def verify_analysis_completeness():\n",
    "    \"\"\"Verify completeness of the analysis\"\"\"\n",
    "    \n",
    "    print(\"üîç ANALYSIS COMPLETENESS CHECK\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check tracking files\n",
    "    tracking_found = len(tracking_files) if tracking_files else 0\n",
    "    tracking_processed = len(tracking_data) if tracking_data else 0\n",
    "    print(f\"üìä Tracking files discovered: {tracking_found}\")\n",
    "    print(f\"üìä Tracking files processed: {tracking_processed}\")\n",
    "    print(f\"üìä Processing success rate: {tracking_processed/tracking_found*100:.1f}%\" if tracking_found > 0 else \"üìä No tracking files found\")\n",
    "    \n",
    "    # Check ground truth data\n",
    "    gt_loaded = ground_truth_data is not None\n",
    "    print(f\"üìä Ground truth data loaded: {'‚úÖ' if gt_loaded else '‚ùå'}\")\n",
    "    if gt_loaded:\n",
    "        print(f\"üìä Ground truth models: {ground_truth_data['num_models']}\")\n",
    "        print(f\"üìä Ground truth dimensions: {ground_truth_data['weight_dim']}\")\n",
    "    \n",
    "    # Check analysis results\n",
    "    analysis_completed = len(analysis_results) if analysis_results else 0\n",
    "    print(f\"üìä Experiments analyzed: {analysis_completed}\")\n",
    "    \n",
    "    # Check results tables\n",
    "    tables_created = 0\n",
    "    if df_intra is not None and not df_intra.empty:\n",
    "        tables_created += 1\n",
    "        print(f\"üìä Table 1 (Intra metrics): ‚úÖ {len(df_intra)} rows\")\n",
    "    else:\n",
    "        print(f\"üìä Table 1 (Intra metrics): ‚ùå\")\n",
    "    \n",
    "    if df_inter is not None and not df_inter.empty:\n",
    "        tables_created += 1\n",
    "        print(f\"üìä Table 2 (Inter metrics): ‚úÖ {len(df_inter)} rows\")\n",
    "    else:\n",
    "        print(f\"üìä Table 2 (Inter metrics): ‚ùå\")\n",
    "    \n",
    "    if df_layer_intra is not None and not df_layer_intra.empty:\n",
    "        tables_created += 1\n",
    "        print(f\"üìä Table 3 (Layer-wise intra): ‚úÖ {len(df_layer_intra)} rows\")\n",
    "    else:\n",
    "        print(f\"üìä Table 3 (Layer-wise intra): ‚ùå\")\n",
    "    \n",
    "    if df_layer_inter is not None and not df_layer_inter.empty:\n",
    "        tables_created += 1\n",
    "        print(f\"üìä Table 4 (Layer-wise inter): ‚úÖ {len(df_layer_inter)} rows\")\n",
    "    else:\n",
    "        print(f\"üìä Table 4 (Layer-wise inter): ‚ùå\")\n",
    "    \n",
    "    print(f\"üìä Total tables created: {tables_created}/4\")\n",
    "    \n",
    "    # Check layer analysis\n",
    "    layer_bounds = get_layer_bounds()\n",
    "    print(f\"üìä Layer boundaries: {layer_bounds}\")\n",
    "    print(f\"üìä Number of layers: {len(layer_bounds)}\")\n",
    "    \n",
    "    # Check exports\n",
    "    if 'exported_files' in locals():\n",
    "        print(f\"üìä Files exported: {len(exported_files)}\")\n",
    "        for file in exported_files:\n",
    "            print(f\"üìÅ {file}\")\n",
    "    \n",
    "    # Overall status\n",
    "    print(\"\\nüéØ OVERALL STATUS:\")\n",
    "    if tracking_found >= 44 and tracking_processed >= 44 and gt_loaded and analysis_completed >= 44 and tables_created == 4:\n",
    "        print(\"üü¢ ANALYSIS COMPLETE - All requirements met!\")\n",
    "        print(\"üü¢ All 44 tracking files processed\")\n",
    "        print(\"üü¢ All 4 results tables created\")\n",
    "        print(\"üü¢ Layer-wise analysis completed\")\n",
    "        print(\"üü¢ Comprehensive metrics computed\")\n",
    "        print(\"üü¢ Results exported successfully\")\n",
    "    else:\n",
    "        print(\"üü° ANALYSIS PARTIAL - Some requirements may not be met\")\n",
    "        if tracking_found < 44:\n",
    "            print(f\"‚ö†Ô∏è  Expected 44 tracking files, found {tracking_found}\")\n",
    "        if tracking_processed < 44:\n",
    "            print(f\"‚ö†Ô∏è  Expected 44 processed files, got {tracking_processed}\")\n",
    "        if analysis_completed < 44:\n",
    "            print(f\"‚ö†Ô∏è  Expected 44 analyses, got {analysis_completed}\")\n",
    "        if tables_created < 4:\n",
    "            print(f\"‚ö†Ô∏è  Expected 4 tables, got {tables_created}\")\n",
    "    \n",
    "    return {\n",
    "        'tracking_found': tracking_found,\n",
    "        'tracking_processed': tracking_processed,\n",
    "        'ground_truth_loaded': gt_loaded,\n",
    "        'analysis_completed': analysis_completed,\n",
    "        'tables_created': tables_created,\n",
    "        'layer_bounds': layer_bounds\n",
    "    }\n",
    "\n",
    "# Run verification\n",
    "verification_results = verify_analysis_completeness()\n",
    "\n",
    "print(\"\\nüéâ ENHANCED CHECKPOINT EVALUATION AND METRICS ANALYSIS COMPLETE!\")\n",
    "print(\"üéâ Notebook successfully processed all tracking files with comprehensive metrics and layer-wise analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
