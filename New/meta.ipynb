{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571e8b99-8b1a-4389-95fa-ad8351993fe5",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626d13b6-b392-41f3-b537-d1d4acc5aa15",
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorCompatNotSupportedOnDevice: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#%load_ext cudf.pandas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#import pandas as pd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/cudf/__init__.py:15\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m libcudf\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcudf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpu_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_setup\n\u001b[0;32m---> 15\u001b[0m \u001b[43mvalidate_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m validate_setup\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcupy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/cudf/utils/gpu_utils.py:57\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CUDARuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m notify_caller_errors:\n\u001b[0;32m---> 57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# We must distinguish between \"CPU only\" and \"the driver is\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# insufficient for the runtime\".\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInsufficientDriver:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# cudaDriverGetVersion() returns 0 when ``libcuda.so`` is\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# missing. Otherwise there is a CUDA driver but it is\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;66;03m# insufficient for the runtime, so we re-raise the original\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# exception\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/cudf/utils/gpu_utils.py:54\u001b[0m, in \u001b[0;36mvalidate_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m notify_caller_errors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInitializationError,\n\u001b[1;32m     36\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorInvalidDeviceFunction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     cudaError_t\u001b[38;5;241m.\u001b[39mcudaErrorApiFailureBase,\n\u001b[1;32m     51\u001b[0m }\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     gpus_count \u001b[38;5;241m=\u001b[39m \u001b[43mgetDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CUDARuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m notify_caller_errors:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/rmm/_cuda/gpu.py:102\u001b[0m, in \u001b[0;36mgetDeviceCount\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m status, count \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mcudaGetDeviceCount()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mcudaError_t\u001b[38;5;241m.\u001b[39mcudaSuccess:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CUDARuntimeError(status)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m count\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorCompatNotSupportedOnDevice: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "#%load_ext cudf.pandas\n",
    "#import pandas as pd\n",
    "\n",
    "import cudf as pd\n",
    "import shap\n",
    "import gc\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, ConcatDataset\n",
    "import torchaudio\n",
    "\n",
    "import random\n",
    "import ast\n",
    "        \n",
    "import sys\n",
    "\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "from torch.optim import Adam ,SGD ,Adadelta\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import vjp\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation ,FFMpegWriter ,PillowWriter\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os \n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "import wandb\n",
    "\n",
    "\n",
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA tools\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "# ML tools\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from gtda.mapper.filter import Projection,Entropy,Eccentricity\n",
    "from gtda.mapper.cover import CubicalCover\n",
    "# scikit-learn method\n",
    "# giotto-tda method\n",
    "from gtda.mapper.cluster import FirstSimpleGap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='threadpoolctl')\n",
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "# Redirect stderr to null to suppress the exception messages\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "import ctypes\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress low-level warnings from C code\n",
    "libc = ctypes.CDLL(None)\n",
    "libc.prctl(15, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6526714d-c8dc-4253-93d8-bf37d614d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4d3a6-c181-46d2-9416-0020bbe03d40",
   "metadata": {},
   "source": [
    "### W&B login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcbf895-4f7c-4bbd-beb4-2397585e473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from Double_input_transformer import CustomDataset,TransformerAE\n",
    "wandb.login(key=\"ab631efc36e2c87f5f54d82b5cdbd6c501d5221f\")\n",
    "\n",
    "seed=271\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "import kaleido\n",
    "#kaleido.get_chrome_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0538a79-25e8-4568-9562-e13f9ef3e137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: multipers\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#os.environ[\"WANDB_MODE\"]= \"offline\"\n",
    "#!wandb sync\n",
    "!pip show multipers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd417af-04f2-4720-ae79-0b432fdb4cca",
   "metadata": {},
   "source": [
    "### Lazy load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd79b7b-32e1-451c-bd07-05f1e1642da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size, batch_limit=100):\n",
    "    batches = [lst[i:i + batch_size] for i in range(0, len(lst), batch_size)]\n",
    "    return batches[:batch_limit]\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "def get_plot_object(vec1, tg):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))  # Create figure and axis objects\n",
    "    canvas = FigureCanvas(fig)  # Create a canvas to hold the figure\n",
    "\n",
    "    # Convert tensors to NumPy and flatten\n",
    "    vec1_np = vec1.cpu().numpy().flatten()\n",
    "    tg_np = tg[0].cpu().numpy().flatten()\n",
    "\n",
    "    # Plot data\n",
    "    ax.plot(vec1_np, label='Predicted', alpha=0.7)\n",
    "    ax.plot(tg_np, label='Target', alpha=0.7)\n",
    "\n",
    "    # Labels and legend\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Neurone Index')\n",
    "    ax.set_ylabel('Neurone Value')\n",
    "    ax.set_title('Plot of Predicted vs Target')\n",
    "\n",
    "    return fig \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281e8c3-6af5-4981-85ef-9746fe77b3b6",
   "metadata": {},
   "source": [
    "### Example of static Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1404361f-844e-40b2-a26b-771563de45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, seed=22):\n",
    "        super().__init__()\n",
    "        #print(\"EmbedderNeuroneGroup\")\n",
    "        self.neuron_l1 = nn.Linear(200, d_model) #8\n",
    "        self.neuron_l2 = nn.Linear(72, d_model) #12\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        #print(\"multi-linear method\",v.shape)\n",
    "\n",
    "        l = []\n",
    "\n",
    "        for ndx in range(8):\n",
    "            idx_start = ndx * 200\n",
    "            idx_end = idx_start + 200\n",
    "            l.append(self.neuron_l1(v[:,idx_start:idx_end]))\n",
    "\n",
    "        # l2\n",
    "        for ndx in range(12):\n",
    "            idx_start = 200*8 + ndx * 72\n",
    "            idx_end = idx_start + 72\n",
    "            l.append(self.neuron_l2(v[:,idx_start:idx_end]))\n",
    "        #print(len(l))\n",
    "        #print(len(l[0]))\n",
    "        final = torch.stack(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff64df2-22ea-4c27-8d3f-2833f0b20736",
   "metadata": {},
   "source": [
    "### Label specific DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f18e937-5d09-4098-9701-83ec55f3938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "class ClassSpecificImageFolder(datasets.DatasetFolder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root,\n",
    "            dropped_classes=[],\n",
    "            transform = None,\n",
    "            target_transform = None,\n",
    "            loader = datasets.folder.default_loader,\n",
    "            is_valid_file = None,\n",
    "    ):\n",
    "        self.dropped_classes = dropped_classes\n",
    "        super(ClassSpecificImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                                       transform=transform,\n",
    "                                                       target_transform=target_transform,\n",
    "                                                       is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "        classes = [c for c in classes if c not in self.dropped_classes]\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361251f1-0664-4baf-8730-8c77bd82e8ea",
   "metadata": {},
   "source": [
    "### CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f6ecd6-9f56-43cd-ad30-30219cb0965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in,\n",
    "        nlin=\"leakyrelu\",\n",
    "        dropout=0.0,\n",
    "        init_type=\"uniform\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init module list\n",
    "        self.module_list = nn.ModuleList()\n",
    "        ### ASSUMES 28x28 image size\n",
    "        ## compose layer 1\n",
    "        self.module_list.append(nn.Conv2d(channels_in, 8, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        # apply dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 2\n",
    "        self.module_list.append(nn.Conv2d(8, 6, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 3\n",
    "        self.module_list.append(nn.Conv2d(6, 4, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add flatten layer\n",
    "        self.module_list.append(nn.Flatten())\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(3 * 3 * 4, 20))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(20, 10))\n",
    "\n",
    "        ### initialize weights with se methods\n",
    "        self.initialize_weights(init_type)\n",
    "\n",
    "    def initialize_weights(self, init_type):\n",
    "        # print(\"initialze model\")\n",
    "        for m in self.module_list:\n",
    "            if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "                if init_type == \"xavier_uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if init_type == \"xavier_normal\":\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                if init_type == \"uniform\":\n",
    "                    torch.nn.init.uniform_(m.weight)\n",
    "                if init_type == \"normal\":\n",
    "                    torch.nn.init.normal_(m.weight)\n",
    "                if init_type == \"kaiming_normal\":\n",
    "                    torch.nn.init.kaiming_normal_(m.weight)\n",
    "                if init_type == \"kaiming_uniform\":\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight)\n",
    "                # set bias to some small non-zero value\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    def get_nonlin(self, nlin):\n",
    "        # apply nonlinearity\n",
    "        if nlin == \"leakyrelu\":\n",
    "            return nn.LeakyReLU()\n",
    "        if nlin == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        if nlin == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        if nlin == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        if nlin == \"silu\":\n",
    "            return nn.SiLU()\n",
    "        if nlin == \"gelu\":\n",
    "            return nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward prop through module_list\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward_activations(self, x):\n",
    "        # forward prop through module_list\n",
    "        activations = []\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "            if (\n",
    "                isinstance(layer, nn.Tanh)\n",
    "                or isinstance(layer, nn.Sigmoid)\n",
    "                or isinstance(layer, nn.ReLU)\n",
    "                or isinstance(layer, nn.LeakyReLU)\n",
    "                or isinstance(layer, nn.SiLU)\n",
    "                or isinstance(layer, nn.GELU)\n",
    "                or isinstance(layer, ORU)\n",
    "                or isinstance(layer, ERU)\n",
    "            ):\n",
    "                activations.append(x)\n",
    "        return x, activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e53c5-70f6-4b35-bcfe-c8f7146766ed",
   "metadata": {},
   "source": [
    "### Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7edc56-5b7c-467a-9ea8-de5427e2e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion,nb_classes,First=False,df=None,verbose=False,log_freq_steps=25):\n",
    "    List_mx=[]\n",
    "    model.train()\n",
    "    print(f'Training {str(datetime.datetime.now())}')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image\n",
    "        labels = labels\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        #mx=multiclass_confusion_matrix(preds ,labels,nb_classes,normalize=\"pred\")\n",
    "        #List_mx.append(mx)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "        if First==True and i%log_freq_steps==0 :\n",
    "            epoch_loss = train_running_loss / counter\n",
    "            epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "            if verbose==True:\n",
    "                print(epoch_acc,\"%\")\n",
    "            #print(f\"step {i}:\",epoch_loss, epoch_acc)\n",
    "            if type(df)!='NoneType':\n",
    "                df.at[track,f\"Step {i}\"]=epoch_acc\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b74c27-45ee-4d70-84d1-903d07db1b71",
   "metadata": {},
   "source": [
    "### Validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e36f76-d9bf-4004-82b0-e0faef41e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testloader, criterion, nb_classes,epoch,vec):\n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    correct_counts = {str(i): 0 for i in range(nb_classes+1)}\n",
    "    total_counts = {str(i): 0 for i in range(nb_classes+1)}\n",
    "    #shapley_values_per_class = {str(i): [] for i in range(nb_classes)}\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            if data is None:\n",
    "                continue\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "            # Update correct/total counts\n",
    "            for j in range(len(labels)):\n",
    "                class_label = str(labels[j].item())\n",
    "                total_counts[class_label] += 1\n",
    "                if preds[j] == labels[j]:\n",
    "                    correct_counts[class_label] += 1\n",
    "            for j in range(len(labels)):\n",
    "                class_label = str(labels[j].item())\n",
    "                #shapley_values_per_class[class_label].append(np.mean(shap_values[j].numpy()))\n",
    "    \n",
    "    # Compute final metrics\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "\n",
    "    wandb.log({\n",
    "        f\"Epoch {epoch} Loss batch {vec} \": epoch_loss,\n",
    "        f\"Epoch{epoch}  Accuracy {vec}\": epoch_acc})\n",
    "    return epoch_loss, epoch_acc,correct_counts,total_counts\n",
    "def create_frame(step,ax,data):\n",
    "    ax=ax.cla()\n",
    "    sns.heatmap(data[step][-1].cpu(),annot=True,cmap=\"cubehelix\",ax=ax,cbar=False)\n",
    "    plt.title('Epoch {} training {}'.format(step,exp)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6ff12-f499-4278-8b00-bb0fe7a6f9a6",
   "metadata": {},
   "source": [
    "### Record-Tracking Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0972f8-c072-4c1b-97c8-091fd923a99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label task 1</th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy task1</th>\n",
       "      <th>label task 2</th>\n",
       "      <th>Accuracy task2</th>\n",
       "      <th>weight 0</th>\n",
       "      <th>weight 1</th>\n",
       "      <th>weight 2</th>\n",
       "      <th>weight 3</th>\n",
       "      <th>weight 4</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Loader Set</th>\n",
       "      <th>Reconstructed Accuracy ID</th>\n",
       "      <th>Actual Accuracy</th>\n",
       "      <th>Reconstructed Accuracy OOD</th>\n",
       "      <th>Transformer Loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochCNN</th>\n",
       "      <th>ActivationCNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 2477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label task 1, index, Accuracy task1, label task 2, Accuracy task2, weight 0, weight 1, weight 2, weight 3, weight 4, weight 5, weight 6, weight 7, weight 8, weight 9, weight 10, weight 11, weight 12, weight 13, weight 14, weight 15, weight 16, weight 17, weight 18, weight 19, weight 20, weight 21, weight 22, weight 23, weight 24, weight 25, weight 26, weight 27, weight 28, weight 29, weight 30, weight 31, weight 32, weight 33, weight 34, weight 35, weight 36, weight 37, weight 38, weight 39, weight 40, weight 41, weight 42, weight 43, weight 44, weight 45, weight 46, weight 47, weight 48, weight 49, weight 50, weight 51, weight 52, weight 53, weight 54, weight 55, weight 56, weight 57, weight 58, weight 59, weight 60, weight 61, weight 62, weight 63, weight 64, weight 65, weight 66, weight 67, weight 68, weight 69, weight 70, weight 71, weight 72, weight 73, weight 74, weight 75, weight 76, weight 77, weight 78, weight 79, weight 80, weight 81, weight 82, weight 83, weight 84, weight 85, weight 86, weight 87, weight 88, weight 89, weight 90, weight 91, weight 92, weight 93, weight 94, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 2477 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cols=[\"label task 1\",\"index\",\"Accuracy task1\",\\\n",
    "      \"label task 2\",\"Accuracy task2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Loader Set\",\"Reconstructed Accuracy ID\",\"Actual Accuracy\",\"Reconstructed Accuracy OOD\",\"Transformer Loss\",\"lr\",'epochCNN','ActivationCNN'] \n",
    "\n",
    "print(len(Cols))\n",
    "predicted_Weights= pd.DataFrame(columns=Cols)\n",
    "\n",
    "# row=[\"\".format(task1),int(ind[0]),ACC[0],\"\".format(task2),ACC[1]]+vector_aux.to_list()+[\"train\",valid_epoch_acc0,ACC[2],valid_epoch_acc1,L_train[-1]]\n",
    "# predicted_Weights.append(row, ignore_index=True)\n",
    "predicted_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "348444f9-c667-4a52-a173-9d002ad8a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7745\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "Cols=[\"label task 1\",\"label task 2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(200)]+[\"Pred bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(208,1408)]+[\"Pred bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1414,1510)]+[\"Pred bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1514,2234)]+[\"Pred bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(2254,2454)]+[\"Pred bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(200)]+[\"FN bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(208,1408)]+[\"FN bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1414,1510)]+[\"FN bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1514,2234)]+[\"FN bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(2254,2454)]+[\"FN bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "[\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KLD\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "[\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "[\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "[\"WSD FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\\n",
    "[\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\",\"epoch 0\",\"epoch 1\",\"epoch 2\"]+\\\n",
    "[\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]+[\"Representation Z{}\".format(x) for x in range(256)]+[\"Z1 L2\",\"Z1 L2\",\"z1 L2\",\"z2 L2\"]\n",
    "#incoporate Norm                                                     \n",
    "#The Fisher information metric X norm of the target  \n",
    "#dataset distance , topological distance ,UMAP distance\n",
    "\n",
    "print(len(Cols))\n",
    "DF= pd.DataFrame(columns=Cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c8fee0-b462-4e95-bf4c-8f0d59ebd389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(type(str(datetime.datetime.now())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96aceb-559d-4c9f-9226-c4b4aabd789d",
   "metadata": {},
   "source": [
    "## Metrics to Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9a53f-af34-4a20-af0e-6efe05a00aff",
   "metadata": {},
   "source": [
    "### Frobenius Norm of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18091770-a855-492a-a5f7-6f38209b5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobnorm(mod):\n",
    "    S=0\n",
    "    for l in mod.parameters():\n",
    "        S += (l.grad**2).sum()\n",
    "    S = S**0.5\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8130b9-a60b-481e-af64-b567eab779d9",
   "metadata": {},
   "source": [
    "### Q-quantile loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc9c11c-fa7e-42fe-a20a-ed12c3a882c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_quantile_loss(pred, target, q):\n",
    "    error = target - pred\n",
    "    return torch.mean(torch.max(q * error, (q - 1) * error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5ba41-ee9b-454d-bdf0-991ef2cf258c",
   "metadata": {},
   "source": [
    "### Norm of Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f0336f-70f2-478a-be27-551d72da68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frobenius_norm_jacobian(model, x1,x2):\n",
    "    x1 = x1.clone().detach().requires_grad_(True)\n",
    "    x2 = x2.clone().detach().requires_grad_(True)\n",
    "    y = model(x1,x2)\n",
    "    jacobian = []\n",
    "    for i in range(y[0].shape[0]):\n",
    "        grad_outputs = torch.zeros_like(y[0])\n",
    "        grad_outputs[:, i] = 1\n",
    "        grad = torch.autograd.grad(outputs=y[0], inputs=(x1,x2), grad_outputs=grad_outputs,\n",
    "                                   retain_graph=True, create_graph=True)[0]\n",
    "        jacobian.append(grad)\n",
    "    jacobian = torch.stack(jacobian, dim=1)\n",
    "    frobenius_norm = torch.norm(jacobian, p='fro')\n",
    "    return frobenius_norm.detach().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce967536-89b9-45f6-8025-6c232ffe9910",
   "metadata": {},
   "source": [
    "### Fisher Information Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d904872-e4f1-45b3-9854-da6b1434abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def kernel_density_estimate(vector):\n",
    "    kde = gaussian_kde(vector)\n",
    "    return kde\n",
    "\n",
    "def finite_difference_gradient(x, kde, h=1e-5):\n",
    "    pdf_value_plus_h = kde.evaluate(x + h)\n",
    "    pdf_value_minus_h = kde.evaluate(x - h)\n",
    "    gradient = (np.log(pdf_value_plus_h) - np.log(pdf_value_minus_h)) / (2 * h)\n",
    "    return gradient\n",
    "\n",
    "def score_function(x, kde):\n",
    "    return finite_difference_gradient(x, kde)\n",
    "\n",
    "def fisher_information_integral(kde, xmin, xmax):\n",
    "    def integrand(x):\n",
    "        score = score_function(x, kde)\n",
    "        return score**2\n",
    "    \n",
    "    fisher_info, _ = quad(integrand, xmin, xmax)\n",
    "    return fisher_info\n",
    "\n",
    "def fisher_information(vector):\n",
    "    kde = kernel_density_estimate(vector)\n",
    "    xmin, xmax = np.min(vector), np.max(vector)\n",
    "    fisher_info = fisher_information_integral(kde, xmin, xmax)\n",
    "    return fisher_info\n",
    "\n",
    "def fisher_information_difference(vec1, vec2):\n",
    "    fisher_info_1 = fisher_information(vec1)\n",
    "    fisher_info_2 = fisher_information(vec2)\n",
    "    \n",
    "    fisher_info_diff = np.abs(fisher_info_1 - fisher_info_2)\n",
    "    return fisher_info_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907af62-a5d1-4e39-bb5f-0fe1921913e5",
   "metadata": {},
   "source": [
    "## Losses To Train On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb7657-d5e1-4b67-a1b4-17307c7acfd8",
   "metadata": {},
   "source": [
    "### Contractive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd3c9034-bbea-431a-a479-5f39df49e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def loss_Contractive(W, x, recons_x, h, lam):\n",
    "    dh = h * (1 - h) \n",
    "\n",
    "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
    "\n",
    "    w_sum = w_sum.unsqueeze(1) # shape N_hidden x 1\n",
    " \n",
    "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "\n",
    "    return contractive_loss.mul_(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef15a8-b6e2-4d13-a67c-9012da54e6ea",
   "metadata": {},
   "source": [
    "### Wasserstein Distance/Geomloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c80da9-82b9-4ba3-a0bb-7df722ab1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Compiling cuda jit compiler engine ... \n",
      "[KeOps] Warning : There were warnings or errors :\n",
      "/usr/bin/ld: cannot find -lnvrtc: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "\n",
      "OK\n",
      "[pyKeOps] Compiling nvrtc binder for python ... \n",
      "[KeOps] Warning : There were warnings or errors :\n",
      "/usr/bin/ld: cannot find -lnvrtc: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "\n",
      "OK\n",
      "tensor(204.8559) 0.005008445931719497\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "# Convert to numpy arrays\n",
    "# vec1_np = vec1.numpy()\n",
    "# vec2_np = vec2.numpy()\n",
    "\n",
    "# # Compute Wasserstein distance for each pair of vectors\n",
    "# wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "\n",
    "# # If you need an aggregate measure, you can compute the average distance\n",
    "# average_wsd = sum(wsd_list) / len(wsd_list)\n",
    "# average_wsd\n",
    "\n",
    "\n",
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "vec1 = torch.rand(1,2464)\n",
    "vec2 = torch.rand(1,2464)\n",
    "LossWS=SamplesLoss('sinkhorn')\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "# Convert to numpy arrays\n",
    "vec1_np = vec1[0].numpy()\n",
    "vec2_np = vec2[0].numpy()\n",
    "print(LossWS(vec1,vec2), wasserstein_distance(vec1_np,vec2_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175cbd8-acb0-450f-928a-45c0e45e683b",
   "metadata": {},
   "source": [
    "### Q-quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b13139-72cb-4534-b7ae-3ba6de4bde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(torch.nn.Module):\n",
    "    def __init__(self, q):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the quantile loss for each prediction\n",
    "        error = y_true - y_pred\n",
    "        loss = torch.maximum(self.q * error, (self.q - 1) * error)\n",
    "        return loss.mean()\n",
    "quantile = 0.5\n",
    "q_quantile_loss = QuantileLoss(q=quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c5ee6-2897-4eb8-87fd-5e7188147c86",
   "metadata": {},
   "source": [
    "### Difference in Norm of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f9fc6e9-bd8f-4296-8947-b329d655bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NormDifferenceDistance(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormDifferenceDistance, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate L2 norms for each tensor\n",
    "        norm_pred = torch.norm(y_pred, p=2)\n",
    "        norm_true = torch.norm(y_true, p=2)\n",
    "        \n",
    "        # Compute the absolute difference of norms as the distance\n",
    "        distance = torch.abs(norm_pred - norm_true)\n",
    "        return distance\n",
    "\n",
    "class StandardizedNormDifferenceDistance(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardizedNormDifferenceDistance, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Normalize each tensor by its L2 norm\n",
    "        norm_pred = torch.norm(y_pred, p=2)\n",
    "        norm_true = torch.norm(y_true, p=2)\n",
    "        y_pred_normalized = y_pred / (norm_pred + 1e-8)\n",
    "        y_true_normalized = y_true / (norm_true + 1e-8)\n",
    "        \n",
    "        # Calculate L2 distance between normalized tensors\n",
    "        distance = torch.norm(y_pred_normalized - y_true_normalized, p=2)\n",
    "        return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e4ac5-2aa5-48f7-be89-b90f3831fce4",
   "metadata": {},
   "source": [
    "### Auto-regressive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6002e172-706f-4828-8a83-17331a0a5c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.072986602783203\n"
     ]
    }
   ],
   "source": [
    "class AutoregressiveMSELoss(nn.Module):\n",
    "    def __init__(self, lambda_weights=None, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Autoregressive MSE Loss with weighted contributions.\n",
    "\n",
    "        Args:\n",
    "            lambda_weights (list or None): Weights for the autoregressive components.\n",
    "                                           If None, equal weights are used.\n",
    "            epsilon (float): A small constant added to denominators for numerical stability.\n",
    "        \"\"\"\n",
    "        super(AutoregressiveMSELoss, self).__init__()\n",
    "        self.lambda_weights = lambda_weights\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predictions, targets, delimiters):\n",
    "        \"\"\"\n",
    "        Compute the autoregressive MSE loss.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted tensor of shape [batch_size, total_length].\n",
    "            targets (torch.Tensor): Target tensor of shape [batch_size, total_length].\n",
    "            delimiters (list of ints): End indices for chunks, defining tensor splits.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        n_chunks = len(delimiters)\n",
    "        mse_losses = []\n",
    "        start = 0\n",
    "\n",
    "        # Compute MSE loss for each chunk\n",
    "        for end in delimiters:\n",
    "            pred_chunk = predictions[:, start:end]\n",
    "            target_chunk = targets[:, start:end]\n",
    "            mse_loss = nn.functional.mse_loss(pred_chunk, target_chunk, reduction='mean')\n",
    "            mse_losses.append(mse_loss)\n",
    "            start = end\n",
    "\n",
    "        # Set default weights if not provided\n",
    "        if self.lambda_weights is None:\n",
    "            self.lambda_weights = [1.0] * n_chunks\n",
    "\n",
    "        if len(self.lambda_weights) != n_chunks:\n",
    "            raise ValueError(\"lambda_weights must have the same length as the number of chunks.\")\n",
    "\n",
    "        # Compute the weighted autoregressive loss\n",
    "        autoregressive_loss = mse_losses[0]  # Start with the first chunk\n",
    "        for i in range(1, n_chunks):\n",
    "            autoregressive_loss += (\n",
    "                self.lambda_weights[i - 1] * (mse_losses[i] / (mse_losses[i - 1] + self.epsilon))\n",
    "            )\n",
    "        \n",
    "        return autoregressive_loss\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# Suppose the sequence is split into chunks based on delimiters\n",
    "predictions = torch.randn(10, 2464)  # [batch_size, total_length]\n",
    "targets = torch.randn(10, 2464)     # [batch_size, total_length]\n",
    "delimiters = [208, 1414, 1514, 2254, 2464]\n",
    "\n",
    "# Initialize the loss function with custom weights\n",
    "loss_fn = AutoregressiveMSELoss(lambda_weights=[1,0.5, 0.7, 0.9, 1.1])\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(predictions, targets, delimiters)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae9a9c-1ce4-46e8-bba3-2482e0d6687b",
   "metadata": {},
   "source": [
    "### MAPE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3df66f0-856e-4ab5-a09e-1962f615f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MAPE, self).__init__()\n",
    "        def forward(self, vec1,vec2):\n",
    "            loss = torch.mean(torch.abs((vec1 - vec2) / vec2))\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b69774-e0da-4f91-a4cb-12267f26aa68",
   "metadata": {},
   "source": [
    "### Layer-wise-loss Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1951a27-7bde-4b31-895e-185547c65875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWLN_loss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(LWLN_loss, self).__init__()\n",
    "        def forward(self, vec1,vec2):\n",
    "            loss = (torch.mean((vec1[:,0:208]-vec2[:,0:208])**2)/vec2[:,0:208].std() + \n",
    "                     torch.mean((vec1[:,208:1414]-vec2[:,208:1414])**2)/vec2[:,208:1414].std()+ \n",
    "                     torch.mean((vec1[:,1414:1514]-vec2[:,1414:1514])**2)/vec2[:,1414:1514].std()+\n",
    "                     torch.mean((vec1[:,1514:2254]-vec2[:,1514:2254])**2)/vec2[:,1514:2254].std()+\n",
    "                     torch.mean((vec1[:,2254:2464]-vec2[:,2254:2464])**2)/vec2[:,2254:2464].std())/(6)\n",
    "            return loss\n",
    "\n",
    "class LWLN_loss_single(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LWLN_loss_single, self).__init__()\n",
    "    def forward(self, vec1,vec2):\n",
    "        loss = (torch.mean((vec1[0:208]-vec2[0:208])**2)/vec2[0:208].std() + \n",
    "                 torch.mean((vec1[208:1414]-vec2[208:1414])**2)/vec2[208:1414].std()+ \n",
    "                 torch.mean((vec1[1414:1514]-vec2[1414:1514])**2)/vec2[1414:1514].std()+\n",
    "                 torch.mean((vec1[1514:2254]-vec2[1514:2254])**2)/vec2[1514:2254].std()+\n",
    "                 torch.mean((vec1[2254:2464]-vec2[2254:2464])**2)/vec2[2254:2464].std())/(6)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdfdae-5225-4786-b050-a90d2ad16160",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ce812c5-4f09-49ba-89f3-89dd88c05e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class JensenShannonLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-8):\n",
    "        super(JensenShannonLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, p_logits, q_logits):\n",
    "        # Apply softmax to get probability distributions\n",
    "        p = F.softmax(p_logits, dim=1)\n",
    "        q = F.softmax(q_logits, dim=1)\n",
    "        \n",
    "        # Mixture distribution M = 0.5*(P + Q)\n",
    "        m = 0.5 * (p + q)\n",
    "        \n",
    "        # Compute KL(P || M) and KL(Q || M)\n",
    "        # Add eps for numerical stability in log\n",
    "        kl_pm = F.kl_div(\n",
    "            torch.log(m + self.eps),  # log(M)\n",
    "            p,                        # P (as probabilities)\n",
    "            reduction='none',\n",
    "            log_target=False\n",
    "        )\n",
    "        kl_qm = F.kl_div(\n",
    "            torch.log(m + self.eps),  # log(M)\n",
    "            q,                        # Q (as probabilities)\n",
    "            reduction='none',\n",
    "            log_target=False\n",
    "        )\n",
    "        \n",
    "        # Sum over classes (dim=1) to get per-sample KL divergences\n",
    "        kl_pm = kl_pm.sum(dim=1)\n",
    "        kl_qm = kl_qm.sum(dim=1)\n",
    "        \n",
    "        # Jensen-Shannon divergence: 0.5 * (KL(P||M) + KL(Q||M))\n",
    "        js = 0.5 * (kl_pm + kl_qm)\n",
    "        \n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return js.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return js.sum()\n",
    "        else:  # 'none'\n",
    "            return js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76062e03-53c1-4f84-9f15-8ecd179c30f2",
   "metadata": {},
   "source": [
    "### Frobenius Norm Jacobian (Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9ef0ee3-2548-4156-8856-52f77fe5a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrobeniusNormJacobian(nn.Module):\n",
    "    def __init__(self, num_samples=10):\n",
    "        \"\"\"\n",
    "        Custom loss module that calculates the Frobenius norm approximation\n",
    "        of the Jacobian with respect to two inputs using Hutchinson's method.\n",
    "        \n",
    "        Parameters:\n",
    "            num_samples (int): Number of random vectors to sample for the approximation.\n",
    "        \"\"\"\n",
    "        super(FrobeniusNormJacobian, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def forward(self, model, inputs1, inputs2):\n",
    "        \"\"\"\n",
    "        Forward pass to calculate the approximate Frobenius norm of the Jacobian.\n",
    "        \n",
    "        Parameters:\n",
    "            model (torch.nn.Module): The model whose Jacobian's Frobenius norm we want to approximate.\n",
    "            inputs1 (torch.Tensor): The first input tensor for which the Jacobian is computed.\n",
    "            inputs2 (torch.Tensor): The second input tensor for which the Jacobian is computed.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The approximate Frobenius norm of the Jacobian as a loss.\n",
    "        \"\"\"\n",
    "        frobenius_norm_estimate = 0.0\n",
    "        inputs1.requires_grad_(True)\n",
    "        inputs2.requires_grad_(True)\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # Generate a random vector with the same shape as model output\n",
    "            random_vec = torch.randn_like(model(inputs1, inputs2)[0])\n",
    "            \n",
    "            # Compute the vector-Jacobian product for each input\n",
    "            _, vjps1 = vjp(lambda x: model(x, inputs2)[0], inputs1, v=random_vec)\n",
    "            _, vjps2 = vjp(lambda y: model(inputs1, y)[0], inputs2, v=random_vec)\n",
    "            \n",
    "            # Sum up the squared vjp norm for both inputs\n",
    "            frobenius_norm_estimate += (vjps1.norm() ** 2 + vjps2.norm() ** 2)\n",
    "        \n",
    "        # Average over the number of samples and take square root\n",
    "        frobenius_norm_estimate = (frobenius_norm_estimate / self.num_samples) ** 0.5\n",
    "        return frobenius_norm_estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352ea4c-8ac4-4959-9d72-7de360e27b5b",
   "metadata": {},
   "source": [
    "### Audio/Spectral Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f032ac-1e90-46d3-b5e3-f4e4204ec7f4",
   "metadata": {},
   "source": [
    "#### Fourrier transform difference in Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21266a1c-2b42-4b4e-a5da-d011ec14d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTLoss(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        x_fft = torch.fft.fft(x)\n",
    "        y_fft = torch.fft.fft(y)\n",
    "        return torch.mean(torch.abs(x_fft - y_fft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757c9e5-ade2-4128-9333-bb1342b9c0a4",
   "metadata": {},
   "source": [
    "#### Mel spectrogram Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24de8133-d64c-4f00-8516-748eca7ea307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MelSpecL2Loss(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, n_fft=1024, hop_length=256, n_mels=64):\n",
    "        super().__init__()\n",
    "        self.mel = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        m1 = self.mel(x.squeeze(1))\n",
    "        m2 = self.mel(y.squeeze(1))\n",
    "        return F.mse_loss(m1, m2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d007cf1-6d36-42ae-83ec-33772aaa9640",
   "metadata": {},
   "source": [
    "#### Mel Spectrogram Frechet inception distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4ef7b2e-62e2-45b6-9dda-0c05f9191eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtm_newton_schulz(A, num_iter=10):\n",
    "    B, N, _ = A.shape\n",
    "    normA = A.norm(dim=(1, 2), keepdim=True)\n",
    "    Y = A / normA\n",
    "    I = torch.eye(N, device=A.device).unsqueeze(0).expand(B, -1, -1)\n",
    "    Z = I.clone()\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        T = 0.5 * (3.0 * I - Z @ Y)\n",
    "        Y = Y @ T\n",
    "        Z = T @ Z\n",
    "    return Y * torch.sqrt(normA)\n",
    "\n",
    "class MelFIDLoss(nn.Module):\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.mel = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        m1 = self.mel(x.squeeze(1))  # [B, mel, T]\n",
    "        m2 = self.mel(y.squeeze(1))\n",
    "        f1 = m1.transpose(1, 2)  # [B, T, mel]\n",
    "        f2 = m2.transpose(1, 2)\n",
    "        mu1 = f1.mean(dim=1)\n",
    "        mu2 = f2.mean(dim=1)\n",
    "        c1 = (f1 - mu1.unsqueeze(1)).transpose(1, 2) @ (f1 - mu1.unsqueeze(1)) / (f1.shape[1] - 1)\n",
    "        c2 = (f2 - mu2.unsqueeze(1)).transpose(1, 2) @ (f2 - mu2.unsqueeze(1)) / (f2.shape[1] - 1)\n",
    "        sqrt_cov = sqrtm_newton_schulz(c1 @ c2)\n",
    "        trace_term = torch.diagonal(c1 + c2 - 2 * sqrt_cov, dim1=1, dim2=2).sum(dim=1)\n",
    "        mean_term = (mu1 - mu2).pow(2).sum(dim=1)\n",
    "        return (mean_term + trace_term).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762159a-793b-4793-a562-a16bbe8dac68",
   "metadata": {},
   "source": [
    "## List of Losses to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8a35d7f-6592-4653-99ca-6746a11ebf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sinkhorn', 'MAPE', 'MAE', 'MSE', 'AUTO', 'KL divergence', 'Mel L2', 'Mel FID', 'FFT Loss', 'LWLN', 'LWWS', 'Latent', 'Forb_norm', 'CAE', 'JS Loss', 'ws_scipy', 'gw_loss', 'LWWS_scipy', 'Q-quantile_loss', 'FIM', 'log-norm']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Initial list of losses\n",
    "Losses=[\"sinkhorn\",\"MAPE\",\"MAE\",\"MSE\",\"AUTO\",\n",
    "            \"KL divergence\",\n",
    "            \"Mel L2\",\"Mel FID\",\"FFT Loss\",\n",
    "               \"LWLN\",\"LWWS\",\"Latent\",\"Forb_norm\",\"CAE\",\"JS Loss\", \"ws_scipy\",\n",
    "               \"gw_loss\",\"LWWS_scipy\",\n",
    "              \"Q-quantile_loss\",\"FIM\",\"log-norm\"]\n",
    "all_losses=Losses\n",
    "print(all_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8966548-1ee0-46be-985b-5e6295a2ab0a",
   "metadata": {},
   "source": [
    "## Method to update all metrics+losses onto wandb after a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aedd28ce-a731-4c77-be5b-b14433c8013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses=[\"MAPE\",\"MAE\",\"MSE\",\"Latent\",\"sinkhorn\",\n",
    "        \"gw_loss\",\"Mel L2\",\"Mel FID\",\"FFT Loss\",\n",
    "        \"ws_scipy\",\"CAE\",\"Q-quantile_loss\",\"LWLN\",\"LWWS\",\n",
    "        \"FIM\",\"log-norm\",\"AUTO\",\"KL divergence\",\"Forb_norm\",\"JS Loss\",\n",
    "        \"LWWS_scipy\",\"ws_scipy\"]\n",
    "        #py_riemanian , geodesic\n",
    "from torch.nn.functional import pairwise_distance\n",
    "#from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "vec1 = torch.rand(1,2464).cuda()\n",
    "vec2 = torch.rand(1,2464).cuda()\n",
    "\n",
    "def update_all_metrics(d_loos,mod,vec1,vec2,tg,DF):\n",
    "    Losses=[\"Mel L2\",\"Mel FID\",\"FFT Loss\",\"JS Loss\",\"MAE\",\"MSE\",\"Latent\",\"MAPE\",\"sinkhorn\",\"gw_loss\",\"ws_scipy\",\"CAE\",\"Q-quantile_loss\",\"LWLN\",\"LWWS\",\"FIM\",\"log-norm\",\"AUTO\",\"KL divergence\",\"Forb_norm\",\"LWWS_scipy\",\"ws_scipy 0.9\",\"ws_scipy\"]\n",
    "    Lambda=0.05\n",
    "    #with torch.autocast(\"cuda\"):\n",
    "    output=mod(vec1,vec2)\n",
    "    for Loss in Losses:\n",
    "        if Loss==\"FFT Loss\":\n",
    "            FFT=FFTLoss()\n",
    "            loss_tr=FFT(output[0].cpu(),tg.cpu())\n",
    "            wandb.log({Loss:loss_tr})\n",
    "\n",
    "            wandb.log({\"FFT 1-t\":FFT(vec1.cpu(),tg.cpu())})\n",
    "            wandb.log({\"FFT 2-t\":FFT(vec2.cpu(),tg.cpu())})\n",
    "            wandb.log({\"FFT 1-p\":Mel2(output[0].cpu(),vec1.cpu())})\n",
    "            wandb.log({\"FFT 2-p\":Mel2(output[0].cpu(),vec2.cpu())})\n",
    "            \n",
    "            d_loos[Loss]=loss_tr\n",
    "\n",
    "        if Loss==\"Mel FID\":\n",
    "            FID=MelFIDLoss()\n",
    "            loss_tr=FID(output[0].cpu(),tg.cpu())\n",
    "\n",
    "            wandb.log({\"FID 1-t\":FID(vec1.cpu(),tg.cpu())})\n",
    "            wandb.log({\"FID 2-t\":FID(vec2.cpu(),tg.cpu())})\n",
    "            wandb.log({\"FID 1-p\":Mel2(output[0].cpu(),vec1.cpu())})\n",
    "            wandb.log({\"FID 2-p\":Mel2(output[0].cpu(),vec2.cpu())})\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "\n",
    "        if Loss== \"Mel L2\":\n",
    "            Mel2=MelSpecL2Loss()\n",
    "\n",
    "            wandb.log({\"Mel2 1-t\":Mel2(vec1.cpu(),tg.cpu())})\n",
    "            wandb.log({\"Mel2 2-t\":Mel2(vec2.cpu(),tg.cpu())})\n",
    "            wandb.log({\"Mel2 1-p\":Mel2(output[0].cpu(),vec1.cpu())})\n",
    "            wandb.log({\"Mel2 2-p\":Mel2(output[0].cpu(),vec2.cpu())})\n",
    "            \n",
    "            loss_tr=Mel2(output[0].cpu(),tg.cpu())\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss== \"JS Loss\":\n",
    "            js_loss = JensenShannonLoss()\n",
    "            loss_tr=js_loss(output[0].cpu(),tg.cpu())\n",
    "            wandb.log({Loss:loss_tr})\n",
    "        if Loss==\"Latent\":\n",
    "            mse_loss = torch.nn.MSELoss()\n",
    "            z1_vec1=mod.enc1(vec1)[0]#[0,:,:]\n",
    "            z1_vec2=mod.enc1(vec2)[0]#[0,:,:]\n",
    "            z1_pred=mod.enc1(output[0])[0]#[0,:,:]\n",
    "            z1_tg=mod.enc1(tg)[0]\n",
    "            \n",
    "            out3=torch.cat([z1_tg,z1_tg], dim=2)\n",
    "            Z1_tg = mod.tanh(mod.vec2neck(torch.sum(out3, dim=1, keepdim=False))).to(\"cpu\")\n",
    "            \n",
    "            l2_z11 = mse_loss(z1_vec1, z1_tg)\n",
    "            l2_z12 = mse_loss(z1_vec2,z1_tg)\n",
    "            l2_z1p = mse_loss(z1_pred,z1_tg)\n",
    "            del(z1_vec1)\n",
    "            del(z1_vec2)\n",
    "            del(z1_pred)\n",
    "            del(z1_tg)\n",
    "            \n",
    "            z2_vec1=mod.enc2(vec1)[0]#[0,:,:]\n",
    "            z2_vec2=mod.enc2(vec2)[0]#[0,:,:]\n",
    "            z2_pred=mod.enc2(output[0])[0]#[0,:,:]\n",
    "            z2_tg=mod.enc2(tg)[0]#[0,:,:]\n",
    "            \n",
    "            \n",
    "            out3=torch.cat([z2_tg,z2_tg], dim=2)\n",
    "            Z2_tg = mod.tanh(mod.vec2neck(torch.sum(out3, dim=1, keepdim=False))).to(\"cpu\")\n",
    "            \n",
    "            l2_z21 = mse_loss(z2_vec1, z2_tg)\n",
    "            l2_z22 = mse_loss(z2_vec2,z2_tg)\n",
    "            l2_z2p = mse_loss(z2_pred,z2_tg)\n",
    "            del(z2_vec1)\n",
    "            del(z2_vec2)\n",
    "            del(z2_pred)\n",
    "            del(z2_tg)\n",
    "            \n",
    "            LZ1= mse_loss(Z1_tg.cuda() ,output[1].cuda())\n",
    "            LZ2=mse_loss(Z2_tg.cuda(),output[1].cuda())\n",
    "            \n",
    "            LZ = LZ1 +LZ2\n",
    "            del(Z1_tg)\n",
    "            del(Z2_tg)\n",
    "            loss_tr =LZ +(2.0*l2_z1p/(l2_z11 + l2_z12))+(2.0*l2_z2p/(l2_z21 + l2_z22))\n",
    "\n",
    "            wandb.log({\"Branch 1  x1-p MSE Z\":l2_z11})\n",
    "            wandb.log({\"Branch 1  x2-p MSE Z\":l2_z12})\n",
    "            wandb.log({\"Branch 1  p-t MSE Z\":l2_z1p})\n",
    "            \n",
    "            wandb.log({\"Branch 2  x1-p MSE Z\":l2_z21})\n",
    "            wandb.log({\"Branch 2  x2-p MSE Z\":l2_z22})\n",
    "            wandb.log({\"Branch 2  p-t MSE Z\":l2_z2p})\n",
    "\n",
    "            wandb.log({\"Branch 1  before after merge\":LZ1})\n",
    "            wandb.log({\"Branch 2  before after merge\":LZ2})\n",
    "\n",
    "\n",
    "            \n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "            mod=mod.cuda()\n",
    "        if Loss==\"MAPE\":\n",
    "            mape=MAPE()\n",
    "            loss_tr = mape(output[0],tg)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"MAE\":\n",
    "            mae = nn.L1Loss()\n",
    "            loss_tr = mae(output[0],tg)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"MSE\":\n",
    "            loss_tr = criterion(output[0],tg)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"sinkhorn\":\n",
    "            loss_tr=LossWS(output[0],tg)\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"AUTO\":\n",
    "            delimiters = [208, 1414, 1514, 2254, 2464]\n",
    "            predictions=output[0]\n",
    "            targets=tg\n",
    "            # Initialize the loss function with custom weights\n",
    "            loss_fn = AutoregressiveMSELoss(lambda_weights=[10,5,2, 1, 0.5])\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_tr = loss_fn(predictions, targets, delimiters)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"ws_scipy 0.9\":\n",
    "            vec1_np = output[0].detach().cpu().numpy()\n",
    "            vec2_np = tg.detach().cpu().numpy()\n",
    "            wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "            average_wsd = sum(wsd_list) / len(wsd_list)\n",
    "            loss_tr = average_wsd *0.9\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"ws_scipy\":\n",
    "            vec1_np = output[0].detach().cpu().numpy()\n",
    "            vec2_np = tg.detach().cpu().numpy()\n",
    "            wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "            average_wsd = sum(wsd_list) / len(wsd_list)\n",
    "            loss_tr = average_wsd *0.9\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"gw_loss\":\n",
    "            try:\n",
    "                if len(DF)>0:\n",
    "                    PD=DF[Cols[2466:4930]]\n",
    "                    dataPD=PD.to_numpy()\n",
    "                    GT=DF[Cols[2:2466]]\n",
    "                    dataGT=GT.to_numpy()\n",
    "                    FN=DF[Cols[4930:7394]]\n",
    "                    dataF=FN.to_numpy()\n",
    "                    # Define filter function â€“ can be any scikit-learn transformer\n",
    "                    filter_func = Projection(columns=[0,1])\n",
    "                    # Define cover\n",
    "                    cover = CubicalCover(n_intervals=20, overlap_frac=0.5)\n",
    "                    # Choose clustering algorithm â€“ default is DBSCAN\n",
    "                    clusterer = DBSCAN()\n",
    "    \n",
    "                    # Configure parallelism of clustering step\n",
    "                    n_jobs = 8\n",
    "    \n",
    "                    # Initialise pipeline\n",
    "                    pipe = make_mapper_pipeline(\n",
    "                        filter_func=filter_func,\n",
    "                        cover=cover,\n",
    "                        clusterer=clusterer,\n",
    "                        verbose=True,\n",
    "                       n_jobs=n_jobs)\n",
    "                    graphT=pipe.fit_transform(dataGT)\n",
    "                    graphP = pipe.fit_transform(dataPD)\n",
    "                    # Transform the second point cloud using the fitted pipeline\n",
    "    \n",
    "                    T=graphT.to_networkx()\n",
    "                    P=graphP.to_networkx()\n",
    "    \n",
    "                    # Convert graphs to adjacency matrices\n",
    "                    def graph_to_adjacency_matrix(graph):\n",
    "                        return torch.tensor(nx.to_numpy_array(graph), dtype=torch.float32)\n",
    "    \n",
    "                    # Assume G1 and G2 are your two NetworkX graph objects\n",
    "                    adj1 = graph_to_adjacency_matrix(T)\n",
    "                    adj2 = graph_to_adjacency_matrix(P)\n",
    "    \n",
    "                    def gw_loss(C1, C2, P):\n",
    "                        C1P = torch.matmul(C1, P)  # n x m matrix\n",
    "                        PC2 = torch.matmul(P, C2)  # n x m matrix\n",
    "                        return torch.sum((C1P - PC2) ** 2)\n",
    "                    # Compute pairwise distance matrices for each graph\n",
    "                    C1 = torch.cdist(adj1, adj1, p=2)  # Pairwise distance matrix for G1\n",
    "                    C2 = torch.cdist(adj2, adj2, p=2)  # Pairwise distance matrix for G2\n",
    "                    # Initialize coupling matrix with uniform distribution\n",
    "                    n, m = C1.shape[0], C2.shape[0]\n",
    "                    P = torch.ones((n, m), dtype=torch.float32, requires_grad=True) / (n * m)\n",
    "                    P=P.detach()\n",
    "                    P.requires_grad=True\n",
    "    \n",
    "                    P=P.detach()\n",
    "                    P.requires_grad=True\n",
    "                    #print(P.requires_grad)\n",
    "                    ##optimizer = torch.optim.Adam([P], lr=0.01)\n",
    "                    ##optimizer.step()\n",
    "                    loss_tr=gw_loss(C1,C2,P)*0.0005\n",
    "                else:\n",
    "                    loss_tr=0\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            \n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"CAE\":\n",
    "            W=mod.vec2neck.weight\n",
    "            CL=loss_Contractive(W,output[0],tg, output[1], Lambda)\n",
    "            loss_tr = CL\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"Q-quantile_loss\":    \n",
    "            loss_tr=q_quantile_loss(output[0],tg).cuda()\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"LWLN\":\n",
    "            LW=LWLN_loss()\n",
    "            loss_tr=LW(output[0],tg)\n",
    "            #print(Loss,\": \\t\",loss_tr\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"LWWS\":\n",
    "            loss_tr=LossWS(output[0][:,0:208],tg[:,0:208]) +LossWS(output[0][:,208:1414],tg[:,208:1414])+LossWS(output[0][:,1414:1514],tg[:,1414:1514])+LossWS(output[0][:,1514:2254],tg[:,1514:2254])+LossWS(output[0][:,2254:2464],tg[:,2254:2464])\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "            \n",
    "            \n",
    "        if Loss==\"LWWS_scipy\":\n",
    "            vec1_np = output[0].detach().cpu().numpy()\n",
    "            vec2_np = tg.detach().cpu().numpy()\n",
    "            \n",
    "            delimiters = [208, 1414, 1514, 2254, 2464]\n",
    "            # Compute Wasserstein distance for each chunk and sum the averages\n",
    "            total_average_wsd = 0\n",
    "            start = 0\n",
    "            for end in delimiters:\n",
    "                chunk1 = vec1_np[:, start:end]\n",
    "                chunk2 = vec2_np[:, start:end]\n",
    "                wsd_list = [wasserstein_distance(chunk1[i], chunk2[i]) for i in range(chunk1.shape[0])]\n",
    "                average_wsd = sum(wsd_list) / len(wsd_list)  # Average Wasserstein distance for the current chunk\n",
    "                total_average_wsd += average_wsd  # Add to total\n",
    "                start = end\n",
    "            # Print the total sum of batch averages\n",
    "            #print(f\"Total Sum of Batch Averages across all chunks: {total_average_wsd}\")\n",
    "\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:total_average_wsd})\n",
    "            d_loos[Loss]=total_average_wsd\n",
    "        if Loss==\"FIM\":\n",
    "            # Example numpy arrays\n",
    "            output[0].detach().requires_grad=True\n",
    "            tg.requires_grad=True\n",
    "            # Step 1: Convert to probabilities and compute log-probabilities with gradient tracking\n",
    "            log_probs1 = torch.log_softmax(output[0], dim=1)\n",
    "            log_probs2 = torch.log_softmax(tg, dim=1)\n",
    "\n",
    "            # Step 2: Convert log-probs to probs while ensuring gradients are tracked\n",
    "            probs1 = log_probs1.exp()\n",
    "            probs2 = log_probs2.exp()\n",
    "\n",
    "            # Step 3: Compute gradients of log-probabilities with respect to original tensors\n",
    "            # Using create_graph=True to allow second-order gradients\n",
    "            grad_log_probs1 = torch.autograd.grad(outputs=log_probs1, inputs=output[0], grad_outputs=torch.ones_like(log_probs1), retain_graph=True, create_graph=True)[0]\n",
    "            grad_log_probs2 = torch.autograd.grad(outputs=log_probs2, inputs=tg, grad_outputs=torch.ones_like(log_probs2), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "            # Step 4: Approximate Fisher Information for each tensor\n",
    "            fim1 = probs1 * grad_log_probs1**2\n",
    "            fim2 = probs2 * grad_log_probs2**2\n",
    "\n",
    "            # Step 5: Compute Fisher Distance (use Frobenius norm)\n",
    "            fisher_distance = torch.norm(fim1 - fim2, p='fro')  # Frobenius norm for matrix difference\n",
    "            loss_tr=fisher_distance*100\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"log-norm\":\n",
    "            N1=NormDifferenceDistance()\n",
    "            loss_tr=N1(output[0],tg)\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"KL divergence\":\n",
    "            kl_loss = nn.KLDivLoss(reduction=\"batchmean\",log_target=True)\n",
    "            loss_tr=kl_loss(output[0],nn.functional.log_softmax(tg))*0.1\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"Forb_norm\":\n",
    "            f=FrobeniusNormJacobian(10)\n",
    "            loss_tr=f(mod,output[0],tg)*0.015\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "    return d_loos,output,mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889329c3-3482-456c-a2a2-b4f199dee9f9",
   "metadata": {},
   "source": [
    "# Creating a shared meta-learner starting checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "439f333e-9793-4ef7-a17f-37a179756e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BrainTransformer=TransformerAE(max_seq_len=50,\n",
    "#             N=4,\n",
    "#             heads=4,\n",
    "#             d_model=960,#960\n",
    "#             d_ff=960,#960\n",
    "#             neck=512,#256\n",
    "#             dropout=0.07#0.12\n",
    "#             )\n",
    "\n",
    "# torch.save({'epoch':-1,'model_state_dict': BrainTransformer.state_dict()},\n",
    "#             './AE epoch{} {} {}.pth'.format( \"initial\" , \"original\", -1))\n",
    "# del(BrainTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46f847c7-1d3b-4ba6-8ed4-31164991744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string='overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83fd9b-ac1f-4189-b694-18264632e152",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cccec8e2-8c2d-4e72-bb04-6ef3e9e086f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 26 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 11 gelu/train pair.npy 440\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 11 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 21 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 gelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 16 gelu/train pair.npy 20\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 16 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 36 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 16 tanh/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 26 relu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 11 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 31 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 26 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 26 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 16 gelu/train pair.npy 380\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 31 gelu/train pair.npy 20\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 16 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 26 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 26 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 31 gelu/train pair.npy 440\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 21 gelu/train pair.npy 440\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 11 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 21 leakyrelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 21 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 26 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 26 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 36 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 26 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 11 gelu/train pair.npy 20\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 31 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 26 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 gelu/train pair.npy 11076\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 16 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 36 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 relu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 16 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 36 gelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 26 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 relu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 36 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 11 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 31 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 26 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 31 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 36 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 36 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 11 gelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 relu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 26 gelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 11 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 11 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 36 gelu/train pair.npy 20\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 26 gelu/train pair.npy 20\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 36 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 36 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 36 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 21 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 21 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 11 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 21 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 16 gelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 31 gelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 16 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 36 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 gelu/train pair.npy 11076\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 31 gelu/train pair.npy 380\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 31 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 21 gelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 31 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 16 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 11 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 21 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 16 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 36 tanh/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 36 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 31 leakyrelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 11 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 16 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 21 gelu/train pair.npy 20\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 11 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 36 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 21 tanh/train pair.npy 450\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 16 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 36 leakyrelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 26 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 11 gelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 16 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 26 leakyrelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 11 tanh/train pair.npy 450\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 21 tanh/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 31 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 11 leakyrelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 26 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 36 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 16 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 relu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 11 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 11 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 21 silu/train pair.npy 450\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 36 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 gelu/train pair.npy 11076\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 21 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 36 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 36 sigmoid/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 gelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 31 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 relu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 36 gelu/train pair.npy 380\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 gelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 26 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 26 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 31 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 26 tanh/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 21 silu/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 21 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 16 gelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5] 36 sigmoid/train pair.npy 180\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 31 tanh/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 11 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 31 relu/train pair.npy 180\n",
      "./data/Scenario/overlapping 3 [0, 1, 2, 3, 4, 5] 36 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 silu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 36 sigmoid/train pair.npy 55572\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 36 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 21 leakyrelu/train pair.npy 55572\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 21 tanh/train pair.npy 180\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 26 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 16 gelu/train pair.npy 11076\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5] 16 leakyrelu/train pair.npy 180\n",
      "./data/Scenario/overlapping 4 [0, 1, 2, 3, 4, 5] 36 relu/train pair.npy 450\n",
      "./data/Scenario/overlapping 2 [0, 1, 2, 3, 4, 5] 26 leakyrelu/train pair.npy 450\n",
      "./data/Scenario/overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu/train pair.npy 11076\n",
      "--------------------2025-10-14 16:23:25.429794 name 'mod' is not defined\n",
      "encoder droupout init 0.07\n",
      "encoder droupout init 0.07\n",
      "decoder droupout init 0.07\n",
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "TransformerAE                                      --\n",
      "â”œâ”€EncoderNeuronGroup: 1-1                          --\n",
      "â”‚    â””â”€EmbedderNeuronGroup: 2-1                    --\n",
      "â”‚    â”‚    â””â”€Linear: 3-1                            16,320\n",
      "â”‚    â”‚    â””â”€Linear: 3-2                            77,760\n",
      "â”‚    â””â”€PositionalEncoder: 2-2                      --\n",
      "â”‚    â””â”€ModuleList: 2-3                             --\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-3                      5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-4                      5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-5                      5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-6                      5,539,200\n",
      "â”‚    â””â”€Norm: 2-4                                   1,920\n",
      "â”œâ”€EncoderNeuronGroup: 1-2                          --\n",
      "â”‚    â””â”€EmbedderNeuronGroup: 2-5                    --\n",
      "â”‚    â”‚    â””â”€Linear: 3-7                            16,320\n",
      "â”‚    â”‚    â””â”€Linear: 3-8                            77,760\n",
      "â”‚    â””â”€PositionalEncoder: 2-6                      --\n",
      "â”‚    â””â”€ModuleList: 2-7                             --\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-9                      5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-10                     5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-11                     5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-12                     5,539,200\n",
      "â”‚    â””â”€Norm: 2-8                                   1,920\n",
      "â”œâ”€DecoderNeuronGroup: 1-3                          --\n",
      "â”‚    â””â”€Neck2Seq: 2-9                               --\n",
      "â”‚    â”‚    â””â”€ModuleList: 3-13                       24,624,000\n",
      "â”‚    â””â”€PositionalEncoder: 2-10                     --\n",
      "â”‚    â””â”€ModuleList: 2-11                            --\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-14                     5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-15                     5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-16                     5,539,200\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-17                     5,539,200\n",
      "â”‚    â””â”€Norm: 2-12                                  1,920\n",
      "â”‚    â””â”€Seq2Vec: 2-13                               --\n",
      "â”‚    â”‚    â””â”€Linear: 3-18                           118,274,464\n",
      "â”œâ”€Linear: 1-4                                      983,552\n",
      "â”œâ”€Tanh: 1-5                                        --\n",
      "===========================================================================\n",
      "Total params: 210,546,336\n",
      "Trainable params: 210,546,336\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/asyncio/locks.py:214\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/asyncio/tasks.py:456\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/response_handle.py:82\u001b[0m, in \u001b[0;36mMailboxResponseHandle.wait_async\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done_event\u001b[38;5;241m.\u001b[39mwait(), timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mTimeoutError, \u001b[38;5;167;01mTimeoutError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/asyncio/tasks.py:458\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 458\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1004\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self, settings, config, run_printer)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mwait_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_init_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_init_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:23\u001b[0m, in \u001b[0;36mwait_with_progress\u001b[0;34m(handle, timeout, display_progress)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for a handle, possibly displaying progress to the user.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mEquivalent to passing a single handle to `wait_all_with_progress`.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_all_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:77\u001b[0m, in \u001b[0;36mwait_all_with_progress\u001b[0;34m(handle_list, timeout, display_progress)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait_handles_async(\n\u001b[1;32m     73\u001b[0m             handle_list,\n\u001b[1;32m     74\u001b[0m             timeout\u001b[38;5;241m=\u001b[39mremaining_timeout,\n\u001b[1;32m     75\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress_loop_with_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[0m, in \u001b[0;36mAsyncioManager.run\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mCancelledError:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[0m, in \u001b[0;36mAsyncioManager._wrap\u001b[0;34m(self, fn, daemon, name)\u001b[0m\n\u001b[1;32m    217\u001b[0m         task\u001b[38;5;241m.\u001b[39mset_name(name)\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:72\u001b[0m, in \u001b[0;36mwait_all_with_progress.<locals>.progress_loop_with_timeout\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait_handles_async(\n\u001b[1;32m     73\u001b[0m     handle_list,\n\u001b[1;32m     74\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mremaining_timeout,\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:95\u001b[0m, in \u001b[0;36m_wait_handles_async\u001b[0;34m(handle_list, timeout)\u001b[0m\n\u001b[1;32m     93\u001b[0m     results[index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mwait_async(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m asyncio_compat\u001b[38;5;241m.\u001b[39mopen_task_group() \u001b[38;5;28;01mas\u001b[39;00m task_group:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(handle_list)):\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/contextlib.py:206\u001b[0m, in \u001b[0;36m_AsyncGeneratorContextManager.__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py:240\u001b[0m, in \u001b[0;36mopen_task_group\u001b[0;34m(exit_timeout, race)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m task_group\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task_group\u001b[38;5;241m.\u001b[39m_wait_all(race\u001b[38;5;241m=\u001b[39mrace, timeout\u001b[38;5;241m=\u001b[39mexit_timeout)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py:180\u001b[0m, in \u001b[0;36mTaskGroup._wait_all\u001b[0;34m(self, race, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;241m:=\u001b[39m task\u001b[38;5;241m.\u001b[39mexception():\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Wait for remaining tasks to clean up, then re-raise any exceptions\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# that arise. Note that pending is only non-empty when race=True.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py:93\u001b[0m, in \u001b[0;36m_wait_handles_async.<locals>.wait_single\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     92\u001b[0m handle \u001b[38;5;241m=\u001b[39m handle_list[index]\n\u001b[0;32m---> 93\u001b[0m results[index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mwait_async(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/mailbox_handle.py:133\u001b[0m, in \u001b[0;36m_MailboxMappedHandle.wait_async\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _S:\n\u001b[0;32m--> 133\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mwait_async(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fn(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/mailbox/response_handle.py:90\u001b[0m, in \u001b[0;36mMailboxResponseHandle.wait_async\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out waiting for response on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_address\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Timed out waiting for response on n2ub3fg6b74p",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m LWs\u001b[38;5;241m=\u001b[39mLWLN_loss_single()\n\u001b[1;32m    117\u001b[0m device \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 118\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# Set the project where this run will be logged\u001b[39;49;00m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWR4CL-project\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcombination\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnamef\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnamef\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# Track hyperparameters and run metadata\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mScenario\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mnamef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstances in train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_pair2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr Encoder 1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrE1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr Encoder 2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrE2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr Linear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr Decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msched_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msched_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLambda_contractive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mLambda\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#run = wandb.init(project=\"aymen-project\", id=\"ar1497fk\", resume=\"must\")\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# \"factor\":factor,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#     \"eps\":eps,\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m#random.shuffle(train_pair2)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m cs_tr\u001b[38;5;241m=\u001b[39mCustomDataset(train_pair2,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,batch_limit\u001b[38;5;241m=\u001b[39mbatch_limit)\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1601\u001b[0m, in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     wl\u001b[38;5;241m.\u001b[39m_get_logger()\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/analytics/sentry.py:162\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1582\u001b[0m, in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_settings\u001b[38;5;241m.\u001b[39mx_server_side_derived_summary:\n\u001b[1;32m   1580\u001b[0m     init_telemetry\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mserver_side_derived_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1582\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_printer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;66;03m# Set up automatic Weave integration if Weave is installed\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m weave\u001b[38;5;241m.\u001b[39msetup(run_settings\u001b[38;5;241m.\u001b[39mentity, run_settings\u001b[38;5;241m.\u001b[39mproject)\n",
      "File \u001b[0;32m~/anaconda3/envs/FCL/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1016\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self, settings, config, run_printer)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     run_init_handle\u001b[38;5;241m.\u001b[39mcancel(backend\u001b[38;5;241m.\u001b[39minterface)\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# This may either be an issue with the W&B server (a CommError)\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# or a bug in the SDK (an Error). We cannot distinguish between\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# the two causes here.\u001b[39;00m\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CommError(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun initialization has timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please try increasing the timeout with the `init_timeout`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrun_result\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;241m:=\u001b[39m ProtobufErrorHandler\u001b[38;5;241m.\u001b[39mto_exception(result\u001b[38;5;241m.\u001b[39mrun_result\u001b[38;5;241m.\u001b[39merror):\n",
      "\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "           \n",
    "track=0\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "global track , output , Brain ,model\n",
    "grads=None\n",
    "\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\")\n",
    "nb_scen=len(os.listdir(\"./data/Scenario/\"))\n",
    "for t in range(nb_scen):\n",
    "    namef=os.listdir(\"./data/Scenario/\")[t]\n",
    "    train_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/train pair.npy\", allow_pickle=True)\n",
    "    print(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/\"+\"train pair.npy\" ,len(train_pair2))\n",
    "    val_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/val pair.npy\", allow_pickle=True)\n",
    "    test_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/test pair.npy\", allow_pickle=True)\n",
    "    \n",
    "    train_pair2 = [ list(x) for x in train_pair2]\n",
    "    test_pair2 = [ list(x) for x in test_pair2]\n",
    "    val_pair2 = [ list(x) for x in val_pair2]\n",
    "    \n",
    "    #random.shuffle(test_pair2)\n",
    "    #random.shuffle(val_pair2)\n",
    "    \n",
    "    #if len(train_pair2)>1000 and  len(val_pair2)>1000 and  len(test_pair2)>100:\n",
    "    if namef=='overlapping 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 11 gelu' :\n",
    "        for combination in all_losses:\n",
    "            d_loos=dict()\n",
    "            DF= pd.DataFrame(columns=Cols)\n",
    "            track=0\n",
    "\n",
    "\n",
    "            cnn_samples=20\n",
    "            num_epochs=800\n",
    "            batch_size=36\n",
    "            batch_limit=10000\n",
    "            epochs_fn=5\n",
    "\n",
    "            if not(os.path.isdir(f\".//Experiments//{os.listdir('.//data//Scenario//')[t]}\")):\n",
    "                os.mkdir(f\".//Experiments//{os.listdir('.//data//Scenario//')[t]}\")\n",
    "            results_path=f\".//Experiments//{os.listdir('.//data//Scenario//')[t]}//{combination } {str(datetime.datetime.now())} {num_epochs} /\"\n",
    "            if not(os.path.isdir(results_path)):\n",
    "                os.mkdir(results_path)\n",
    "            if not(os.path.isdir(results_path+f'Tracking/')):\n",
    "                os.mkdir(results_path+f'Tracking/')\n",
    "            if not(os.path.isdir(results_path+f'Attention/')):\n",
    "                os.mkdir(results_path+f'Attention/')\n",
    "    \n",
    "            cnn_acc_ID=[]\n",
    "            cnn_acc_OOD=[]\n",
    "            \n",
    "            Lambda=0\n",
    "            step_size=0\n",
    "            factor=0\n",
    "            threshhold=0\n",
    "            threshold_mode = 0\n",
    "            eps=0\n",
    "            try:\n",
    "                del(mod)\n",
    "                del(optimizerEnc1)\n",
    "                del(optimizerEnc2) \n",
    "                del(optimizerDense)\n",
    "                del(optimizerDec)\n",
    "            except Exception as e:\n",
    "                print(f\"--------------------{str(datetime.datetime.now())} {e}\")\n",
    "            ch=torch.load('./AE epoch{} {} {}.pth'.format( \"initial\" , \"original\", -1),map_location=\"cpu\")\n",
    "            mod=TransformerAE(max_seq_len=50,\n",
    "                                    N=4,\n",
    "                                    heads=4,\n",
    "                                    d_model=960,#960\n",
    "                                    d_ff=960,#960\n",
    "                                    neck=512,#256\n",
    "                                    dropout=0.07)#0.12)\n",
    "            mod.load_state_dict(ch['model_state_dict'])\n",
    "            del(ch)\n",
    "            minimal_loss=100000\n",
    "            print(summary(mod))\n",
    "            lrE1=0.15 #trial.suggest_float(\"Learning_rate\",0.0002,0.5)\n",
    "            lrE2=0.04 \n",
    "            lrL=0.15 \n",
    "            lrD=0.085\n",
    "            optimizerEnc1 = Adam(mod.enc1.parameters(), lr=lrE1,eps=1e-10,weight_decay=0.005)\n",
    "            optimizerEnc2 = Adadelta(mod.enc2.parameters(), lr=lrE2,eps=1e-10,weight_decay=0.005)\n",
    "            optimizerDense = SGD(mod.vec2neck.parameters(), lr=lrL,weight_decay=0.005)\n",
    "            optimizerDec = Adadelta(mod.dec.parameters(), lr=lrD,eps=1e-10,weight_decay=0.01)\n",
    "            mod = torch.compile(mod)\n",
    "    \n",
    "    \n",
    "    \n",
    "            sched_name=\"CyclicLR\"\n",
    "            if sched_name==\"CyclicLR\" :\n",
    "                step_size=24000#trial.suggest_int(\"step_size_up\",900,2800)\n",
    "                schedulerEnc1 = torch.optim.lr_scheduler.CyclicLR(optimizerEnc1, base_lr=1e-4, max_lr=lrE1, step_size_up=step_size, step_size_down=80000,scale_mode=\"iterations\",mode=\"triangular2\",cycle_momentum=False)\n",
    "                schedulerEnc2 = torch.optim.lr_scheduler.CyclicLR(optimizerEnc2, base_lr=1e-4, max_lr=lrE2, step_size_up=step_size, step_size_down=80000,scale_mode=\"iterations\",mode=\"triangular2\",cycle_momentum=False)\n",
    "                scheduler = torch.optim.lr_scheduler.CyclicLR(optimizerDense, base_lr=1e-4, max_lr=lrD, step_size_up=8000, step_size_down=8000,scale_mode=\"iterations\",mode=\"triangular\",cycle_momentum=False)\n",
    "\n",
    "    \n",
    "            resume_epoch=0\n",
    "            # ch=torch.load(\"/media/crns/ADATA HD330/Experiments/mixed model/AE epoch 0 600.pth\",map_location=\"cpu\")\n",
    "            # resume_epoch=ch[\"epoch\"]\n",
    "            # mod.load_state_dict(ch['model_state_dict'])\n",
    "            # optimizerEnc1.load_state_dict(ch['optimizerENC1_state_dict'])\n",
    "            # optimizerEnc2.load_state_dict(ch['optimizerENC2_state_dict'])\n",
    "            # optimizerDense.load_state_dict(ch['optimizerDense_state_dict'])\n",
    "            # optimizerDec.load_state_dict(ch['optimizerDec_state_dict'])\n",
    "    \n",
    "            #del(ch)\n",
    "    \n",
    "            criterion = nn.MSELoss()\n",
    "            LW=LWLN_loss()\n",
    "            LossWS=SamplesLoss('sinkhorn')\n",
    "            LWs=LWLN_loss_single()\n",
    "    \n",
    "            device = accelerator.device\n",
    "            run = wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"WR4CL-project\",\n",
    "            name= f\"{str(datetime.datetime.now())} {combination} {namef[12:16]}-{namef[42:44]} {num_epochs}\" ,\n",
    "            # Track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"Scenario\":namef,\n",
    "                \"instances in train\":len(train_pair2),\n",
    "                \"Loss\":combination,\n",
    "                \"lr Encoder 1\": lrE1,\n",
    "                \"lr Encoder 2\": lrE2,\n",
    "                \"lr Linear\": lrL,\n",
    "                \"lr Decoder\": lrD,\n",
    "                \"epochs\": num_epochs,\n",
    "                \"sched_name\": sched_name,\n",
    "                \"step_size\": step_size,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"step_size\":step_size,\n",
    "                \"Lambda_contractive\":Lambda\n",
    "            },)\n",
    "    \n",
    "            #run = wandb.init(project=\"aymen-project\", id=\"ar1497fk\", resume=\"must\")\n",
    "    \n",
    "            # \"factor\":factor,\n",
    "            #     \"threshhold\":threshhold,\n",
    "            #     \"threshold_mode\":threshold_mode,\n",
    "            #     \"eps\":eps,\n",
    "            #random.shuffle(train_pair2)\n",
    "            cs_tr=CustomDataset(train_pair2,batch_size=batch_size,batch_limit=batch_limit)\n",
    "            cs_tr=accelerator.prepare(cs_tr)\n",
    "            \n",
    "            nb_batches = len(cs_tr)\n",
    "            \n",
    "            print(\"Number of training batchs:\",nb_batches)\n",
    "            cs_val=CustomDataset(val_pair2,batch_size=batch_size,batch_limit=batch_limit)\n",
    "            nb_val_batches = len(cs_val)\n",
    "\n",
    "            cs_ts=CustomDataset(test_pair2,batch_size=batch_size,batch_limit=batch_limit)\n",
    "    \n",
    "            mod, optimizerEnc1,optimizerEnc2,optimizerDense,optimizerDec, cs_tr= accelerator.prepare(mod, optimizerEnc1,optimizerEnc2,optimizerDense,optimizerDec,cs_tr)\n",
    "            # wandb.watch(mod, log_freq=10000 ,criterion=criterion,\n",
    "            #     log='parameters',\n",
    "            #     log_graph=True)\n",
    "            #mod.train()\n",
    "            \n",
    "            mod.train()\n",
    "            \n",
    "            wandb.define_metric(\"custom_step\")\n",
    "            wandb.define_metric(\"Prediction Histogram\", step_metric='custom_step')\n",
    "            wandb.define_metric(\"Total Histogram\", step_metric='custom_step')\n",
    "            used_Loss=0.0\n",
    "            for epoch in tqdm(range(resume_epoch,num_epochs),total=num_epochs):\n",
    "                # for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(locals().items())), key= lambda x: -x[1])[:10]:\n",
    "                #     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "                wandb.log({f\"Current epoch\":epoch})\n",
    "                random.shuffle(train_pair2)\n",
    "                \n",
    "                #start_time_epoch = time.time()\n",
    "                for i in range(nb_batches):#nb_batches for training'''\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    Dataset,EXP,ACC,U = cs_tr[i]\n",
    "                    \n",
    "                    x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]\n",
    "    \n",
    "                    x1=x1.cuda() #.to(torch.float32)\n",
    "                    x2=x2.cuda() #.to(torch.float32)\n",
    "                    tg=tg.cuda() #.to(torch.float32)\n",
    "    \n",
    "                    optimizerEnc1.zero_grad()\n",
    "                    optimizerEnc2.zero_grad()\n",
    "                    optimizerDense.zero_grad()\n",
    "                    optimizerDec.zero_grad()\n",
    "    \n",
    "                    #output = mod(x1,x2)\n",
    "    \n",
    "                    d_loos,output,mod=update_all_metrics(d_loos,mod,x1,x2,tg,DF)\n",
    "                    torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=2.0)\n",
    "    \n",
    "    \n",
    "                    used_Loss=0\n",
    "                    #print(used_Loss,d_loos)\n",
    "                    if isinstance(combination, list):\n",
    "                        for pair in d_loos.items():\n",
    "                            if pair[0]==combination[0] or pair[0]==combination[1] :\n",
    "                                used_Loss=used_Loss+pair[1]\n",
    "                    if isinstance(combination, str):\n",
    "                        for pair in d_loos.items():\n",
    "                            if pair[0]==combination: \n",
    "                                used_Loss=used_Loss+pair[1]\n",
    "                    #print(combination ,\"\\t \\t\",used_Loss)\n",
    "    \n",
    "                    accelerator.backward(used_Loss)\n",
    "                    #loss_tr.backward()\n",
    "                    frob = frobnorm(mod)\n",
    "                    optimizerEnc1.step()\n",
    "                    optimizerEnc2.step()\n",
    "                    optimizerDense.step()\n",
    "                    optimizerDec.step()\n",
    "                    if sched_name==\"ReduceLROnPlateau\" :\n",
    "                        scheduler.step(used_Loss)\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                        schedulerEnc1.step()\n",
    "                        schedulerEnc2.step()\n",
    "                    \n",
    "                    loss_to_save = float(used_Loss.detach().cpu().item())\n",
    "                    wandb.log({f\"Tracked Loss {combination}\":loss_to_save})\n",
    "                \n",
    "                \n",
    "                \n",
    "                if used_Loss.detach().cpu().item()<minimal_loss:\n",
    "                    minimal_loss=used_Loss.detach().cpu().item()\n",
    "                    torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),\n",
    "                                'optimizerENC1_state_dict':  optimizerEnc1.state_dict() ,\n",
    "                                'optimizerENC2_state_dict':optimizerEnc2.state_dict(),\n",
    "                                'optimizerDense_state_dict':optimizerDense.state_dict(),\n",
    "                                'optimizerDec_state_dict': optimizerDec.state_dict(),\n",
    "                                'Batch Loss':used_Loss.detach().cpu().item()},\n",
    "                                results_path+'AE epoch best_.pth')\n",
    "    \n",
    "                end_time = time.time() \n",
    "                execution_time = end_time - start_time  \n",
    "                #if execution_time>4000 :\n",
    "                #    break\n",
    "                if (epoch+1)%50==0 or (epoch+1) in [1,5,10,30]:\n",
    "                    print(\"validating\")\n",
    "                    for block in [2,3,4]:\n",
    "                            for head in range(4):\n",
    "                                plt.figure(figsize=(20, 20))\n",
    "                                hm=sns.heatmap(torch.mean( torch.mean(output[block][head], dim=1), dim=0).detach().cpu(), annot=False, cmap='cubehelix')\n",
    "                                plt.title('Attention Heatmap')\n",
    "                                heatmap_path = f'heatmap {block-2}_{head}_epoch{epoch}_step_{i}.png'\n",
    "                                plt.savefig(results_path+\"Attention/\"+heatmap_path,format='svg', dpi=800)\n",
    "                                wandb.log({f\"attention_heatmap {block-2}_{head}\":  wandb.Image(hm,caption=f\"attention_heatmap attention_heatmap {block-2}_{head}\")})\n",
    "                                plt.close()\n",
    "                    mod.eval()\n",
    "                    loss_val = []\n",
    "                    for i_val in range(nb_val_batches):#nb_val_batches batches for validation'''\n",
    "                        #start_time_batch = time.time()\n",
    "                        Dataset_val,EXP_val,ACC_val,U_val = cs_val[i_val]\n",
    "                        x1_val,x2_val,tg_val = Dataset_val[:,0,:], Dataset_val[:,1,:],Dataset_val[:,2,:]\n",
    "                        x1_val=x1_val.cuda() #.to(torch.float32)\n",
    "                        x2_val=x2_val.cuda() #.to(torch.float32)\n",
    "                        tg_val=tg_val.cuda() #.to(torch.float32)\n",
    "                        with torch.no_grad():\n",
    "                            #output_val = mod(x1_val,x2_val)\n",
    "                            \n",
    "                            loss_val.append(criterion(mod(x1_val,x2_val)[0],tg_val))\n",
    "                            #print(x1_val.shape,mod(x1_val,x2_val)[0].shape)\n",
    "                    loss_value = sum(loss_val)/len(loss_val)\n",
    "                    wandb.log({f\"val_loss\":loss_value})\n",
    "                    print(\"entering test set\")\n",
    "                    DF= pd.DataFrame(columns=Cols)\n",
    "                    track=0\n",
    "                    nb_test_batches = len(cs_ts)\n",
    "                    \n",
    "                    mod,cs_ts= accelerator.prepare(mod, cs_ts)\n",
    "                    mod.eval()\n",
    "    \n",
    "                    #loss_values = []\n",
    "                    \n",
    "                    nb_test= min(60,nb_test_batches)\n",
    "                    print(\"test batches :\",nb_test_batches,\"batch size\",batch_size,\"temporarly testing on batchs:\",nb_test)\n",
    "                    for j in range(nb_test): #nb_test_batches number of batches for testing'''\n",
    "                        Dataset,EXP,ACC,U = cs_ts[j]\n",
    "                        x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]\n",
    "    \n",
    "                        x1=x1.cuda() #.to(torch.float32)\n",
    "                        x2=x2.cuda() #.to(torch.float32)\n",
    "                        tg=tg.cuda() #.to(torch.float32)\n",
    "                        #with torch.no_grad():\n",
    "                        #    output = mod(x1,x2)\n",
    "                        #    loss_values.append(criterion(output[0],tg))\n",
    "    \n",
    "                        for vec in range(len(x1)):\n",
    "                            DF.at[track,\"label task 1\"]=f'{EXP[vec][0]}'\n",
    "                            DF.at[track,\"label task 2\"]=f'{EXP[vec][1]}'\n",
    "                            #print(Cols[2:2466][-2:],Cols[2466:4930][-2:],Cols[4930:7394][-2:])\n",
    "    \n",
    "                            vec1=mod(x1,x2)[0][vec].detach().cpu() #prediction\n",
    "                            vec2=tg[vec].detach().cpu()            #target\n",
    "                            DF.loc[track,Cols[2:2466]]=vec2.tolist()\n",
    "                            DF.loc[track,Cols[2466:4930]]=vec1.tolist()#Cols[4930:7394][-2:])\n",
    "    \n",
    "    \n",
    "    \n",
    "                            MSE=criterion(vec1,vec2).item()\n",
    "                            MS1=criterion(vec1[:208],vec2[:208]).item()\n",
    "                            MS2=criterion(vec1[208:1414],vec2[208:1414]).item()\n",
    "                            MS3=criterion(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "                            MS4=criterion(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "                            MS5=criterion(vec1[2254:],vec2[2254:]).item()\n",
    "                            #print(\"MSE :\",MS1,MS2,MS3,MS4,MS5,MSE)\n",
    "                            DF.at[track,\"MSE\"]=MSE\n",
    "                            DF.at[track,\"MSE 1\"]=MS1\n",
    "                            DF.at[track,\"MSE 2\"]=MS2\n",
    "                            DF.at[track,\"MSE 3\"]=MS3\n",
    "                            DF.at[track,\"MSE 4\"]=MS4\n",
    "                            DF.at[track,\"MSE 5\"]=MS5\n",
    "    \n",
    "    \n",
    "    \n",
    "                            kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "                            KLD=kl_loss(vec1,vec2).item() #pred, true\n",
    "                            KL1=kl_loss(vec1[:208],vec2[:208]).item()\n",
    "                            KL2=kl_loss(vec1[208:1414],vec2[208:1414]).item()\n",
    "                            KL3=kl_loss(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "                            KL4=kl_loss(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "                            KL5=kl_loss(vec1[2254:],vec2[2254:]).item()\n",
    "                            #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "                            DF.at[track,\"KLD\"]=KLD\n",
    "                            DF.at[track,\"KL 1\"]=KL1\n",
    "                            DF.at[track,\"KL 2\"]=KL2\n",
    "                            DF.at[track,\"KL 3\"]=KL3\n",
    "                            DF.at[track,\"KL 4\"]=KL4\n",
    "                            DF.at[track,\"KL 5\"]=KL5\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                            WSD=wasserstein_distance(vec1,vec2)\n",
    "                            WS1=wasserstein_distance(vec1[:208],vec2[:208])\n",
    "                            WS2=wasserstein_distance(vec1[208:1414],vec2[208:1414])\n",
    "                            WS3=wasserstein_distance(vec1[1414:1514],vec2[1414:1514])\n",
    "                            WS4=wasserstein_distance(vec1[1514:2254],vec2[1514:2254])\n",
    "                            WS5=wasserstein_distance(vec1[2254:],vec2[2254:])\n",
    "                            #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "                            DF.at[track,\"Wasserstein Loss\"]=WSD\n",
    "                            DF.at[track,\"WS 1\"]=WS1\n",
    "                            DF.at[track,\"WS 2\"]=WS2\n",
    "                            DF.at[track,\"WS 3\"]=WS3\n",
    "                            DF.at[track,\"WS 4\"]=WS4\n",
    "                            DF.at[track,\"WS 5\"]=WS5\n",
    "    \n",
    "                            for name, param in mod.named_parameters():\n",
    "                                if name == 'vec2neck.weight':\n",
    "                                    W = param\n",
    "                                    break\n",
    "    \n",
    "                            CL=loss_Contractive(mod.vec2neck.weight ,vec1,vec2, output[1], 0.00001)\n",
    "                            DF.at[track,\"contractive distance\"]=CL.cpu().item()\n",
    "    \n",
    "                            #print(\"Contractive :\",CL.cpu().item())\n",
    "                            Norm1=np.sum(np.abs(vec1.numpy()))\n",
    "                            N11=np.sum(np.abs(vec1[:208].numpy()))\n",
    "                            N12=np.sum(np.abs(vec1[208:1414].numpy()))\n",
    "                            N13=np.sum(np.abs(vec1[1414:1514].numpy()))\n",
    "                            N14=np.sum(np.abs(vec1[1514:2254].numpy()))\n",
    "                            N15=np.sum(np.abs(vec1[2254:].numpy()))\n",
    "                            #print(\"Weight pred L1: \",Norm1, N11,N12,N13,N14,N15)\n",
    "                            DF.at[track,\"N1\"]=Norm1\n",
    "                            DF.at[track,\"N11\"]=N11\n",
    "                            DF.at[track,\"N12\"]=N12\n",
    "                            DF.at[track,\"N13\"]=N13\n",
    "                            DF.at[track,\"N14\"]=N14\n",
    "                            DF.at[track,\"N15\"]=N15\n",
    "    \n",
    "                            Norm2=np.sum(np.abs(vec2.numpy()))\n",
    "                            N21=np.sum(np.abs(vec2[:208].numpy()))\n",
    "                            N22=np.sum(np.abs(vec2[208:1414].numpy()))\n",
    "                            N23=np.sum(np.abs(vec2[1414:1514].numpy()))\n",
    "                            N24=np.sum(np.abs(vec2[1514:2254].numpy()))\n",
    "                            N25=np.sum(np.abs(vec2[2254:].numpy()))\n",
    "                            #print(\"Weight GT L1: \",Norm2, N21,N22,N23,N24,N25)\n",
    "                            DF.at[track,\"N2\"]=Norm2\n",
    "                            DF.at[track,\"N21\"]=N21\n",
    "                            DF.at[track,\"N22\"]=N22\n",
    "                            DF.at[track,\"N23\"]=N23\n",
    "                            DF.at[track,\"N24\"]=N24\n",
    "                            DF.at[track,\"N25\"]=N25\n",
    "    \n",
    "                            DF.at[track,\"saturated in pred(%)\"]=100*sum(1 for x in vec1 if x > 0.95 or x<0.05)/len(vec1)\n",
    "                            DF.at[track,\"saturated in GT(%)\"]=100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2)\n",
    "                            DF.at[track,\"LWLN\"]=LWs(vec1,vec2).cpu().item()\n",
    "                            if vec<cnn_samples and j==0:\n",
    "                                print(f\"Test vector {vec} batch {j} Epoch: {epoch}\")\n",
    "                                figure=get_plot_object(vec1, tg)\n",
    "                                wandb.log({\"Plot of pred vs target\":  wandb.Image(figure,caption=f\"Test vector {vec} batch {j} Epoch: {epoch} vs pred\")})\n",
    "                                plt.close(figure)\n",
    "    \n",
    "                                #import umap\n",
    "                                # reducer = umap.UMAP()\n",
    "                                # Batch_UMAP=torch.rand(80,2464)\n",
    "                                # fitted_reduced = reducer.fit(Batch_UMAP)\n",
    "                                # fitted_reduced.transform(vec1)\n",
    "        \n",
    "                                ###########RECONSTRUCTING##############\n",
    "                                print(\"Entering Reconstruction for fine-tuning and eigenvalues\")\n",
    "                                y_pred=torch.unsqueeze(output[0][vec], 0) \n",
    "                                y =torch.unsqueeze(tg[vec], 0) \n",
    "        \n",
    "                                selected_row = cs_ts.df.iloc[int(U[vec][0]), 11:17]  \n",
    "                                columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "                                activ=columns_with_one\n",
    "                                epochCNN=cs_ts.df.loc[int(U[vec][0])]['epoch']\n",
    "        \n",
    "        \n",
    "                                checkpoint=OrderedDict()\n",
    "                                checkpoint1=OrderedDict()\n",
    "                                checkpoint2=OrderedDict()\n",
    "                                vector_aux= output[0][vec].detach()\n",
    "                                y_pred=vector_aux.cpu()\n",
    "        \n",
    "                                task1=[int(x) for x in EXP[vec][0]]\n",
    "                                task2=[int(x) for x in EXP[vec][1]]\n",
    "                                task3=sorted(list(set(task1+task2)))\n",
    "        \n",
    "        \n",
    "                                All=list(range(10))\n",
    "                                L2=[k for k in All if k not in task3] # Out of distribution classes\n",
    "                                L_others=[k for k in All if k not in task3] #Classes to test on (In distribution)\n",
    "        \n",
    "                                checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(y_pred[0:200]).reshape([8, 1, 5, 5]))\n",
    "                                checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(y_pred[200:208]).reshape([8]))\n",
    "        \n",
    "                                checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(y_pred[208:1408]).reshape([6, 8, 5, 5]))\n",
    "                                checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(y_pred[1408:1414]).reshape([6]))\n",
    "        \n",
    "                                checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(y_pred[1414:1510]).reshape([4, 6, 2, 2]))\n",
    "                                checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(y_pred[1510:1514]).reshape([4]))\n",
    "        \n",
    "                                checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(y_pred[1514:2234]).reshape([20,36]))\n",
    "                                checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(y_pred[2234:2254]).reshape([20]))\n",
    "        \n",
    "                                checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(y_pred[2254:2454]).reshape([10,20]))\n",
    "                                checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(y_pred[2454:2464]).reshape([10]))\n",
    "        \n",
    "                                Brain = CNN(1,activ[0],0,\"kaiming_uniform\")\n",
    "        \n",
    "                                model=copy.deepcopy(Brain)\n",
    "                                model.load_state_dict(checkpoint)\n",
    "                           \n",
    "                           \n",
    "                                \n",
    "                                checkpoint1[\"module_list.0.weight\"]=torch.tensor(np.array(x1[vec][0:200].cpu()).reshape([8, 1, 5, 5]))\n",
    "                                checkpoint1[\"module_list.0.bias\"]=torch.tensor(np.array(x1[vec][200:208].cpu()).reshape([8]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.3.weight\"]=torch.tensor(np.array(x1[vec][208:1408].cpu()).reshape([6, 8, 5, 5]))\n",
    "                                checkpoint1[\"module_list.3.bias\"]=torch.tensor(np.array(x1[vec][1408:1414].cpu()).reshape([6]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.6.weight\"]=torch.tensor(np.array(x1[vec][1414:1510].cpu()).reshape([4, 6, 2, 2]))\n",
    "                                checkpoint1[\"module_list.6.bias\"]=torch.tensor(np.array(x1[vec][1510:1514].cpu()).reshape([4]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.9.weight\"]=torch.tensor(np.array(x1[vec][1514:2234].cpu()).reshape([20,36]))\n",
    "                                checkpoint1[\"module_list.9.bias\"]=torch.tensor(np.array(x1[vec][2234:2254].cpu()).reshape([20]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.11.weight\"]=torch.tensor(np.array(x1[vec][2254:2454].cpu()).reshape([10,20]))\n",
    "                                checkpoint1[\"module_list.11.bias\"]=torch.tensor(np.array(x1[vec][2454:2464].cpu()).reshape([10]))\n",
    "                            \n",
    "                                CNN_x1=copy.deepcopy(Brain)\n",
    "                                CNN_x1.load_state_dict(checkpoint1)\n",
    "                                # Extract weights from a specific layer (e.g., 'fc' layer)\n",
    "                                # for name, param in CNN_x1.named_parameters():\n",
    "                                #     if len(param.shape)>1:\n",
    "                                #         weights = param\n",
    "                                #         weights = weights.view(weights.shape[0], -1)\n",
    "                                #         weights = weights.T @ weights\n",
    "                                #         # Compute eigenvalues of the weight matrix\n",
    "                                #         eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "    \n",
    "                                #         # Extract real and imaginary parts\n",
    "                                #         real_eigenvalues = eigenvalues.real\n",
    "                                #         imag_eigenvalues = eigenvalues.imag\n",
    "    \n",
    "                                #             # Plot histogram of the real parts of the eigenvalues\n",
    "                                #         plt.figure(figsize=(8, 6))\n",
    "                                #         plt.hist(real_eigenvalues, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "                                #         plt.xlabel('Real Part of Eigenvalues')\n",
    "                                #         plt.ylabel('Frequency')\n",
    "                                #         plt.title(f'Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}')\n",
    "                                        \n",
    "                                #         plt.grid(True)\n",
    "                                #         #plt.show()\n",
    "                                #         image_path = results_path+f\"Tracking/Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}.png\"\n",
    "                                #         plt.savefig(image_path)\n",
    "                                        \n",
    "                                #         wandb.log({f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\":  wandb.Image(image_path,caption=f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\")})\n",
    "                                #         plt.close()\n",
    "                            \n",
    "    \n",
    "                                for name, param in CNN_x1.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                        \n",
    "                                \n",
    "                                checkpoint2[\"module_list.0.weight\"]=torch.tensor(np.array(x2[vec][0:200].cpu()).reshape([8, 1, 5, 5]))\n",
    "                                checkpoint2[\"module_list.0.bias\"]=torch.tensor(np.array(x2[vec][200:208].cpu()).reshape([8]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.3.weight\"]=torch.tensor(np.array(x2[vec][208:1408].cpu()).reshape([6, 8, 5, 5]))\n",
    "                                checkpoint2[\"module_list.3.bias\"]=torch.tensor(np.array(x2[vec][1408:1414].cpu()).reshape([6]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.6.weight\"]=torch.tensor(np.array(x2[vec][1414:1510].cpu()).reshape([4, 6, 2, 2]))\n",
    "                                checkpoint2[\"module_list.6.bias\"]=torch.tensor(np.array(x2[vec][1510:1514].cpu()).reshape([4]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.9.weight\"]=torch.tensor(np.array(x2[vec][1514:2234].cpu()).reshape([20,36]))\n",
    "                                checkpoint2[\"module_list.9.bias\"]=torch.tensor(np.array(x2[vec][2234:2254].cpu()).reshape([20]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.11.weight\"]=torch.tensor(np.array(x2[vec][2254:2454].cpu()).reshape([10,20]))\n",
    "                                checkpoint2[\"module_list.11.bias\"]=torch.tensor(np.array(x2[vec][2454:2464].cpu()).reshape([10]))\n",
    "                            \n",
    "                                CNN_x2=copy.deepcopy(Brain)\n",
    "                                CNN_x2.load_state_dict(checkpoint2)\n",
    "                                # Extract weights from a specific layer (e.g., 'fc' layer)\n",
    "                                # for name, param in CNN_x2.named_parameters():\n",
    "                                #     if len(param.shape)>1:\n",
    "                                #         weights = param\n",
    "                                #         weights = weights.view(weights.shape[0], -1)\n",
    "                                #         weights = weights.T @ weights\n",
    "                                #         # Compute eigenvalues of the weight matrix\n",
    "                                #         eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "    \n",
    "                                #         # Extract real and imaginary parts\n",
    "                                #         real_eigenvalues = eigenvalues.real\n",
    "                                #         imag_eigenvalues = eigenvalues.imag\n",
    "    \n",
    "                                #             # Plot histogram of the real parts of the eigenvalues\n",
    "                                #         plt.figure(figsize=(8, 6))\n",
    "                                #         plt.hist(real_eigenvalues, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "                                #         plt.xlabel('Real Part of Eigenvalues')\n",
    "                                #         plt.ylabel('Frequency')\n",
    "                                #         plt.title(f'Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}')\n",
    "                                        \n",
    "                                #         plt.grid(True)\n",
    "                                #         #plt.show()\n",
    "                                #         image_path = results_path+f\"Tracking/Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}.png\"\n",
    "                                #         plt.savefig(image_path)\n",
    "                                        \n",
    "                                #         wandb.log({f\"Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\":  wandb.Image(image_path,caption=f\"Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\")})\n",
    "                                #         plt.close()\n",
    "                                \n",
    "    \n",
    "                                for name, param in CNN_x2.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                        \n",
    "                               \n",
    "                                for name, param in model.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                        \n",
    "                                criterion_CNN0=CrossEntropyLoss()\n",
    "        \n",
    "                                test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                                Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=36, num_workers=0, shuffle=False)\n",
    "        \n",
    "                                _, valid_epoch_acc0,correct_counts,total_counts= validate(model, Ts_DL0,  criterion_CNN0,10,epoch,vec)\n",
    "    \n",
    "                                if len(task3)==10:\n",
    "                                    valid_epoch_acc1=valid_epoch_acc0\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    criterion_CNN1=CrossEntropyLoss()\n",
    "                                    test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                                    Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=36, num_workers=0, shuffle=False)\n",
    "        \n",
    "                                valid_epoch_loss0, valid_epoch_acc1,correct_counts,total_counts= validate(model, Ts_DL1,  criterion_CNN1,10,epoch,vec)\n",
    "                                epoch_nb=-1\n",
    "    \n",
    "                                print(\"Reconstructed cnn acc ID\",valid_epoch_acc0)\n",
    "                                print(\"Reconstructed cnn acc OOD\",valid_epoch_acc1)\n",
    "                                DF.at[track,\"Reconstructed Accuracy ID\"]=valid_epoch_acc0\n",
    "        \n",
    "                                optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "                                schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "                                criterion_CNN=CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "                                train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                                Tr_DLr = DataLoader(dataset=train_IF0, batch_size=36, num_workers=0, shuffle=True)\n",
    "        \n",
    "        \n",
    "                                fine_tune_needed=0\n",
    "                                #FINETUNING\n",
    "                                for epoch_cnn in range(epochs_fn):\n",
    "                                    if epoch_cnn==0:\n",
    "                                        train_epoch_loss, train_epoch_acc = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF,First=True)\n",
    "                                        valid_epoch_loss0FN, valid_epoch_acc0FN,correct_counts,total_counts= validate(model, Ts_DL0,  criterion_CNN,10,epoch,vec)\n",
    "                                        DF.at[track,\"epoch 0\"]=valid_epoch_acc0FN\n",
    "                                    else:\n",
    "                                        train_epoch_loss, train_epoch_acc = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF)\n",
    "                                        valid_epoch_loss0FN, valid_epoch_acc0FN,correct_counts,total_counts= validate(model, Ts_DL0,  criterion_CNN,10,epoch,vec)\n",
    "                                        \n",
    "                                        DF.at[track,f\"epoch {epoch_cnn}\"]=valid_epoch_acc0FN\n",
    "                                    schedulerCNN.step()\n",
    "                                    fine_tune_needed+=1\n",
    "        \n",
    "                                for name, param in model.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                    \n",
    "    \n",
    "                            L_param=[]\n",
    "                            for param in model.parameters():\n",
    "                                m = nn.Flatten(0,-1)\n",
    "                                L_param.append(m(param))\n",
    "                            vec1FN = torch.Tensor()\n",
    "                            for idx in L_param:\n",
    "                                vec1FN = torch.cat((vec1FN, idx.view(-1)))\n",
    "                            vec1FN=vec1FN.detach().cpu()\n",
    "                            vec2=tg[vec].cpu()\n",
    "                            DF.loc[track,Cols[4930:7394]]=vec1FN.tolist()\n",
    "                            MSE=criterion(vec1FN,vec2).item()\n",
    "                            MS1=criterion(vec1FN[:208],vec2[:208]).item()\n",
    "                            MS2=criterion(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "                            MS3=criterion(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "                            MS4=criterion(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "                            MS5=criterion(vec1FN[2254:],vec2[2254:]).item()\n",
    "    \n",
    "    \n",
    "                            #print(\"MSE :\",MS1,MS2,MS3,MS4,MS5,MSE)\n",
    "                            DF.at[track,\"MSE FN\"]=MSE\n",
    "                            DF.at[track,\"MSE 1 FN\"]=MS1\n",
    "                            DF.at[track,\"MSE 2 FN\"]=MS2\n",
    "                            DF.at[track,\"MSE 3 FN\"]=MS3\n",
    "                            DF.at[track,\"MSE 4 FN\"]=MS4\n",
    "                            DF.at[track,\"MSE 5 FN\"]=MS5\n",
    "                            kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "                            KLD=kl_loss(vec1FN,vec2).item() #pred, true\n",
    "                            KL1=kl_loss(vec1FN[:208],vec2[:208]).item()\n",
    "                            KL2=kl_loss(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "                            KL3=kl_loss(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "                            KL4=kl_loss(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "                            KL5=kl_loss(vec1FN[2254:],vec2[2254:]).item()\n",
    "                            #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "                            DF.at[track,\"KL divergence FN\"]=KLD\n",
    "                            DF.at[track,\"KL 1 FN\"]=KL1\n",
    "                            DF.at[track,\"KL 2 FN\"]=KL2\n",
    "                            DF.at[track,\"KL 3 FN\"]=KL3\n",
    "                            DF.at[track,\"KL 4 FN\"]=KL4\n",
    "                            DF.at[track,\"KL 5 FN\"]=KL5\n",
    "    \n",
    "                            WSD=wasserstein_distance(vec1FN,vec2)\n",
    "                            WS1=wasserstein_distance(vec1FN[:208],vec2[:208])\n",
    "                            WS2=wasserstein_distance(vec1FN[208:1414],vec2[208:1414])\n",
    "                            WS3=wasserstein_distance(vec1FN[1414:1514],vec2[1414:1514])\n",
    "                            WS4=wasserstein_distance(vec1FN[1514:2254],vec2[1514:2254])\n",
    "                            WS5=wasserstein_distance(vec1FN[2254:],vec2[2254:])\n",
    "                            #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "    \n",
    "                            DF.at[track,\"WSD FN\"]=WSD\n",
    "                            DF.at[track,\"WS 1 FN\"]=WS1\n",
    "                            DF.at[track,\"WS 2 FN\"]=WS2\n",
    "                            DF.at[track,\"WS 3 FN\"]=WS3\n",
    "                            DF.at[track,\"WS 4 FN\"]=WS4\n",
    "                            DF.at[track,\"WS 5 FN\"]=WS5\n",
    "                            for name, param in mod.named_parameters():\n",
    "                                if name == 'vec2neck.weight':\n",
    "                                    W = param\n",
    "                                    break\n",
    "                            CL=loss_Contractive(mod.vec2neck.weight,vec1FN,vec2, output[1], 0.00001)        \n",
    "                            #print(\"Contractive :\",CL.cpu().item())\n",
    "                            DF.at[track,\"contractive distance FN\"]=CL.cpu().item()\n",
    "                            Norm1=np.sum(np.abs(vec1FN.numpy()))\n",
    "                            N11=np.sum(np.abs(vec1FN[:208].numpy()))\n",
    "                            N12=np.sum(np.abs(vec1FN[208:1414].numpy()))\n",
    "                            N13=np.sum(np.abs(vec1FN[1414:1514].numpy()))\n",
    "                            N14=np.sum(np.abs(vec1FN[1514:2254].numpy()))\n",
    "                            N15=np.sum(np.abs(vec1FN[2254:].numpy()))\n",
    "                            #print(\"Weight pred L1: \",Norm1, N11,N12,N13,N14,N15)\n",
    "                            DF.at[track,\"N1 FN\"]=Norm1\n",
    "                            DF.at[track,\"N11 FN\"]=N11\n",
    "                            DF.at[track,\"N12 FN\"]=N12\n",
    "                            DF.at[track,\"N13 FN\"]=N13\n",
    "                            DF.at[track,\"N14 FN\"]=N14\n",
    "                            DF.at[track,\"N15 FN\"]=N15\n",
    "                            Norm2=np.sum(np.abs(vec2.numpy()))\n",
    "                            N21=np.sum(np.abs(vec2[:208].numpy()))\n",
    "                            N22=np.sum(np.abs(vec2[208:1414].numpy()))\n",
    "                            N23=np.sum(np.abs(vec2[1414:1514].numpy()))\n",
    "                            N24=np.sum(np.abs(vec2[1514:2254].numpy()))\n",
    "                            N25=np.sum(np.abs(vec2[2254:].numpy()))\n",
    "                            #print(\"Weight GT L1: \",Norm2, N21,N22,N23,N24,N25)\n",
    "                            DF.at[track,\"N2 FN\"]=Norm2\n",
    "                            DF.at[track,\"N21 FN\"]=N21\n",
    "                            DF.at[track,\"N22 FN\"]=N22\n",
    "                            DF.at[track,\"N23 FN\"]=N23\n",
    "                            DF.at[track,\"N24 FN\"]=N24\n",
    "                            DF.at[track,\"N25 FN\"]=N25\n",
    "                            #print(\"Saturation in pred \",100*sum(1 for x in vec1FN if x > 0.95 or x<0.05)/len(vec1FN),\"% \\t saturaion in target\",100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2),\"%\")\n",
    "                            #print(\"LWLN \",LW(vec1FN,vec2).item())\n",
    "                            DF.at[track,\"saturated in pred FN(%)\"]=100*sum(1 for x in vec1FN if x > 0.95 or x<0.05)/len(vec1FN)\n",
    "                            DF.at[track,\"saturated in GT FN(%)\"]=100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2)\n",
    "                            DF.at[track,\"LWLN FN\"]=LWs(vec1FN,vec2).cpu().item()\n",
    "    \n",
    "                            \n",
    "                            track=track+1\n",
    "                    \n",
    "                    try :\n",
    "                        print(f\"{str(datetime.datetime.now())} begin topo\")\n",
    "                        # Define filter function â€“ can be any scikit-learn transformer\n",
    "                        filter_func = Projection(columns=[0,1])\n",
    "                        # Define cover\n",
    "                        cover = CubicalCover(n_intervals=12, overlap_frac=0.15)\n",
    "                        # Choose clustering algorithm â€“ default is DBSCAN\n",
    "                        clusterer = DBSCAN()\n",
    "        \n",
    "                        # Configure parallelism of clustering step\n",
    "                        n_jobs = 8\n",
    "        \n",
    "                        GT=DF[Cols[2:2466]]\n",
    "                        dataG=GT.to_numpy()\n",
    "                        #print(Cols[2],Cols[2466],Cols[4930])\n",
    "                        PD=DF[Cols[2466:4930]]\n",
    "                        dataP=PD.to_numpy()\n",
    "        \n",
    "                        FN=DF[Cols[4930:7394]]\n",
    "                        dataF=FN.to_numpy()\n",
    "                        # Initialise pipeline\n",
    "                        pipe = make_mapper_pipeline(filter_func=filter_func,cover=cover,clusterer=clusterer,verbose=False,n_jobs=n_jobs)\n",
    "        \n",
    "        \n",
    "                        plotly_params = {\"node_trace\": {\"marker_colorscale\": \"Blues\"}}\n",
    "                        fig = plot_static_mapper_graph(pipe, dataG, color_data=dataG, plotly_params=plotly_params)\n",
    "        \n",
    "                        fig1 = go.Figure(fig)\n",
    "                        #fig1.write_image(results_path+f\"Tracking/Mapper GT Graph 10 test set batches {epoch}.png\")\n",
    "                        wandb.log({f\"Mapper GTruth Graph 10 test set {nb_test} batches \": wandb.Plotly(fig1)})\n",
    "        \n",
    "        \n",
    "        \n",
    "                        fig = plot_static_mapper_graph(pipe, dataP, color_data=dataP, plotly_params=plotly_params)\n",
    "        \n",
    "                        fig1 = go.Figure(fig)\n",
    "                        #fig1.write_image(results_path+f\"Tracking/Mapper Pred Graph 10 test set batches {epoch}.png\")\n",
    "                        wandb.log({f\"Mapper Pred Graph 10 test set {nb_test} batches \": wandb.Plotly(fig1)})\n",
    "        \n",
    "        \n",
    "                        fig = plot_static_mapper_graph(pipe, dataF, color_data=dataF, plotly_params=plotly_params)\n",
    "        \n",
    "                        fig1 = go.Figure(fig)\n",
    "                        #fig1.write_image(results_path+f\"Tracking/Mapper FN Graph 10 test set batches {epoch}.png\")\n",
    "                        wandb.log({f\"Mapper Finetuned Graph test set {nb_test} batches \": wandb.Plotly(fig1)})\n",
    "                        print(f\"{str(datetime.datetime.now())} end topo\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"{str(datetime.datetime.now())} {e}\")\n",
    "                    wandb.log({\"Epoch 0 Test CNN accuracy on 10 batches \": DF['epoch 0'].mean()})\n",
    "                    wandb.log({\"Epoch 1 Test CNN accuracy on 10 batches \": DF['epoch 1'].mean()})\n",
    "                    wandb.log({\"Epoch 2 Test CNN accuracy on 10 batches \": DF['epoch 2'].mean()})\n",
    "                    wandb.log({\"Epoch 3 Test CNN accuracy on 10 batches \": DF['epoch 3'].mean()})\n",
    "                    wandb.log({\"Epoch 4 Test CNN accuracy on 10 batches \": DF['epoch 4'].mean()})\n",
    "    \n",
    "    \n",
    "    \n",
    "                if (epoch+1)%50==0:\n",
    "                    DF.to_csv(results_path+f\"Tracking/AE epoch {combination} {epoch}.csv\")\n",
    "                    torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),\n",
    "                                'optimizerENC1_state_dict':  optimizerEnc1.state_dict() ,\n",
    "                                'optimizerENC2_state_dict':optimizerEnc2.state_dict(),\n",
    "                                'optimizerDense_state_dict':optimizerDense.state_dict(),\n",
    "                                'optimizerDec_state_dict': optimizerDec.state_dict(),\n",
    "                                'Batch Loss':used_Loss.detach().cpu().item(),\n",
    "                                'validation entire testset MSE':loss_value.detach().cpu().item()},\n",
    "                                results_path+'AE epoch {} {} {}.pth'.format(combination,track,epoch))\n",
    "                    torch.cuda.empty_cache()\n",
    "                if (epoch+1)==num_epochs:\n",
    "                    del(mod)\n",
    "                    del(optimizerEnc1)\n",
    "                    del(optimizerEnc2) \n",
    "                    del(optimizerDense)\n",
    "                    del(optimizerDec)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                    gc.collect()\n",
    "                \n",
    "            wandb.finish()\n",
    "            track=+1\n",
    "        #     return loss_tr\n",
    "        # study= optuna.create_study(direction=\"minimize\",storage=storage)\n",
    "        # study.optimize(objective,n_trials=2,callbacks=[lambda study, trial: gc.collect()]+[logging_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a88a64-f4d0-4e9f-8eb8-48cf10974baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loos[\"JS Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c25ff4-879d-49af-8ba1-354aaf791451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type of used_Loss: {type(used_Loss)}\")\n",
    "print(f\"Is tensor: {torch.is_tensor(used_Loss)}\")\n",
    "if torch.is_tensor(used_Loss):\n",
    "    print(f\"Requires grad: {used_Loss.requires_grad}\")\n",
    "\n",
    "accelerator.backward(used_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479903cd-536d-4458-840d-dc2e9cf45a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.shape , j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c443d5-6158-4b9a-89bf-1fd23053107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of NaN values\n",
    "nan_indices = np.where(np.isnan(dataP))[0]\n",
    "print(nan_indices)  # [2 4]\n",
    "\n",
    "# Delete those indices\n",
    "dataP = np.delete(dataP, nan_indices)\n",
    "print(dataP.shape)  # [1. 2. 4. 6.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a310d6-66a6-4540-97f5-d1453cee28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdebf2e-ab27-4366-97d2-a9ee01c3bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52af697-d382-4c42-8c0a-5aad90149db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fac97-458e-4a1c-83bc-3686ab424a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_func = Projection(columns=[0,1])\n",
    "# Define cover\n",
    "cover = CubicalCover(n_intervals=12, overlap_frac=0.5)\n",
    "# Choose clustering algorithm â€“ default is DBSCAN\n",
    "clusterer = DBSCAN()\n",
    "\n",
    "# Configure parallelism of clustering step\n",
    "n_jobs = 8\n",
    "dataG = np.random.random((30, 30))\n",
    "pipe = make_mapper_pipeline(filter_func=filter_func,cover=cover,clusterer=clusterer,verbose=False,n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "plotly_params = {\"node_trace\": {\"marker_colorscale\": \"Blues\"}}\n",
    "fig = plot_static_mapper_graph(pipe, dataG, color_data=dataG, plotly_params=plotly_params)\n",
    "\n",
    "fig1 = go.Figure(fig)\n",
    "#fig1.write_image(results_path+f\"Tracking/Mapper GT Graph 10 test set batches {epoch}.png\")\n",
    "wandb.log({\"Mapper GT Graph 10 test set batches\": wandb.Image(fig1)\n",
    "\n",
    "\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad648e9-8f42-430a-b126-622afd46dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly==5.11.0\n",
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840484e7-edc8-4848-9e90-1d1523628a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues\n",
    "profiler\n",
    "Multipers\n",
    "Bottleneck distance\n",
    "Optimal transport mapping imagery\n",
    "Shapley\n",
    "Add 3 dicts instead of 1 to detect significant relative change and launch Reconstructions/checkpointing\n",
    "Kolmagrov-Arnold functions\n",
    "Matrix diagonalization github\n",
    "RL\n",
    "Gromov-wassertstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895af7a-8eb6-4a02-a4e3-1b28f7910ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdb4470-103e-45c6-8b60-2cbdcd6f1ebd",
   "metadata": {},
   "source": [
    "# Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e7320-8361-4a92-8b2b-b415fd8e4ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f34f8-4a68-4aea-bb8c-6e70dae9b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\usepackage{amsmath}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{algpseudocode}\n",
    "\\usepackage{algorithm}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259feab-6027-473e-aff1-58ed34555aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a065f8a-4123-4367-a2e8-2596a86fe402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b45d2-e66a-4b3b-b2fe-aabec4a283b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe753f-5119-4ffe-baae-7ba973b2e199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacc22e-54f5-4726-8ca7-be6226b175c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb472966-e3ad-4710-ae57-252ac8dcd3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a6240-8f36-4e14-8da0-31e2f94c60f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392de71-b929-491d-aeb5-0ee73970762c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96bab4-0113-4840-9407-2ceab23465b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e45fa8-ff87-4274-b636-a516f517aa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07861397-33b0-4650-92e4-7c7c65bfe67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2cd99-808d-450a-8659-167ce19442cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ea216-9340-4bd4-81cc-336a8fade191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8537e-60d2-41dd-8058-bde802082361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9091e63-d333-4410-bac9-a867e7a06dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0aac87-e079-47b3-a786-df5246aeef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475617f-df66-446a-8db9-2d8c3d4e0451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
