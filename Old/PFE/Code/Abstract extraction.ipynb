{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827b3289-693a-44bd-9860-688990c7e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install: invalid option -- 'U'\n",
      "Try 'install --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "#!pip install PyPDF2\n",
    "#!pip install nltk\n",
    "#!python -m nltk.downloader stopwords\n",
    "#!python -m nltk.downloader punkt\n",
    "!install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a198d4-f811-4a4d-bcfb-f74164c8fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "df = pd.DataFrame({'Article': pd.Series(dtype='str'),\n",
    "                   'Abstract': pd.Series(dtype='str'),\n",
    "                   'labels': pd.Series(dtype='str'),\n",
    "                   })\n",
    "df\n",
    "dfNA=pd.DataFrame({'Article': pd.Series(dtype='str'),\n",
    "                   'labels': pd.Series(dtype='str'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa901e1-96d4-4342-933e-afd9d9c30a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Simple Framework for Contrastive Learning of Visual Representations.pdf',\n",
       " 'Training Networks in Null Space of Feature Covariance for Continual Learning.pdf',\n",
       " 'Federated Unsupervised Representation Learning.pdf',\n",
       " 'Model-agnostic round-optimal federated learning via knowledge transfer.pdf',\n",
       " 'On information and sufficiency.pdf',\n",
       " 'Learning Deep Parsimonious Representations.pdf',\n",
       " 'Reading Digits in Natural Images with Unsupervised Feature Learning.pdf',\n",
       " 'Communication-Efficient Learning of Deep Networks.pdf',\n",
       " 'Effects of Degradations on Deep Neural Network.pdf',\n",
       " 'Learning without Forgetting.pdf',\n",
       " 'Going deeper with convolutions.pdf',\n",
       " 'FEDERATED OPTIMIZATION IN HETEROGENEOUS NETWORKS.pdf',\n",
       " 'The.MIT.Press.Dataset.Shift.in.Machine.Learning.Feb.2009.eBook-DDU.pdf',\n",
       " 'Multi-site fMRI Analysis Using Privacy-preserving Federated Learning and Domain.pdf',\n",
       " 'The information bottleneck method.pdf',\n",
       " 'The Cityscapes Dataset for Semantic Urban Scene Understanding.pdf',\n",
       " 'Microsoft COCO--Common Objects in Context.pdf',\n",
       " 'Federated Machine Learning--Concept and Applications.pdf',\n",
       " 'Privacy-preserving technology to help millions of people: Federated prediction model for stroke.pdf',\n",
       " 'Ovecoming forgetting in federated learning on non-iid data.pdf',\n",
       " 'Data-free knowledge distillation for heterogeneous federated learning.pdf',\n",
       " 'FedDG--Federated Domain Generalization on Medical Image Segmentation.pdf',\n",
       " 'FEDERATED LEARNING FOR MOBILE KEYBOARD PREDICTION.pdf',\n",
       " 'FedMD--Heterogenous Federated Learning.pdf',\n",
       " 'Overcoming_catastrophic_forgetting_in_neural_netwok.pdf',\n",
       " 'A unifying view on dataset shift in classification.pdf',\n",
       " 'Deep Hashing Network for Unsupervised Domain Adaptation.pdf',\n",
       " 'Momentum Contrast for Unsupervised Visual Representation Learning.pdf',\n",
       " 'Database for handwritten text recognition research.pdf',\n",
       " 'Dimensionality Reduction by Learning an Invariant Mapping.pdf',\n",
       " 'Barlow Twins--Self-Supervised Learning via Redundancy Reduction.pdf',\n",
       " 'Exploring Simple Siamese Representation Learning.pdf',\n",
       " 'Advances and open problems in federated learning.pdf',\n",
       " 'Personalized Federated Learning with Moreau Envolopes.pdf',\n",
       " 'Overcoming Forgetting in Federated Learning on non i.i.d data.pdf',\n",
       " 'Three scenarios for continual learning.pdf',\n",
       " 'Federated learning on non-iid data silos--An experimental study.pdf',\n",
       " 'FEDBN--FEDERATED LEARNING ON NON-IID FEATURES VIA LOCAL BATCH NORMALIZATION.pdf',\n",
       " 'Fashion-MNIST--a Novel Image Dataset for Benchmarking Machine Learning Algorithms.pdf',\n",
       " 'Gradient-based learning applied to document recognition.pdf',\n",
       " 'FEDERATED LEARNING--STRATEGIES FOR IMPROVING COMMUNICATION EFFICIENTLY.pdf',\n",
       " 'Bootstrap Your Own Latent.pdf',\n",
       " 'Distilling the Knowledge in a Neural Network.pdf',\n",
       " 'Learning Representations by Predicting Bags of Visual Words.pdf',\n",
       " 'Overcoming catastrophic forgetting in neural networks.pdf',\n",
       " 'ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION.pdf',\n",
       " 'WHAT MAKES INSTANCE DISCRIMINATION GOOD for transfer learning.pdf',\n",
       " 'FEDERATED ADVERSARIAL DOMAIN ADAPTATION.pdf',\n",
       " 'MobileNets: Efficient Convolutional Neural Networks for Mobile Vision.pdf',\n",
       " 'Federated learning--Challenges, methods, and future.pdf',\n",
       " 'Model-Contrastive Federated Learning.pdf',\n",
       " 'Model Compression.pdf',\n",
       " 'WebVision Database--Visual Learning and Understanding from Web Data.pdf',\n",
       " 'FedVision--An Online Visual Object Detection Platform Powered by.pdf',\n",
       " 'Towards Non-I.I.D. and Invisible Data with FedNAS.pdf',\n",
       " 'Ensemble Distillation for Robust Model Fusion in.pdf',\n",
       " 'Unsupervised Visual Representation Learning by Context Prediction.pdf',\n",
       " 'Distilling a Neural Network Into a Soft Decision.pdf',\n",
       " 'Understanding Self-Supervised Learning Dynamics without Contrastive Pairs.pdf',\n",
       " 'FBNet--Hardware-Aware Efficient ConvNet Design.pdf',\n",
       " 'ImageNet_Large_Scale_Visual_Recognition_Challenge.pdf',\n",
       " 'EfficientNet--Rethinking Model Scaling for Convolutional Neural Networks.pdf',\n",
       " 'SecureML--A System for Scalable Privacy-Preserving.pdf',\n",
       " 'Parametric Instance Classification for Unsupervised.pdf',\n",
       " 'An Empirical Investigation of Catastrophic Forgetting in gradient-based networks.pdf',\n",
       " 'Communication-Efficient Federated Distillation.pdf',\n",
       " 'Federated learning with non-iid.pdf',\n",
       " 'Cronus: Robust and Heterogeneous Collaborative.pdf',\n",
       " 'Federated Learning for Personalized Humor Recognition.pdf',\n",
       " 'Learning Multiple Layers of Features from Tiny Images.pdf',\n",
       " 'CATASTROPHIC INTERFERENCE IN CONNECTIONIST NETWORKS--THE SEQUENTIAL Learning problem.pdf',\n",
       " 'Group knowledge transfer: Federated learning of large cnns.pdf',\n",
       " 'Unsupervised Embedding Learning via Invariant and Spreading.pdf',\n",
       " 'An Empirical Study of Training Self-Supervised Vision Transformers.pdf',\n",
       " 'Federated Mutual Learning.pdf',\n",
       " 'A Survey on Transfer Learning.pdf',\n",
       " 'Specialized federated learning using a mixture of experts.pdf',\n",
       " 'Federated Optmization--Distributed Machine Learning for On-Device Intelligence.pdf',\n",
       " 'A continual learning survey.pdf',\n",
       " 'NETTAILOR--Tuning the architecture, not just the weights.pdf',\n",
       " 'Rainbow Memory: Continual Learning with a Memory of Diverse Samples.pdf',\n",
       " 'Toward Understanding Catastrophic Forgetting in Continual Learning.pdf',\n",
       " 'Federated semi-supervised learning with inter-client consistency & disjoint learning .pdf',\n",
       " 'Deep Residual Learning for Image Recognition.pdf',\n",
       " 'Progressive Neural Networks.pdf',\n",
       " 'Think Locally, Act Globally-- Fderated Learning with local and global representations.pdf',\n",
       " 'Robust Federated Learning with Noisy and Heterogeneous Clients.pdf',\n",
       " 'iCaRL--Incremental Classifier and Representation Learning.pdf',\n",
       " 'Resource-Constrained Federated Learning with Heterogeneous.pdf',\n",
       " 'Rotate your Networks Better Weight Consolidation and Less Catastrophic Forgetting.pdf',\n",
       " 'Federated Model Distillation with Noise-Free Differential Privacy.pdf',\n",
       " 'PackNet--Adding Multiple Tasks to a Single Network by Iterative Prunin.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Open a file\n",
    "path = \"/home/aymen/Desktop/PFE/Articles/Refrences 1/\"\n",
    "dirs = os.listdir( path )\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05a8274-2f1a-4d14-952d-2adeae553a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Abstract article duplicates : 0\n",
      "#Abstractless article duplicates : 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Simple Framework for Contrastive Learning of...</td>\n",
       "      <td>Framework for Contrastive Learning of Visual ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Training Networks in Null Space of Feature Cov...</td>\n",
       "      <td>Networks in Null Space of Feature Covariance ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federated Unsupervised Representation Learning...</td>\n",
       "      <td>d Unsupervised Representation LearningFengda Z...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model-agnostic round-optimal federated learnin...</td>\n",
       "      <td>view as a conference paper at ICLR 2021MODEL -...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning Deep Parsimonious Representations.pdf</td>\n",
       "      <td>Deep Parsimonious RepresentationsRenjie Liao1...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reading Digits in Natural Images with Unsuperv...</td>\n",
       "      <td>Digits in Natural Imageswith Unsupervised Feat...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Communication-Efficient Learning of Deep Netwo...</td>\n",
       "      <td>ation-Efﬁcient Learning of Deep Networksfrom D...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Effects of Degradations on Deep Neural Network...</td>\n",
       "      <td>s of Degradations on Deep Neural NetworkArchit...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Learning without Forgetting.pdf</td>\n",
       "      <td>ng without ForgettingZhizhong Li, Derek Hoiem,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Going deeper with convolutions.pdf</td>\n",
       "      <td>eper with convolutionsChristian SzegedyGoogle ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FEDERATED OPTIMIZATION IN HETEROGENEOUS NETWOR...</td>\n",
       "      <td>D OPTIMIZATION IN HETEROGENEOUS NETWORKSTian L...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Cityscapes Dataset for Semantic Urban Scen...</td>\n",
       "      <td>on. Scenerecognition aims to determine the ove...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Microsoft COCO--Common Objects in Context.pdf</td>\n",
       "      <td>oft COCO: Common Objects in ContextTsung-Yi Li...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Federated Machine Learning--Concept and Applic...</td>\n",
       "      <td>ted Machine Learning: Concept and Applications...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Privacy-preserving technology to help millions...</td>\n",
       "      <td>Preserving Technology to Help Millions of Peop...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ovecoming forgetting in federated learning on ...</td>\n",
       "      <td>om Others and Be Yourself in Heterogeneous Fed...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data-free knowledge distillation for heterogen...</td>\n",
       "      <td>e Knowledge Distillation for Heterogeneous Fed...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FedDG--Federated Domain Generalization on Medi...</td>\n",
       "      <td>ederated Domain Generalization on Medical Imag...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FEDERATED LEARNING FOR MOBILE KEYBOARD PREDICT...</td>\n",
       "      <td>D LEARNING FOR MOBILE KEYBOARD PREDICTIONAndre...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FedMD--Heterogenous Federated Learning.pdf</td>\n",
       "      <td>eterogenous Federated Learningvia Model Distil...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A unifying view on dataset shift in classifica...</td>\n",
       "      <td>The ﬁeld of dataset shift has received a growi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Deep Hashing Network for Unsupervised Domain A...</td>\n",
       "      <td>hing Network for Unsupervised Domain Adaptatio...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Momentum Contrast for Unsupervised Visual Repr...</td>\n",
       "      <td>Contrast for Unsupervised Visual Representati...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Dimensionality Reduction by Learning an Invari...</td>\n",
       "      <td>nality Reduction by Learning anInvariantMappin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Barlow Twins--Self-Supervised Learning via Red...</td>\n",
       "      <td>wins: Self-Supervised Learning via Redundancy ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Exploring Simple Siamese Representation Learni...</td>\n",
       "      <td>g Simple Siamese Representation LearningXinlei...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Advances and open problems in federated learni...</td>\n",
       "      <td>and Open Problems in Federated LearningPeter ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Personalized Federated Learning with Moreau En...</td>\n",
       "      <td>ized Federated Learning with MoreauEnvelopesCa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Overcoming Forgetting in Federated Learning on...</td>\n",
       "      <td>ng Forgetting in Federated Learning onNon-IID ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Three scenarios for continual learning.pdf</td>\n",
       "      <td>enarios for continual learningGido M. van de V...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Federated learning on non-iid data silos--An e...</td>\n",
       "      <td>d Learning on Non-IID Data Silos: AnExperiment...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>FEDBN--FEDERATED LEARNING ON NON-IID FEATURES ...</td>\n",
       "      <td>d as a conference paper at ICLR 2021FEDBN: F E...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fashion-MNIST--a Novel Image Dataset for Bench...</td>\n",
       "      <td>08.07747v2  [cs.LG]  15 Sep 2017Fashion-MNIST:...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>FEDERATED LEARNING--STRATEGIES FOR IMPROVING C...</td>\n",
       "      <td>D LEARNING : STRATEGIES FOR IMPROVINGCOMMUNICA...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Bootstrap Your Own Latent.pdf</td>\n",
       "      <td>p Your Own LatentA New Approach to Self-Superv...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Distilling the Knowledge in a Neural Network.pdf</td>\n",
       "      <td>view of the knowledge, that frees it from any ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Learning Representations by Predicting Bags of...</td>\n",
       "      <td>Representations by Predicting Bags of Visual ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Overcoming catastrophic forgetting in neural n...</td>\n",
       "      <td>ng Forgetting in Federated Learning onNon-IID ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION.pdf</td>\n",
       "      <td>d as a conference paper at ICLR 2015ADAM : A M...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>WHAT MAKES INSTANCE DISCRIMINATION GOOD for tr...</td>\n",
       "      <td>d as a conference paper at ICLR 2021WHAT MAKES...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>FEDERATED ADVERSARIAL DOMAIN ADAPTATION.pdf</td>\n",
       "      <td>d as a conference paper at ICLR 2020FEDERATED ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MobileNets: Efficient Convolutional Neural Net...</td>\n",
       "      <td>ts: Efﬁcient Convolutional Neural Networks for...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Federated learning--Challenges, methods, and f...</td>\n",
       "      <td>d Learning:Challenges, Methods, and Future Dir...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Model-Contrastive Federated Learning.pdf</td>\n",
       "      <td>ntrastive Federated LearningQinbin LiNational ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Model Compression.pdf</td>\n",
       "      <td>mpressionCristian Bucil ˘aComputer ScienceCorn...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>WebVision Database--Visual Learning and Unders...</td>\n",
       "      <td>08.02862v1  [cs.CV]  9 Aug 2017WebVision Datab...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>FedVision--An Online Visual Object Detection P...</td>\n",
       "      <td>n: An Online Visual Object Detection Platform ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Towards Non-I.I.D. and Invisible Data with Fed...</td>\n",
       "      <td>Non-I.I.D. and Invisible Data with FedNAS:Fede...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Ensemble Distillation for Robust Model Fusion ...</td>\n",
       "      <td>Distillation for Robust Model Fusion inFedera...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Unsupervised Visual Representation Learning by...</td>\n",
       "      <td>ised Visual Representation Learning by Context...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Distilling a Neural Network Into a Soft Decisi...</td>\n",
       "      <td>ng a Neural Network Into a Soft DecisionTreeNi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Understanding Self-Supervised Learning Dynamic...</td>\n",
       "      <td>nding Self-Supervised Learning Dynamics withou...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>FBNet--Hardware-Aware Efficient ConvNet Design...</td>\n",
       "      <td>ardware-Aware Efﬁcient ConvNet Designvia Diffe...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>EfficientNet--Rethinking Model Scaling for Con...</td>\n",
       "      <td>Net: Rethinking Model Scaling for Convolutiona...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SecureML--A System for Scalable Privacy-Preser...</td>\n",
       "      <td>: A System for Scalable Privacy-PreservingMach...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Parametric Instance Classification for Unsuper...</td>\n",
       "      <td>ic Instance Classiﬁcation for UnsupervisedVisu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>An Empirical Investigation of Catastrophic For...</td>\n",
       "      <td>ical Investigation of Catastrophic Forgetting ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Communication-Efficient Federated Distillation...</td>\n",
       "      <td>ication-Efﬁcient Federated DistillationFelix S...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Federated learning with non-iid.pdf</td>\n",
       "      <td>d Learning with Non-IID DataYue Zhao*yzhao727@...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Cronus: Robust and Heterogeneous Collaborative...</td>\n",
       "      <td>12.11279v1  [stat.ML]  24 Dec 2019Cronus: Robu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Federated Learning for Personalized Humor Reco...</td>\n",
       "      <td>d Learning for Personalized Humor RecognitionX...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Group knowledge transfer: Federated learning o...</td>\n",
       "      <td>owledge Transfer:Federated Learning of Large C...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Unsupervised Embedding Learning via Invariant ...</td>\n",
       "      <td>ised Embedding Learning via Invariant and Spre...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>An Empirical Study of Training Self-Supervised...</td>\n",
       "      <td>ical Study of Training Self-Supervised Vision ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Federated Mutual Learning.pdf</td>\n",
       "      <td>d Mutual LearningTao ShenDepartment of Compute...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>A Survey on Transfer Learning.pdf</td>\n",
       "      <td>on Transfer LearningSinno Jialin Pan and Qian...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Specialized federated learning using a mixture...</td>\n",
       "      <td>zed federated learning using a mixture of expe...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Federated Optmization--Distributed Machine Lea...</td>\n",
       "      <td>d Optimization:Distributed Machine Learning fo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>A continual learning survey.pdf</td>\n",
       "      <td>09.08383v3  [cs.CV]  16 Apr 2021Copyright (c) ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>NETTAILOR--Tuning the architecture, not just t...</td>\n",
       "      <td>R : Tuning the architecture, not just the weig...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Rainbow Memory: Continual Learning with a Memo...</td>\n",
       "      <td>Memory: Continual Learning with a Memory of Di...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Toward Understanding Catastrophic Forgetting i...</td>\n",
       "      <td>nderstanding Catastrophic Forgetting in Contin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Federated semi-supervised learning with inter-...</td>\n",
       "      <td>d as a conference paper at ICLR 2021FEDERATED ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Deep Residual Learning for Image Recognition.pdf</td>\n",
       "      <td>idual Learning for Image RecognitionKaiming He...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Progressive Neural Networks.pdf</td>\n",
       "      <td>ive Neural NetworksAndrei A. Rusu*, Neil C. Ra...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Think Locally, Act Globally-- Fderated Learnin...</td>\n",
       "      <td>cally, Act Globally:Federated Learning with Lo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Robust Federated Learning with Noisy and Heter...</td>\n",
       "      <td>ederated Learning with Noisy and Heterogeneous...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>iCaRL--Incremental Classifier and Representati...</td>\n",
       "      <td>ncremental Classiﬁer and Representation Learni...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Resource-Constrained Federated Learning with H...</td>\n",
       "      <td>-Constrained Federated Learning with Heterogen...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Federated Model Distillation with Noise-Free D...</td>\n",
       "      <td>d Model Distillation with Noise-Free Different...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>PackNet--Adding Multiple Tasks to a Single Net...</td>\n",
       "      <td>Adding Multiple Tasks to a Single Network by ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Article                                           Abstract labels\n",
       "0   A Simple Framework for Contrastive Learning of...   Framework for Contrastive Learning of Visual ...       \n",
       "1   Training Networks in Null Space of Feature Cov...   Networks in Null Space of Feature Covariance ...       \n",
       "2   Federated Unsupervised Representation Learning...  d Unsupervised Representation LearningFengda Z...       \n",
       "3   Model-agnostic round-optimal federated learnin...  view as a conference paper at ICLR 2021MODEL -...       \n",
       "4      Learning Deep Parsimonious Representations.pdf   Deep Parsimonious RepresentationsRenjie Liao1...       \n",
       "5   Reading Digits in Natural Images with Unsuperv...  Digits in Natural Imageswith Unsupervised Feat...       \n",
       "6   Communication-Efficient Learning of Deep Netwo...  ation-Efﬁcient Learning of Deep Networksfrom D...       \n",
       "7   Effects of Degradations on Deep Neural Network...  s of Degradations on Deep Neural NetworkArchit...       \n",
       "8                     Learning without Forgetting.pdf  ng without ForgettingZhizhong Li, Derek Hoiem,...       \n",
       "9                  Going deeper with convolutions.pdf  eper with convolutionsChristian SzegedyGoogle ...       \n",
       "10  FEDERATED OPTIMIZATION IN HETEROGENEOUS NETWOR...  D OPTIMIZATION IN HETEROGENEOUS NETWORKSTian L...       \n",
       "11  The Cityscapes Dataset for Semantic Urban Scen...  on. Scenerecognition aims to determine the ove...       \n",
       "12      Microsoft COCO--Common Objects in Context.pdf  oft COCO: Common Objects in ContextTsung-Yi Li...       \n",
       "13  Federated Machine Learning--Concept and Applic...  ted Machine Learning: Concept and Applications...       \n",
       "14  Privacy-preserving technology to help millions...  Preserving Technology to Help Millions of Peop...       \n",
       "15  Ovecoming forgetting in federated learning on ...  om Others and Be Yourself in Heterogeneous Fed...       \n",
       "16  Data-free knowledge distillation for heterogen...  e Knowledge Distillation for Heterogeneous Fed...       \n",
       "17  FedDG--Federated Domain Generalization on Medi...  ederated Domain Generalization on Medical Imag...       \n",
       "18  FEDERATED LEARNING FOR MOBILE KEYBOARD PREDICT...  D LEARNING FOR MOBILE KEYBOARD PREDICTIONAndre...       \n",
       "19         FedMD--Heterogenous Federated Learning.pdf  eterogenous Federated Learningvia Model Distil...       \n",
       "20  A unifying view on dataset shift in classifica...  The ﬁeld of dataset shift has received a growi...       \n",
       "21  Deep Hashing Network for Unsupervised Domain A...  hing Network for Unsupervised Domain Adaptatio...       \n",
       "22  Momentum Contrast for Unsupervised Visual Repr...   Contrast for Unsupervised Visual Representati...       \n",
       "23  Dimensionality Reduction by Learning an Invari...  nality Reduction by Learning anInvariantMappin...       \n",
       "24  Barlow Twins--Self-Supervised Learning via Red...  wins: Self-Supervised Learning via Redundancy ...       \n",
       "25  Exploring Simple Siamese Representation Learni...  g Simple Siamese Representation LearningXinlei...       \n",
       "26  Advances and open problems in federated learni...   and Open Problems in Federated LearningPeter ...       \n",
       "27  Personalized Federated Learning with Moreau En...  ized Federated Learning with MoreauEnvelopesCa...       \n",
       "28  Overcoming Forgetting in Federated Learning on...  ng Forgetting in Federated Learning onNon-IID ...       \n",
       "29         Three scenarios for continual learning.pdf  enarios for continual learningGido M. van de V...       \n",
       "30  Federated learning on non-iid data silos--An e...  d Learning on Non-IID Data Silos: AnExperiment...       \n",
       "31  FEDBN--FEDERATED LEARNING ON NON-IID FEATURES ...  d as a conference paper at ICLR 2021FEDBN: F E...       \n",
       "32  Fashion-MNIST--a Novel Image Dataset for Bench...  08.07747v2  [cs.LG]  15 Sep 2017Fashion-MNIST:...       \n",
       "33  FEDERATED LEARNING--STRATEGIES FOR IMPROVING C...  D LEARNING : STRATEGIES FOR IMPROVINGCOMMUNICA...       \n",
       "34                      Bootstrap Your Own Latent.pdf  p Your Own LatentA New Approach to Self-Superv...       \n",
       "35   Distilling the Knowledge in a Neural Network.pdf  view of the knowledge, that frees it from any ...       \n",
       "36  Learning Representations by Predicting Bags of...   Representations by Predicting Bags of Visual ...       \n",
       "37  Overcoming catastrophic forgetting in neural n...  ng Forgetting in Federated Learning onNon-IID ...       \n",
       "38     ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION.pdf  d as a conference paper at ICLR 2015ADAM : A M...       \n",
       "39  WHAT MAKES INSTANCE DISCRIMINATION GOOD for tr...  d as a conference paper at ICLR 2021WHAT MAKES...       \n",
       "40        FEDERATED ADVERSARIAL DOMAIN ADAPTATION.pdf  d as a conference paper at ICLR 2020FEDERATED ...       \n",
       "41  MobileNets: Efficient Convolutional Neural Net...  ts: Efﬁcient Convolutional Neural Networks for...       \n",
       "42  Federated learning--Challenges, methods, and f...  d Learning:Challenges, Methods, and Future Dir...       \n",
       "43           Model-Contrastive Federated Learning.pdf  ntrastive Federated LearningQinbin LiNational ...       \n",
       "44                              Model Compression.pdf  mpressionCristian Bucil ˘aComputer ScienceCorn...       \n",
       "45  WebVision Database--Visual Learning and Unders...  08.02862v1  [cs.CV]  9 Aug 2017WebVision Datab...       \n",
       "46  FedVision--An Online Visual Object Detection P...  n: An Online Visual Object Detection Platform ...       \n",
       "47  Towards Non-I.I.D. and Invisible Data with Fed...  Non-I.I.D. and Invisible Data with FedNAS:Fede...       \n",
       "48  Ensemble Distillation for Robust Model Fusion ...   Distillation for Robust Model Fusion inFedera...       \n",
       "49  Unsupervised Visual Representation Learning by...  ised Visual Representation Learning by Context...       \n",
       "50  Distilling a Neural Network Into a Soft Decisi...  ng a Neural Network Into a Soft DecisionTreeNi...       \n",
       "51  Understanding Self-Supervised Learning Dynamic...  nding Self-Supervised Learning Dynamics withou...       \n",
       "52  FBNet--Hardware-Aware Efficient ConvNet Design...  ardware-Aware Efﬁcient ConvNet Designvia Diffe...       \n",
       "53  EfficientNet--Rethinking Model Scaling for Con...  Net: Rethinking Model Scaling for Convolutiona...       \n",
       "54  SecureML--A System for Scalable Privacy-Preser...  : A System for Scalable Privacy-PreservingMach...       \n",
       "55  Parametric Instance Classification for Unsuper...  ic Instance Classiﬁcation for UnsupervisedVisu...       \n",
       "56  An Empirical Investigation of Catastrophic For...  ical Investigation of Catastrophic Forgetting ...       \n",
       "57  Communication-Efficient Federated Distillation...  ication-Efﬁcient Federated DistillationFelix S...       \n",
       "58                Federated learning with non-iid.pdf  d Learning with Non-IID DataYue Zhao*yzhao727@...       \n",
       "59  Cronus: Robust and Heterogeneous Collaborative...  12.11279v1  [stat.ML]  24 Dec 2019Cronus: Robu...       \n",
       "60  Federated Learning for Personalized Humor Reco...  d Learning for Personalized Humor RecognitionX...       \n",
       "61  Group knowledge transfer: Federated learning o...  owledge Transfer:Federated Learning of Large C...       \n",
       "62  Unsupervised Embedding Learning via Invariant ...  ised Embedding Learning via Invariant and Spre...       \n",
       "63  An Empirical Study of Training Self-Supervised...  ical Study of Training Self-Supervised Vision ...       \n",
       "64                      Federated Mutual Learning.pdf  d Mutual LearningTao ShenDepartment of Compute...       \n",
       "65                  A Survey on Transfer Learning.pdf   on Transfer LearningSinno Jialin Pan and Qian...       \n",
       "66  Specialized federated learning using a mixture...  zed federated learning using a mixture of expe...       \n",
       "67  Federated Optmization--Distributed Machine Lea...  d Optimization:Distributed Machine Learning fo...       \n",
       "68                    A continual learning survey.pdf  09.08383v3  [cs.CV]  16 Apr 2021Copyright (c) ...       \n",
       "69  NETTAILOR--Tuning the architecture, not just t...  R : Tuning the architecture, not just the weig...       \n",
       "70  Rainbow Memory: Continual Learning with a Memo...  Memory: Continual Learning with a Memory of Di...       \n",
       "71  Toward Understanding Catastrophic Forgetting i...  nderstanding Catastrophic Forgetting in Contin...       \n",
       "72  Federated semi-supervised learning with inter-...  d as a conference paper at ICLR 2021FEDERATED ...       \n",
       "73   Deep Residual Learning for Image Recognition.pdf  idual Learning for Image RecognitionKaiming He...       \n",
       "74                    Progressive Neural Networks.pdf  ive Neural NetworksAndrei A. Rusu*, Neil C. Ra...       \n",
       "75  Think Locally, Act Globally-- Fderated Learnin...  cally, Act Globally:Federated Learning with Lo...       \n",
       "76  Robust Federated Learning with Noisy and Heter...  ederated Learning with Noisy and Heterogeneous...       \n",
       "77  iCaRL--Incremental Classifier and Representati...  ncremental Classiﬁer and Representation Learni...       \n",
       "78  Resource-Constrained Federated Learning with H...  -Constrained Federated Learning with Heterogen...       \n",
       "79  Federated Model Distillation with Noise-Free D...  d Model Distillation with Noise-Free Different...       \n",
       "80  PackNet--Adding Multiple Tasks to a Single Net...   Adding Multiple Tasks to a Single Network by ...       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pdfI in range(len(dirs)): \n",
    "    reader = PdfReader(\"../Articles/Refrences 1/{pdfName}\".format(pdfName=dirs[pdfI]))\n",
    "    page = reader.pages[0]\n",
    "    if \"abstract\" in page.extract_text().lower():\n",
    "            #print( dirs[pdfI] , \"has Abstract\")\n",
    "            text=page.extract_text()\n",
    "            df.loc[len(df.index)] = [dirs[pdfI],text[text.find(\"abstract\")+9:text.find(\"introduction\")].replace(\"\\n\",''),'']\n",
    "            continue\n",
    "    if \"abstract\" not in page.extract_text().lower():\n",
    "        dfNA.loc[len(df.index)]=[dirs[pdfI] ,\"\"]\n",
    "print(\"#Abstract article duplicates :\" , df.Article.duplicated().sum())\n",
    "print(\"#Abstractless article duplicates :\" ,dfNA.Article.duplicated().sum())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214e537a-20d1-4234-99cb-4a38768b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Refrences 1.csv\",index=False)\n",
    "dfNA.to_csv(\"../Refrences 1Abstractless.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0a9e0a-5ac9-457f-acab-50ba347e3cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a2bd76-4271-4a20-bc5a-739ba9324e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc5c573-ee5e-46ea-bc17-27996908c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d as a conference paper at ICLR 2015ADAM : A M ETHOD FOR STOCHASTIC OPTIMIZATIONDiederik P. Kingma*University of Amsterdam, OpenAIdpkingma@openai.comJimmy Lei Ba∗University of Torontojimmy@psi.utoronto.caABSTRACTWe introduce Adam , an algorithm for ﬁrst-order gradient-based optimization ofstochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efﬁcient,has little memory requirements, is invariant to diagonal rescaling of the gradients,and is well suited for problems that are large in terms of data and/or parameters.The method is also appropriate for non-stationary objectives and problems withvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms,on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convexoptimization framework. Empirical results demonstrate that Adam works well inpractice and compares favorably to other stochastic optimization methods. Finally,we discuss AdaMax , a variant of Adam based on the inﬁnity norm.1 I NTRODUCTIONStochastic gradient-based optimization is of core practical importance in many ﬁelds of science andengineering. Many problems in these ﬁelds can be cast as the optimization of some scalar parameter-ized objective function requiring maximization or minimization with respect to its parameters. If thefunction is differentiable w.r.t. its parameters, gradient descent is a relatively efﬁcient optimizationmethod, since the computation of ﬁrst-order partial derivatives w.r.t. all the parameters is of the samecomputational complexity as just evaluating the function. Often, objective functions are stochastic.For example, many objective functions are composed of a sum of subfunctions evaluated at differentsubsamples of data; in this case optimization can be made more efﬁcient by taking gradient stepsw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itselfas an efﬁcient and effective optimization method that was central in many machine learning successstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have othersources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. Forall such noisy objectives, efﬁcient stochastic optimization techniques are required. The focus of thispaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. Inthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will berestricted to ﬁrst-order methods.We propose Adam , a method for efﬁcient stochastic optimization that only requires ﬁrst-order gra-dients with little memory requirement. The method computes individual adaptive learning rates fordifferent parameters from estimates of ﬁrst and second moments of the gradients; the name Adamis derived from adaptive moment estimation. Our method is designed to combine the advantagesof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-dients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationarysettings; important connections to these and other stochastic optimization methods are clariﬁed insection 5. Some of Adam’s advantages are that the magnitudes of parameter updates are invariant torescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,it does not require a stationary objective, it works with sparse gradients, and it naturally performs aform of step size annealing.∗Equal contribution. Author ordering determined by coin ﬂip over a Google Hangout.1arXiv:1412.6980v9  [cs.LG]  30 Jan 201'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.at[38,\"Abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b25299e-925f-4ace-8fb5-5fc83aa02933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and extracting TF-IDF features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m38\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_features\u001b[38;5;241m=\u001b[39mn_features,\n\u001b[1;32m     23\u001b[0m                              stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m38\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAbstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone in \u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time() \u001b[38;5;241m-\u001b[39m t0))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Fit the NMF model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLops/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2130\u001b[0m )\n\u001b[0;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLops/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1364\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;66;03m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer.\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 4\n",
    "n_top_words = 7\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "t0 = time()\n",
    "print(\"Loading dataset and extracting TF-IDF features...\")\n",
    "dataset = df.at[38,\"Abstract\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(df.at[38,\"Abstract\"])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e935af0-9142-4017-86a0-cdff85008208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeed4d40-d63a-4417-a32f-ce427434ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set (stopwords.words(\"english\"))\n",
    "words = word_tokenize(df.at[38,\"Abstract\"])\n",
    "freqTable = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aadb6aa0-d45c-4bf3-b6d8-baf6bb013540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(df.at[38,\"Abstract\"])\n",
    "import string\n",
    "punctuation = string.punctuation + \"\\n\"\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3373e64a-b66e-4a10-9a63-820ed57c06e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Empirical results demonstrate that Adam works well inpractice and compares favorably to other stochastic optimization methods.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a18fe790-49bb-46fe-993b-ebace4ee5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies ={}\n",
    "for word in words:\n",
    "    if word.lower() not in stopwords:\n",
    "        if word.lower() not in punctuation :\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word]=1\n",
    "            else:\n",
    "                word_frequencies[word]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f14813-e1fe-4578-9897-0b3fe097d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7912d7f6-0202-4ef4-bd7d-bd4ce3f9bed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'The method is straightforward to implement, is computationally efﬁcient,has little memory requirements, is invariant to diagonal rescaling of the gradients,and is well suited for problems that are large in terms of data and/or parameters.The method is also appropriate for non-stationary objectives and problems withvery noisy and/or sparse gradients.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         sentence_weight[sentence] \u001b[38;5;241m=\u001b[39m word_frequencies[word_weight]   \n\u001b[0;32m---> 12\u001b[0m sentence_weight[sentence] \u001b[38;5;241m=\u001b[39m \u001b[43msentence_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'The method is straightforward to implement, is computationally efﬁcient,has little memory requirements, is invariant to diagonal rescaling of the gradients,and is well suited for problems that are large in terms of data and/or parameters.The method is also appropriate for non-stationary objectives and problems withvery noisy and/or sparse gradients.'"
     ]
    }
   ],
   "source": [
    "sentence_weight =dict()\n",
    "for sentence in sentences:\n",
    "    sentence_wordcount =len(word_tokenize(sentence))\n",
    "    sentence_wordcount_without_stop_words=0\n",
    "    for word_weight in word_frequencies:\n",
    "        if word_weight in sentence.lower():\n",
    "            sentence_wordcount_without_stop_words +=1\n",
    "            if sentence in sentence_weight:\n",
    "                sentence_weight[sentence]+= word_frequencies[word_weight]\n",
    "            else:\n",
    "                sentence_weight[sentence] = word_frequencies[word_weight]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67d16979-8d33-4159-942c-a4950f60e122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'d as a conference paper at ICLR 2015ADAM : A M ETHOD FOR STOCHASTIC OPTIMIZATIONDiederik P. Kingma*University of Amsterdam, OpenAIdpkingma@openai.comJimmy Lei Ba∗University of Torontojimmy@psi.utoronto.caABSTRACTWe introduce Adam , an algorithm for ﬁrst-order gradient-based optimization ofstochastic objective functions, based on adaptive estimates of lower-order mo-ments.': 62},\n",
       " {'conference': 1,\n",
       "  'paper': 2,\n",
       "  'ICLR': 1,\n",
       "  '2015ADAM': 1,\n",
       "  'ETHOD': 1,\n",
       "  'STOCHASTIC': 1,\n",
       "  'OPTIMIZATIONDiederik': 1,\n",
       "  'P.': 1,\n",
       "  'Kingma': 1,\n",
       "  'University': 1,\n",
       "  'Amsterdam': 1,\n",
       "  'OpenAIdpkingma': 1,\n",
       "  'openai.comJimmy': 1,\n",
       "  'Lei': 1,\n",
       "  'Ba∗University': 1,\n",
       "  'Torontojimmy': 1,\n",
       "  'psi.utoronto.caABSTRACTWe': 1,\n",
       "  'introduce': 1,\n",
       "  'Adam': 6,\n",
       "  'algorithm': 2,\n",
       "  'ﬁrst-order': 4,\n",
       "  'gradient-based': 2,\n",
       "  'optimization': 11,\n",
       "  'ofstochastic': 1,\n",
       "  'objective': 5,\n",
       "  'functions': 3,\n",
       "  'based': 2,\n",
       "  'adaptive': 3,\n",
       "  'estimates': 2,\n",
       "  'lower-order': 1,\n",
       "  'mo-ments': 1,\n",
       "  'method': 6,\n",
       "  'straightforward': 1,\n",
       "  'implement': 1,\n",
       "  'computationally': 1,\n",
       "  'efﬁcient': 6,\n",
       "  'little': 3,\n",
       "  'memory': 2,\n",
       "  'requirements': 1,\n",
       "  'invariant': 2,\n",
       "  'diagonal': 1,\n",
       "  'rescaling': 1,\n",
       "  'gradients': 4,\n",
       "  'well': 4,\n",
       "  'suited': 1,\n",
       "  'problems': 3,\n",
       "  'large': 1,\n",
       "  'terms': 1,\n",
       "  'data': 3,\n",
       "  'and/or': 2,\n",
       "  'parameters.The': 1,\n",
       "  'also': 3,\n",
       "  'appropriate': 1,\n",
       "  'non-stationary': 1,\n",
       "  'objectives': 3,\n",
       "  'withvery': 1,\n",
       "  'noisy': 2,\n",
       "  'sparse': 3,\n",
       "  'hyper-parameters': 1,\n",
       "  'intuitive': 1,\n",
       "  'interpre-tations': 1,\n",
       "  'typically': 1,\n",
       "  'require': 2,\n",
       "  'tuning': 1,\n",
       "  'connections': 2,\n",
       "  'related': 1,\n",
       "  'algorithms': 1,\n",
       "  'inspired': 1,\n",
       "  'discussed': 1,\n",
       "  'analyze': 1,\n",
       "  'theoretical': 1,\n",
       "  'con-vergence': 1,\n",
       "  'properties': 1,\n",
       "  'provide': 1,\n",
       "  'regret': 1,\n",
       "  'bound': 1,\n",
       "  'conver-gence': 1,\n",
       "  'rate': 1,\n",
       "  'comparable': 1,\n",
       "  'best': 1,\n",
       "  'known': 1,\n",
       "  'results': 2,\n",
       "  'online': 1,\n",
       "  'convexoptimization': 1,\n",
       "  'framework': 1,\n",
       "  'Empirical': 1,\n",
       "  'demonstrate': 1,\n",
       "  'works': 4,\n",
       "  'inpractice': 1,\n",
       "  'compares': 1,\n",
       "  'favorably': 1,\n",
       "  'stochastic': 6,\n",
       "  'methods': 4,\n",
       "  'Finally': 1,\n",
       "  'discuss': 1,\n",
       "  'AdaMax': 1,\n",
       "  'variant': 1,\n",
       "  'inﬁnity': 1,\n",
       "  'norm.1': 1,\n",
       "  'NTRODUCTIONStochastic': 1,\n",
       "  'core': 1,\n",
       "  'practical': 1,\n",
       "  'importance': 1,\n",
       "  'many': 3,\n",
       "  'ﬁelds': 2,\n",
       "  'science': 1,\n",
       "  'andengineering': 1,\n",
       "  'Many': 1,\n",
       "  'cast': 1,\n",
       "  'scalar': 1,\n",
       "  'parameter-ized': 1,\n",
       "  'function': 2,\n",
       "  'requiring': 1,\n",
       "  'maximization': 1,\n",
       "  'minimization': 1,\n",
       "  'respect': 1,\n",
       "  'parameters': 5,\n",
       "  'thefunction': 1,\n",
       "  'differentiable': 1,\n",
       "  'w.r.t': 2,\n",
       "  'gradient': 4,\n",
       "  'descent': 2,\n",
       "  'relatively': 1,\n",
       "  'optimizationmethod': 1,\n",
       "  'since': 1,\n",
       "  'computation': 1,\n",
       "  'partial': 1,\n",
       "  'derivatives': 1,\n",
       "  'samecomputational': 1,\n",
       "  'complexity': 1,\n",
       "  'evaluating': 1,\n",
       "  'Often': 1,\n",
       "  'stochastic.For': 1,\n",
       "  'example': 1,\n",
       "  'composed': 1,\n",
       "  'sum': 1,\n",
       "  'subfunctions': 2,\n",
       "  'evaluated': 1,\n",
       "  'differentsubsamples': 1,\n",
       "  'case': 1,\n",
       "  'made': 1,\n",
       "  'taking': 1,\n",
       "  'stepsw.r.t': 1,\n",
       "  'individual': 2,\n",
       "  'i.e': 1,\n",
       "  'SGD': 2,\n",
       "  'ascent': 1,\n",
       "  'proved': 1,\n",
       "  'itselfas': 1,\n",
       "  'effective': 1,\n",
       "  'central': 1,\n",
       "  'machine': 1,\n",
       "  'learning': 3,\n",
       "  'successstories': 1,\n",
       "  'recent': 1,\n",
       "  'advances': 1,\n",
       "  'deep': 1,\n",
       "  'Deng': 1,\n",
       "  'et': 6,\n",
       "  'al.': 6,\n",
       "  '2013': 2,\n",
       "  'Krizhevsky': 1,\n",
       "  '2012': 2,\n",
       "  'Hinton': 4,\n",
       "  'Salakhutdinov': 1,\n",
       "  '2006': 1,\n",
       "  '2012a': 1,\n",
       "  'Graves': 1,\n",
       "  'Objectives': 1,\n",
       "  'may': 1,\n",
       "  'othersources': 1,\n",
       "  'noise': 1,\n",
       "  'subsampling': 1,\n",
       "  'dropout': 1,\n",
       "  '2012b': 1,\n",
       "  'regularization': 1,\n",
       "  'Forall': 1,\n",
       "  'techniques': 1,\n",
       "  'required': 1,\n",
       "  'focus': 1,\n",
       "  'thispaper': 1,\n",
       "  'high-dimensional': 1,\n",
       "  'spaces': 1,\n",
       "  'Inthese': 1,\n",
       "  'cases': 1,\n",
       "  'higher-order': 1,\n",
       "  'ill-suited': 1,\n",
       "  'discussion': 1,\n",
       "  'berestricted': 1,\n",
       "  'methods.We': 1,\n",
       "  'propose': 1,\n",
       "  'requires': 1,\n",
       "  'gra-dients': 2,\n",
       "  'requirement': 1,\n",
       "  'computes': 1,\n",
       "  'rates': 1,\n",
       "  'fordifferent': 1,\n",
       "  'ﬁrst': 1,\n",
       "  'second': 1,\n",
       "  'moments': 1,\n",
       "  'name': 1,\n",
       "  'Adamis': 1,\n",
       "  'derived': 1,\n",
       "  'moment': 1,\n",
       "  'estimation': 1,\n",
       "  'designed': 1,\n",
       "  'combine': 1,\n",
       "  'advantagesof': 1,\n",
       "  'two': 1,\n",
       "  'recently': 1,\n",
       "  'popular': 1,\n",
       "  'AdaGrad': 1,\n",
       "  'Duchi': 1,\n",
       "  '2011': 1,\n",
       "  'RMSProp': 1,\n",
       "  'Tieleman': 1,\n",
       "  'on-line': 1,\n",
       "  'non-stationarysettings': 1,\n",
       "  'important': 1,\n",
       "  'clariﬁed': 1,\n",
       "  'insection': 1,\n",
       "  '5': 1,\n",
       "  '’': 1,\n",
       "  'advantages': 1,\n",
       "  'magnitudes': 1,\n",
       "  'parameter': 1,\n",
       "  'updates': 1,\n",
       "  'torescaling': 1,\n",
       "  'stepsizes': 1,\n",
       "  'approximately': 1,\n",
       "  'bounded': 1,\n",
       "  'stepsize': 1,\n",
       "  'hyperparameter': 1,\n",
       "  'stationary': 1,\n",
       "  'naturally': 1,\n",
       "  'performs': 1,\n",
       "  'aform': 1,\n",
       "  'step': 1,\n",
       "  'size': 1,\n",
       "  'annealing.∗Equal': 1,\n",
       "  'contribution': 1,\n",
       "  'Author': 1,\n",
       "  'ordering': 1,\n",
       "  'determined': 1,\n",
       "  'coin': 1,\n",
       "  'ﬂip': 1,\n",
       "  'Google': 1,\n",
       "  'Hangout.1arXiv:1412.6980v9': 1,\n",
       "  'cs.LG': 1,\n",
       "  '30': 1,\n",
       "  'Jan': 1,\n",
       "  '201': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sentence_weight , word_frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932f80a5-2760-4a24-9c15-ca8c9aa9c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\\nArun Mallya and Svetlana Lazebnik\\nUniversity of Illinois at Urbana-Champaign\\n{amallya2,slazebni }@illinois.edu\\nAbstract\\nThis paper presents a method for adding multiple tasks to\\na single deep neural network while avoiding catastrophic for-\\ngetting.',\n",
       " 'Inspired by network pruning techniques, we exploit\\nredundancies in large deep networks to free up parameters\\nthat can then be employed to learn new tasks.',\n",
       " 'By perform-\\ning iterative pruning and network re-training, we are able\\nto sequentially “pack” multiple tasks into a single network\\nwhile ensuring minimal drop in performance and minimal\\nstorage overhead.',\n",
       " 'Unlike prior work that uses proxy losses\\nto maintain accuracy on older tasks, we always optimize\\nfor the task at hand.',\n",
       " 'We perform extensive experiments on\\na variety of network architectures and large-scale datasets,\\nand observe much better robustness against catastrophic\\nforgetting than prior work.',\n",
       " 'In particular, we are able to add\\nthree ﬁne-grained classiﬁcation tasks to a single ImageNet-\\ntrained VGG-16 network and achieve accuracies close to\\nthose of separately trained networks for each task.',\n",
       " '1.',\n",
       " 'Introduction\\nLifelong or continual learning [ 1,14,22] is a key re-\\nquirement for general artiﬁcially intelligent agents.',\n",
       " 'Under\\nthis setting, the agent is required to acquire expertise on\\nnew tasks while maintaining its performance on previously\\nlearned tasks, ideally without the need to store large special-\\nized models for each individual task.',\n",
       " 'In the case of deep\\nneural networks, the most common way of learning a new\\ntask is to ﬁne-tune the network.',\n",
       " 'However, as features rele-\\nvant to the new task are learned through modiﬁcation of the\\nnetwork weights, weights important for prior tasks might\\nbe altered, leading to deterioration in performance referred\\nto as “ catastrophic forgetting ” [4].',\n",
       " 'Without access to older\\ntraining data due to the lack of storage space, data rights, or\\ndeployed nature of the agent, which are all very realistic con-\\nstraints, na ¨ıve ﬁne-tuning is not a viable option for continual\\nlearning.',\n",
       " 'Current approaches to overcoming catastrophic forget-\\nting, such as Learning without Forgetting (LwF) [ 18] and\\nElastic Weight Consolidation (EWC) [ 14], try to preserveknowledge important to prior tasks through the use of proxy\\nlosses.',\n",
       " 'The former tries to preserve activations of the ini-\\ntial network while training on new data, while the latter\\npenalizes the modiﬁcation of parameters deemed to be im-\\nportant to prior tasks.',\n",
       " 'Distinct from such prior work, we\\ndraw inspiration from approaches in network compression\\nthat have shown impressive results for reducing network size\\nand computational footprint by eliminating redundant pa-\\nrameters [ 8,17,19,20].',\n",
       " 'We propose an approach that uses\\nweight-based pruning techniques [ 7,8] to free up redundant\\nparameters across all layers of a deep network after it has\\nbeen trained for a task, with minimal loss in accuracy.',\n",
       " 'Keep-\\ning the surviving parameters ﬁxed, the freed up parameters\\nare modiﬁed for learning a new task.',\n",
       " 'This process is per-\\nformed repeatedly for adding multiple tasks, as illustrated\\nin Figure 1.',\n",
       " 'By using the task-speciﬁc parameter masks\\ngenerated by pruning, our models are able to maintain the\\nsame level of accuracy even after the addition of multiple\\ntasks, and incur a very low storage overhead per each new\\ntask.',\n",
       " 'Our experiments demonstrate the efﬁcacy of our method\\non several tasks for which high-level feature transfer does not\\nperform very well, indicating the need to modify parameters\\nof the network at all layers.',\n",
       " 'In particular, we take a single\\nImageNet-trained VGG-16 network [ 28] and add to it three\\nﬁne-grained classiﬁcation tasks – CUBS birds [ 29], Stanford\\nCars [ 15], and Oxford Flowers [ 21] – while achieving ac-\\ncuracies very close to those of separately trained networks\\nfor each individual task.',\n",
       " 'This signiﬁcantly outperforms prior\\nwork in terms of robustness to catastrophic forgetting, as well\\nas the number and complexity of added tasks.',\n",
       " 'We also show\\nthat our method is superior to joint training when adding the\\nlarge-scale Places365 [ 30] dataset to an ImageNet-trained\\nnetwork, and obtain competitive performance on a broad\\nrange of architectures, including VGG-16 with batch nor-\\nmalization [ 13], ResNets [ 9], and DenseNets [ 11].',\n",
       " '2.',\n",
       " 'Related Work\\nA few prior works and their variants, such as Learning\\nwithout Forgetting (LwF) [ 18,22,27] and Elastic Weight\\nConsolidation (EWC) [ 14,16], are aimed at training a net-\\n1\\n7765']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502790e-bfdd-405d-948f-fed64213a3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
