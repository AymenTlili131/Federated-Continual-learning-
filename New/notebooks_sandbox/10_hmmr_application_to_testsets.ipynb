{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMMR_r Application to Test Sets - Critical Analysis\n",
    "\n",
    "## PhD Supervisor Methodological Concerns\n",
    "\n",
    "**1. Theoretical Mismatch Between HMMR and CNN Classification**\n",
    "You're proposing to apply Hidden Markov Model Regression (HMMR_r) - designed for temporal time series segmentation - to static MNIST classification. This raises fundamental questions:\n",
    "\n",
    "- **Temporal Assumption Violation**: HMMR assumes sequential dependencies and hidden states evolving over time. MNIST digit classification has no inherent temporal structure. What temporal dynamics are you modeling?\n",
    "\n",
    "- **State Interpretation**: What do the hidden states represent in the context of static image classification? Digit classes? Feature extraction stages? Something else entirely?\n",
    "\n",
    "- **Regression vs Classification**: HMMR_r is fundamentally a regression method for time series. How do you map this to the discrete classification task of digit recognition?\n",
    "\n",
    "**2. Federated Learning Context Issues**\n",
    "- **Cross-Scenario Application**: You mention applying this to \"individual or batches of the testset of each scenario\" without clarifying how HMMR_r accounts for federated learning heterogeneity.\n",
    "\n",
    "- **No Overlap Constraint**: The \"0 overlap\" constraint needs precise definition. Are you ensuring no data leakage between federated clients? How does HMMR_r handle this?\n",
    "\n",
    "**3. Evaluation Protocol Gaps**\n",
    "- **Baseline Comparison**: What are you comparing HMMR_r against? Standard CNN? Transformer? Why is HMMR_r expected to improve performance?\n",
    "\n",
    "- **Success Metrics**: How will you determine if HMMR_r application is successful? Accuracy improvement? Better uncertainty quantification? Computational efficiency?\n",
    "\n",
    "### Critical Questions Before Implementation:\n",
    "\n",
    "1. **Temporal Construction**: How do you create meaningful temporal sequences from static MNIST images for HMMR_r?\n",
    "\n",
    "2. **State Definition**: What is your hypothesis about the hidden states in the context of digit classification?\n",
    "\n",
    "3. **Federated Integration**: How does HMMR_r specifically address challenges of federated learning (non-iid data, communication constraints)?\n",
    "\n",
    "4. **Computational Trade-off**: What's the computational overhead of HMMR_r compared to existing methods?\n",
    "\n",
    "### Proposed Approach (Addressing Concerns):\n",
    "\n",
    "This notebook will:\n",
    "1. **Explicitly construct temporal sequences** from test set data with theoretical justification\n",
    "2. **Define clear state semantics** for HMMR_r in classification context\n",
    "3. **Implement proper federated evaluation** with no data leakage\n",
    "4. **Compare against strong baselines** with statistical validation\n",
    "\n",
    "### Research Hypothesis:\n",
    "\n",
    "*If we construct meaningful temporal sequences from MNIST test sets that capture feature extraction hierarchies, then HMMR_r can model the evolution of classification confidence across synthetic time steps, providing better uncertainty quantification and robustness in federated scenarios.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single Initialization Data Loading for HMMR Analysis ===\n",
      "Project root: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New\n",
      "Data directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/data\n",
      "Experiments directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/Experiments\n",
      "Results directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results\n",
      "PyTorch version: 2.7.1+cu128\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.3\n",
      "Device: cuda\n",
      "üîÑ Loading data for HMMR analysis...\n",
      "üîÑ Loading HMMR data from: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/data/Merged zoo.csv\n",
      "‚úÖ Loaded 36468 rows\n",
      "üìä Limited to 800 samples for HMMR efficiency\n",
      "üî¢ Found 2464 weight columns\n",
      "üìã Found 19 metadata columns\n",
      "üéØ Single initialization mode: gelu\n",
      "   Samples with gelu: 800.0\n",
      "üìä Filtered to 800 samples\n",
      "\n",
      "üîß HMMR Data Preparation:\n",
      "   Weight matrix shape: (800, 2464)\n",
      "   Labels shape: (800,)\n",
      "   Unique labels: ['[0, 1, 2, 3, 4, 5]', '[0, 1, 2, 3, 4, 6]', '[0, 1, 2, 3, 4, 7]', '[0, 1, 2, 3, 4, 8]', '[0, 1, 2, 3, 4, 9]', '[0, 1, 2, 3, 4]', '[0, 1, 2, 3, 5, 6]', '[0, 1, 2, 3, 5, 7]', '[0, 1, 2, 3, 5, 8]', '[0, 1, 2, 3, 5, 9]', '[0, 1, 2, 3, 5]', '[0, 1, 2, 3, 6, 7]', '[0, 1, 2, 3, 6, 8]', '[0, 1, 2, 3, 6, 9]', '[0, 1, 2, 3, 6]', '[0, 1, 2, 3, 7, 8]', '[0, 1, 2, 3, 7, 9]', '[0, 1, 2, 3, 7]', '[0, 1, 2, 3, 8, 9]', '[0, 1, 2, 3, 8]', '[0, 1, 2, 3, 9]', '[0, 1, 2, 3]', '[0, 1, 2, 4, 5, 6]', '[0, 1, 2, 4, 5, 7]', '[0, 1, 2, 4, 5, 8]', '[0, 1, 2, 4, 5, 9]', '[0, 1, 2, 4, 5]', '[0, 1, 2, 4, 6, 7]', '[0, 1, 2, 4, 6, 8]', '[0, 1, 2, 4, 6, 9]', '[0, 1, 2, 4, 6]', '[0, 1, 2, 4, 7, 8]', '[0, 1, 2, 4, 7, 9]', '[0, 1, 2, 4, 7]', '[0, 1, 2, 4, 8, 9]', '[0, 1, 2, 4, 8]', '[0, 1, 2, 4, 9]', '[0, 1, 2, 4]', '[0, 1, 2, 5, 6, 7]', '[0, 1, 2, 5, 6, 8]', '[0, 1, 2, 5, 6, 9]', '[0, 1, 2, 5, 6]', '[0, 1, 2, 5, 7, 8]', '[0, 1, 2, 5, 7, 9]', '[0, 1, 2, 5, 7]', '[0, 1, 2, 5, 8, 9]', '[0, 1, 2, 5, 8]', '[0, 1, 2, 5, 9]', '[0, 1, 2, 5]', '[0, 1, 2, 6, 7, 8]', '[0, 1, 2, 6, 7, 9]', '[0, 1, 2, 6, 7]', '[0, 1, 2, 6, 8, 9]', '[0, 1, 2, 6, 8]', '[0, 1, 2, 6, 9]', '[0, 1, 2, 6]', '[0, 1, 2, 7, 8, 9]', '[0, 1, 2, 7, 8]', '[0, 1, 2, 7, 9]', '[0, 1, 2, 7]', '[0, 1, 2, 8, 9]', '[0, 1, 2, 8]', '[0, 1, 2, 9]', '[0, 1, 2]', '[0, 1, 3, 4, 5, 6]', '[0, 1, 3, 4, 5, 7]', '[0, 1, 3, 4, 5, 8]', '[0, 1, 3, 4, 5, 9]', '[0, 1, 3, 4, 5]', '[0, 1, 3, 4, 6, 7]', '[0, 1, 3, 4, 6, 8]', '[0, 1, 3, 4, 6, 9]', '[0, 1, 3, 4, 6]', '[0, 1, 3, 4, 7, 8]', '[0, 1, 3, 4, 7, 9]', '[0, 1, 3, 4, 7]', '[0, 1, 3, 4, 8, 9]', '[0, 1, 3, 4, 8]', '[0, 1, 3, 4, 9]', '[0, 1, 3, 4]', '[0, 1, 3, 5, 6, 7]', '[0, 1, 3, 5, 6, 8]', '[0, 1, 3, 5, 6, 9]', '[0, 1, 3, 5, 6]', '[0, 1, 3, 5, 7, 8]', '[0, 1, 3, 5, 7, 9]', '[0, 1, 3, 5, 7]', '[0, 1, 3, 5, 8, 9]', '[0, 1, 3, 5, 8]', '[0, 1, 3, 5, 9]', '[0, 1, 3, 5]', '[0, 1, 3, 6, 7, 8]', '[0, 1, 3, 6, 7, 9]', '[0, 1, 3, 6, 7]', '[0, 1, 3, 6, 8, 9]', '[0, 1, 3, 6, 8]', '[0, 1, 3, 6, 9]', '[0, 1, 3, 6]', '[0, 1, 3, 7, 8, 9]', '[0, 1, 3, 7, 8]', '[0, 1, 3, 7, 9]', '[0, 1, 3, 7]', '[0, 1, 3, 8, 9]', '[0, 1, 3, 8]', '[0, 1, 3, 9]', '[0, 1, 3]', '[0, 1, 4, 5, 6, 7]', '[0, 1, 4, 5, 6, 8]', '[0, 1, 4, 5, 6, 9]', '[0, 1, 4, 5, 6]', '[0, 1, 4, 5, 7, 8]', '[0, 1, 4, 5, 7, 9]', '[0, 1, 4, 5, 7]', '[0, 1, 4, 5, 8, 9]', '[0, 1, 4, 5, 8]', '[0, 1, 4, 5, 9]', '[0, 1, 4, 5]', '[0, 1, 4, 6, 7, 8]', '[0, 1, 4, 6, 7, 9]', '[0, 1, 4, 6, 7]', '[0, 1, 4, 6, 8, 9]', '[0, 1, 4, 6, 8]', '[0, 1, 4, 6, 9]', '[0, 1, 4, 6]', '[0, 1, 4, 7, 8, 9]', '[0, 1, 4, 7, 8]', '[0, 1, 4, 7, 9]', '[0, 1, 4, 7]', '[0, 1, 4, 8, 9]', '[0, 1, 4, 8]', '[0, 1, 4, 9]', '[0, 1, 4]', '[0, 1, 5, 6, 7, 8]', '[0, 1, 5, 6, 7, 9]', '[0, 1, 5, 6, 7]', '[0, 1, 5, 6, 8, 9]', '[0, 1, 5, 6, 8]', '[0, 1, 5, 6, 9]', '[0, 1, 5, 6]', '[0, 1, 5, 7, 8, 9]', '[0, 1, 5, 7, 8]', '[0, 1, 5, 7, 9]', '[0, 1, 5, 7]', '[0, 1, 5, 8, 9]', '[0, 1, 5, 8]', '[0, 1, 5, 9]', '[0, 1, 5]', '[0, 1, 6, 7, 8, 9]', '[0, 1, 6, 7, 8]', '[0, 1, 6, 7, 9]', '[0, 1, 6, 7]', '[0, 1, 6, 8, 9]', '[0, 1, 6, 8]', '[0, 1, 6, 9]', '[0, 1, 6]', '[0, 1, 7, 8, 9]', '[0, 1, 7, 8]', '[0, 1, 7, 9]', '[0, 1, 7]', '[0, 1, 8, 9]', '[0, 1, 8]', '[0, 1, 9]', '[0, 1]', '[0, 2, 3, 4, 5, 6]', '[0, 2, 3, 4, 5, 7]', '[0, 2, 3, 4, 5, 8]', '[0, 2, 3, 4, 5, 9]', '[0, 2, 3, 4, 5]', '[0, 2, 3, 4, 6, 7]', '[0, 2, 3, 4, 6, 8]', '[0, 2, 3, 4, 6, 9]', '[0, 2, 3, 4, 6]', '[0, 2, 3, 4, 7, 8]', '[0, 2, 3, 4, 7, 9]', '[0, 2, 3, 4, 7]', '[0, 2, 3, 4, 8, 9]', '[0, 2, 3, 4, 8]', '[0, 2, 3, 4, 9]', '[0, 2, 3, 4]', '[0, 2, 3, 5, 6, 7]', '[0, 2, 3, 5, 6, 8]', '[0, 2, 3, 5, 6, 9]', '[0, 2, 3, 5, 6]', '[0, 2, 3, 5, 7, 8]', '[0, 2, 3, 5, 7, 9]', '[0, 2, 3, 5, 7]', '[0, 2, 3, 5, 8, 9]', '[0, 2, 3, 5, 8]', '[0, 2, 3, 5, 9]', '[0, 2, 3, 5]', '[0, 2, 3, 6, 7, 8]', '[0, 2, 3, 6, 7, 9]', '[0, 2, 3, 6, 7]', '[0, 2, 3, 6, 8, 9]', '[0, 2, 3, 6, 8]', '[0, 2, 3, 6, 9]', '[0, 2, 3, 6]', '[0, 2, 3, 7, 8, 9]', '[0, 2, 3, 7, 8]', '[0, 2, 3, 7, 9]', '[0, 2, 3, 7]', '[0, 2, 3, 8, 9]', '[0, 2, 3, 8]', '[0, 2, 3, 9]', '[0, 2, 3]', '[0, 2, 4, 5, 6, 7]', '[0, 2, 4, 5, 6, 8]', '[0, 2, 4, 5, 6, 9]', '[0, 2, 4, 5, 6]', '[0, 2, 4, 5, 7, 8]', '[0, 2, 4, 5, 7, 9]', '[0, 2, 4, 5, 7]', '[0, 2, 4, 5, 8, 9]', '[0, 2, 4, 5, 8]', '[0, 2, 4, 5, 9]', '[0, 2, 4, 5]', '[0, 2, 4, 6, 7, 8]', '[0, 2, 4, 6, 7, 9]', '[0, 2, 4, 6, 7]', '[0, 2, 4, 6, 8, 9]', '[0, 2, 4, 6, 8]', '[0, 2, 4, 6, 9]', '[0, 2, 4, 6]', '[0, 2, 4, 7, 8, 9]', '[0, 2, 4, 7, 8]', '[0, 2, 4, 7, 9]', '[0, 2, 4, 7]', '[0, 2, 4, 8, 9]', '[0, 2, 4, 8]', '[0, 2, 4, 9]', '[0, 2, 4]', '[0, 2, 5, 6, 7, 8]', '[0, 2, 5, 6, 7, 9]', '[0, 2, 5, 6, 7]', '[0, 2, 5, 6, 8, 9]', '[0, 2, 5, 6, 8]', '[0, 2, 5, 6, 9]', '[0, 2, 5, 6]', '[0, 2, 5, 7, 8, 9]', '[0, 2, 5, 7, 8]', '[0, 2, 5, 7, 9]', '[0, 2, 5, 7]', '[0, 2, 5, 8, 9]', '[0, 2, 5, 8]', '[0, 2, 5, 9]', '[0, 2, 5]', '[0, 2, 6, 7, 8, 9]', '[0, 2, 6, 7, 8]', '[0, 2, 6, 7, 9]', '[0, 2, 6, 7]', '[0, 2, 6, 8, 9]', '[0, 2, 6, 8]', '[0, 2, 6, 9]', '[0, 2, 6]', '[0, 2, 7, 8, 9]', '[0, 2, 7, 8]', '[0, 2, 7, 9]', '[0, 2, 7]', '[0, 2, 8, 9]', '[0, 2, 8]', '[0, 2, 9]', '[0, 2]', '[0, 3, 4, 5, 6, 7]', '[0, 3, 4, 5, 6, 8]', '[0, 3, 4, 5, 6, 9]', '[0, 3, 4, 5, 6]', '[0, 3, 4, 5, 7, 8]', '[0, 3, 4, 5, 7, 9]', '[0, 3, 4, 5, 7]', '[0, 3, 4, 5, 8, 9]', '[0, 3, 4, 5, 8]', '[0, 3, 4, 5, 9]', '[0, 3, 4, 5]', '[0, 3, 4, 6, 7, 8]', '[0, 3, 4, 6, 7, 9]', '[0, 3, 4, 6, 7]', '[0, 3, 4, 6, 8, 9]', '[0, 3, 4, 6, 8]', '[0, 3, 4, 6, 9]', '[0, 3, 4, 6]', '[0, 3, 4, 7, 8, 9]', '[0, 3, 4, 7, 8]', '[0, 3, 4, 7, 9]', '[0, 3, 4, 7]', '[0, 3, 4, 8, 9]', '[0, 3, 4, 8]', '[0, 3, 4, 9]', '[0, 3, 4]', '[0, 3, 5, 6, 7, 8]', '[0, 3, 5, 6, 7, 9]', '[0, 3, 5, 6, 7]', '[0, 3, 5, 6, 8, 9]', '[0, 3, 5, 6, 8]', '[0, 3, 5, 6, 9]', '[0, 3, 5, 6]', '[0, 3, 5, 7, 8, 9]', '[0, 3, 5, 7, 8]', '[0, 3, 5, 7, 9]', '[0, 3, 5, 7]', '[0, 3, 5, 8, 9]', '[0, 3, 5, 8]', '[0, 3, 5, 9]', '[0, 3, 5]', '[0, 3, 6, 7, 8, 9]', '[0, 3, 6, 7, 8]', '[0, 3, 6, 7, 9]', '[0, 3, 6, 7]', '[0, 3, 6, 8, 9]', '[0, 3, 6, 8]', '[0, 3, 6, 9]', '[0, 3, 6]', '[0, 3, 7, 8, 9]', '[0, 3, 7, 8]', '[0, 3, 7, 9]', '[0, 3, 7]', '[0, 3, 8, 9]', '[0, 3, 8]', '[0, 3, 9]', '[0, 3]', '[0, 4, 5, 6, 7, 8]', '[0, 4, 5, 6, 7, 9]', '[0, 4, 5, 6, 7]', '[0, 4, 5, 6, 8, 9]', '[0, 4, 5, 6, 8]', '[0, 4, 5, 6, 9]', '[0, 4, 5, 6]', '[0, 4, 5, 7, 8, 9]', '[0, 4, 5, 7, 8]', '[0, 4, 5, 7, 9]', '[0, 4, 5, 7]', '[0, 4, 5, 8, 9]', '[0, 4, 5, 8]', '[0, 4, 5, 9]', '[0, 4, 5]', '[0, 4, 6, 7, 8, 9]', '[0, 4, 6, 7, 8]', '[0, 4, 6, 7, 9]', '[0, 4, 6, 7]', '[0, 4, 6, 8, 9]', '[0, 4, 6, 8]', '[0, 4, 6, 9]', '[0, 4, 6]', '[0, 4, 7, 8, 9]', '[0, 4, 7, 8]', '[0, 4, 7, 9]', '[0, 4, 7]', '[0, 4, 8, 9]', '[0, 4, 8]', '[0, 4, 9]', '[0, 4]', '[0, 5, 6, 7, 8, 9]', '[0, 5, 6, 7, 8]', '[0, 5, 6, 7, 9]', '[0, 5, 6, 7]', '[0, 5, 6, 8, 9]', '[0, 5, 6, 8]', '[0, 5, 6, 9]', '[0, 5, 6]', '[0, 5, 7, 8, 9]', '[0, 5, 7, 8]', '[0, 5, 7, 9]', '[0, 5, 7]', '[0, 5, 8, 9]', '[0, 5, 8]', '[0, 5, 9]', '[0, 5]', '[0, 6, 7, 8, 9]', '[0, 6, 7, 8]', '[0, 6, 7, 9]', '[0, 6, 7]', '[0, 6, 8, 9]', '[0, 6, 8]', '[0, 6, 9]', '[0, 6]', '[0, 7, 8, 9]', '[0, 7, 8]', '[0, 7, 9]', '[0, 7]', '[0, 8, 9]', '[0, 8]', '[0, 9]', '[1, 2, 3, 4, 5, 6]', '[1, 2, 3, 4, 5, 7]', '[1, 2, 3, 4, 5, 8]', '[1, 2, 3, 4, 5, 9]', '[1, 2, 3, 4, 5]', '[1, 2, 3, 4, 6, 7]', '[1, 2, 3, 4, 6, 8]', '[1, 2, 3, 4, 6, 9]', '[1, 2, 3, 4, 6]', '[1, 2, 3, 4, 7, 8]', '[1, 2, 3, 4, 7, 9]', '[1, 2, 3, 4, 7]', '[1, 2, 3, 4, 8, 9]', '[1, 2, 3, 4, 8]', '[1, 2, 3, 4, 9]', '[1, 2, 3, 4]', '[1, 2, 3, 5, 6, 7]', '[1, 2, 3, 5, 6, 8]', '[1, 2, 3, 5, 6, 9]', '[1, 2, 3, 5, 6]', '[1, 2, 3, 5, 7, 8]', '[1, 2, 3, 5, 7, 9]', '[1, 2, 3, 5, 7]', '[1, 2, 3, 5, 8, 9]', '[1, 2, 3, 5, 8]', '[1, 2, 3, 5, 9]', '[1, 2, 3, 5]', '[1, 2, 3, 6, 7, 8]', '[1, 2, 3, 6, 7, 9]', '[1, 2, 3, 6, 7]', '[1, 2, 3, 6, 8, 9]', '[1, 2, 3, 6, 8]', '[1, 2, 3, 6, 9]', '[1, 2, 3, 6]', '[1, 2, 3, 7, 8, 9]', '[1, 2, 3, 7, 8]', '[1, 2, 3, 7, 9]', '[1, 2, 3, 7]', '[1, 2, 3, 8, 9]', '[1, 2, 3, 8]', '[1, 2, 3, 9]', '[1, 2, 3]', '[1, 2, 4, 5, 6, 7]', '[1, 2, 4, 5, 6, 8]', '[1, 2, 4, 5, 6, 9]', '[1, 2, 4, 5, 6]', '[1, 2, 4, 5, 7, 8]', '[1, 2, 4, 5, 7, 9]', '[1, 2, 4, 5, 7]', '[1, 2, 4, 5, 8, 9]', '[1, 2, 4, 5, 8]', '[1, 2, 4, 5, 9]', '[1, 2, 4, 5]', '[1, 2, 4, 6, 7, 8]', '[1, 2, 4, 6, 7, 9]', '[1, 2, 4, 6, 7]', '[1, 2, 4, 6, 8, 9]', '[1, 2, 4, 6, 8]', '[1, 2, 4, 6, 9]', '[1, 2, 4, 6]', '[1, 2, 4, 7, 8, 9]', '[1, 2, 4, 7, 8]', '[1, 2, 4, 7, 9]', '[1, 2, 4, 7]', '[1, 2, 4, 8, 9]', '[1, 2, 4, 8]', '[1, 2, 4, 9]', '[1, 2, 4]', '[1, 2, 5, 6, 7, 8]', '[1, 2, 5, 6, 7, 9]', '[1, 2, 5, 6, 7]', '[1, 2, 5, 6, 8, 9]', '[1, 2, 5, 6, 8]', '[1, 2, 5, 6, 9]', '[1, 2, 5, 6]', '[1, 2, 5, 7, 8, 9]', '[1, 2, 5, 7, 8]', '[1, 2, 5, 7, 9]', '[1, 2, 5, 7]', '[1, 2, 5, 8, 9]', '[1, 2, 5, 8]', '[1, 2, 5, 9]', '[1, 2, 5]', '[1, 2, 6, 7, 8, 9]', '[1, 2, 6, 7, 8]', '[1, 2, 6, 7, 9]', '[1, 2, 6, 7]', '[1, 2, 6, 8, 9]', '[1, 2, 6, 8]', '[1, 2, 6, 9]', '[1, 2, 6]', '[1, 2, 7, 8, 9]', '[1, 2, 7, 8]', '[1, 2, 7, 9]', '[1, 2, 7]', '[1, 2, 8, 9]', '[1, 2, 8]', '[1, 2, 9]', '[1, 2]', '[1, 3, 4, 5, 6, 7]', '[1, 3, 4, 5, 6, 8]', '[1, 3, 4, 5, 6, 9]', '[1, 3, 4, 5, 6]', '[1, 3, 4, 5, 7, 8]', '[1, 3, 4, 5, 7, 9]', '[1, 3, 4, 5, 7]', '[1, 3, 4, 5, 8, 9]', '[1, 3, 4, 5, 8]', '[1, 3, 4, 5, 9]', '[1, 3, 4, 5]', '[1, 3, 4, 6, 7, 8]', '[1, 3, 4, 6, 7, 9]', '[1, 3, 4, 6, 7]', '[1, 3, 4, 6, 8, 9]', '[1, 3, 4, 6, 8]', '[1, 3, 4, 6, 9]', '[1, 3, 4, 6]', '[1, 3, 4, 7, 8, 9]', '[1, 3, 4, 7, 8]', '[1, 3, 4, 7, 9]', '[1, 3, 4, 7]', '[1, 3, 4, 8, 9]', '[1, 3, 4, 8]', '[1, 3, 4, 9]', '[1, 3, 4]', '[1, 3, 5, 6, 7, 8]', '[1, 3, 5, 6, 7, 9]', '[1, 3, 5, 6, 7]', '[1, 3, 5, 6, 8]', '[1, 3, 5, 6, 9]', '[1, 3, 5, 6]', '[1, 3, 5, 7, 8]', '[1, 3, 5, 7, 9]', '[1, 3, 5, 7]', '[1, 3, 5, 8, 9]', '[1, 3, 5, 8]', '[1, 3, 5, 9]', '[1, 3, 5]', '[1, 3, 6, 7, 8]', '[1, 3, 6, 7, 9]', '[1, 3, 6, 7]', '[1, 3, 6, 8, 9]', '[1, 3, 6, 8]', '[1, 3, 6, 9]', '[1, 3, 6]', '[1, 3, 7, 8, 9]', '[1, 3, 7, 8]', '[1, 3, 7, 9]', '[1, 3, 7]', '[1, 3, 8, 9]', '[1, 3, 8]', '[1, 3, 9]', '[1, 3]', '[1, 4, 5, 6, 7]', '[1, 4, 5, 6, 8]', '[1, 4, 5, 6, 9]', '[1, 4, 5, 6]', '[1, 4, 5, 7, 8]', '[1, 4, 5, 7, 9]', '[1, 4, 5, 7]', '[1, 4, 5, 8, 9]', '[1, 4, 5, 8]', '[1, 4, 5, 9]', '[1, 4, 5]', '[1, 4, 6, 7, 8]', '[1, 4, 6, 7, 9]', '[1, 4, 6, 7]', '[1, 4, 6, 8, 9]', '[1, 4, 6, 8]', '[1, 4, 6, 9]', '[1, 4, 6]', '[1, 4, 7, 8, 9]', '[1, 4, 7, 8]', '[1, 4, 7, 9]', '[1, 4, 7]', '[1, 4, 8, 9]', '[1, 4, 8]', '[1, 4, 9]', '[1, 4]', '[1, 5, 6, 7, 8]', '[1, 5, 6, 7, 9]', '[1, 5, 6, 7]', '[1, 5, 6, 8, 9]', '[1, 5, 6, 8]', '[1, 5, 6, 9]', '[1, 5, 6]', '[1, 5, 7, 8, 9]', '[1, 5, 7, 8]', '[1, 5, 7, 9]', '[1, 5, 7]', '[1, 5, 8, 9]', '[1, 5, 8]', '[1, 5, 9]', '[1, 5]', '[1, 6, 7, 8, 9]', '[1, 6, 7, 8]', '[1, 6, 7, 9]', '[1, 6, 7]', '[1, 6, 8, 9]', '[1, 6, 8]', '[1, 6, 9]', '[1, 6]', '[1, 7, 8, 9]', '[1, 7, 8]', '[1, 7, 9]', '[1, 7]', '[1, 8, 9]', '[1, 8]', '[1, 9]', '[2, 3, 4, 5, 6]', '[2, 3, 4, 5, 7]', '[2, 3, 4, 5, 8]', '[2, 3, 4, 5, 9]', '[2, 3, 4, 5]', '[2, 3, 4, 6, 7]', '[2, 3, 4, 6, 8]', '[2, 3, 4, 6, 9]', '[2, 3, 4, 6]', '[2, 3, 4, 7, 8]', '[2, 3, 4, 7, 9]', '[2, 3, 4, 7]', '[2, 3, 4, 8, 9]', '[2, 3, 4, 8]', '[2, 3, 4, 9]', '[2, 3, 4]', '[2, 3, 5, 6, 7]', '[2, 3, 5, 6, 8]', '[2, 3, 5, 6, 9]', '[2, 3, 5, 6]', '[2, 3, 5, 7, 8]', '[2, 3, 5, 7, 9]', '[2, 3, 5, 7]', '[2, 3, 5, 8, 9]', '[2, 3, 5, 8]', '[2, 3, 5, 9]', '[2, 3, 5]', '[2, 3, 6, 7, 8]', '[2, 3, 6, 7, 9]', '[2, 3, 6, 7]', '[2, 3, 6, 8, 9]', '[2, 3, 6, 8]', '[2, 3, 6, 9]', '[2, 3, 6]', '[2, 3, 7, 8, 9]', '[2, 3, 7, 8]', '[2, 3, 7, 9]', '[2, 3, 7]', '[2, 3, 8, 9]', '[2, 3, 8]', '[2, 3, 9]', '[2, 3]', '[2, 4, 5, 6, 7]', '[2, 4, 5, 6, 8]', '[2, 4, 5, 6, 9]', '[2, 4, 5, 6]', '[2, 4, 5, 7, 8]', '[2, 4, 5, 7, 9]', '[2, 4, 5, 7]', '[2, 4, 5, 8, 9]', '[2, 4, 5, 8]', '[2, 4, 5, 9]', '[2, 4, 5]', '[2, 4, 6, 7, 8]', '[2, 4, 6, 7, 9]', '[2, 4, 6, 7]', '[2, 4, 6, 8, 9]', '[2, 4, 6, 8]', '[2, 4, 6, 9]', '[2, 4, 6]', '[2, 4, 7, 8, 9]', '[2, 4, 7, 8]', '[2, 4, 7, 9]', '[2, 4, 7]', '[2, 4, 8, 9]', '[2, 4, 8]', '[2, 4, 9]', '[2, 4]', '[2, 5, 6, 7, 8]', '[2, 5, 6, 7, 9]', '[2, 5, 6, 7]', '[2, 5, 6, 8, 9]', '[2, 5, 6, 8]', '[2, 5, 6, 9]', '[2, 5, 6]', '[2, 5, 7, 8, 9]', '[2, 5, 7, 8]', '[2, 5, 7, 9]', '[2, 5, 7]', '[2, 5, 8, 9]', '[2, 5, 8]', '[2, 5, 9]', '[2, 5]', '[2, 6, 7, 8, 9]', '[2, 6, 7, 8]', '[2, 6, 7, 9]', '[2, 6, 7]', '[2, 6, 8, 9]', '[2, 6, 8]', '[2, 6, 9]', '[2, 6]', '[2, 7, 8, 9]', '[2, 7, 8]', '[2, 7, 9]', '[2, 7]', '[2, 8, 9]', '[2, 8]', '[2, 9]', '[3, 4, 5, 6, 7]', '[3, 4, 5, 6, 8]', '[3, 4, 5, 6, 9]', '[3, 4, 5, 6]', '[3, 4, 5, 7, 8]', '[3, 4, 5, 7, 9]', '[3, 4, 5, 7]', '[3, 4, 5, 8, 9]', '[3, 4, 5, 8]', '[3, 4, 5, 9]', '[3, 4, 5]', '[3, 4, 6, 7, 8]', '[3, 4, 6, 7, 9]', '[3, 4, 6, 7]', '[3, 4, 6, 8, 9]', '[3, 4, 6, 8]', '[3, 4, 6, 9]', '[3, 4, 6]', '[3, 4, 7, 8, 9]', '[3, 4, 7, 8]', '[3, 4, 7, 9]', '[3, 4, 7]', '[3, 4, 8, 9]', '[3, 4, 8]', '[3, 4, 9]', '[3, 4]', '[3, 5, 6, 7, 8]', '[3, 5, 6, 7, 9]', '[3, 5, 6, 7]', '[3, 5, 6, 8, 9]', '[3, 5, 6, 8]', '[3, 5, 6, 9]', '[3, 5, 6]', '[3, 5, 7, 8, 9]', '[3, 5, 7, 8]', '[3, 5, 7, 9]', '[3, 5, 7]', '[3, 5, 8, 9]', '[3, 5, 8]', '[3, 5, 9]', '[3, 5]', '[3, 6, 7, 8, 9]', '[3, 6, 7, 8]', '[3, 6, 7, 9]', '[3, 6, 7]', '[3, 6, 8, 9]', '[3, 6, 8]', '[3, 6, 9]', '[3, 6]', '[3, 7, 8, 9]', '[3, 7, 8]', '[3, 7, 9]', '[3, 7]', '[3, 8, 9]', '[3, 8]', '[3, 9]', '[4, 5, 6, 7, 8]', '[4, 5, 6, 7, 9]', '[4, 5, 6, 7]', '[4, 5, 6, 8, 9]', '[4, 5, 6, 8]', '[4, 5, 6, 9]', '[4, 5, 6]', '[4, 5, 7, 8, 9]', '[4, 5, 7, 8]', '[4, 5, 7, 9]', '[4, 5, 7]', '[4, 5, 8, 9]', '[4, 5, 8]', '[4, 5, 9]', '[4, 5]', '[4, 6, 7, 8, 9]', '[4, 6, 7, 8]', '[4, 6, 7, 9]', '[4, 6, 7]', '[4, 6, 8, 9]', '[4, 6, 8]', '[4, 6, 9]', '[4, 6]', '[4, 7, 8, 9]', '[4, 7, 8]', '[4, 7, 9]', '[4, 7]', '[4, 8, 9]', '[4, 8]', '[4, 9]', '[5, 6, 7, 8, 9]', '[5, 6, 7, 8]', '[5, 6, 7, 9]', '[5, 6, 7]', '[5, 6, 8, 9]', '[5, 6, 8]', '[5, 6, 9]', '[5, 6]', '[5, 7, 8, 9]', '[5, 7, 8]', '[5, 7, 9]', '[5, 7]', '[5, 8, 9]', '[5, 8]', '[5, 9]', '[6, 7, 8, 9]', '[6, 7, 8]', '[6, 7, 9]', '[6, 7]', '[6, 8, 9]', '[6, 8]', '[6, 9]', '[7, 8, 9]', '[7, 8]', '[7, 9]', '[8, 9]']\n",
      "   Accuracy range: 97.582 - 99.237\n",
      "   Epoch range: 36 - 36\n",
      "\n",
      "üîç HMMR Data Validation:\n",
      "   Minimum samples per state: 1\n",
      "   ‚ö†Ô∏è Warning: Some states have very few samples\n",
      "   NaN weights: 0\n",
      "   Inf weights: 0\n",
      "\n",
      "‚úÖ HMMR data preparation complete!\n",
      "   Samples: 800\n",
      "   Features: 2464\n",
      "   States: 800\n",
      "   Ready for temporal sequence construction\n",
      "\n",
      "üéØ Single Initialization HMMR Analysis Ready:\n",
      "   ‚úÖ Data loaded and validated\n",
      "   ‚úÖ Primary activation: gelu\n",
      "   ‚úÖ Weight matrix prepared: (800, 2464)\n",
      "   ‚úÖ Labels extracted: 800 unique states\n",
      "   ‚úÖ Ready for temporal sequence construction\n",
      "\n",
      "üìä State (Label) Distribution:\n",
      "   State [0, 1, 2, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 4]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 3]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 4]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 2, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 2]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 4]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 3, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 3]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 4]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 5]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 6]: 1 samples (0.1%)\n",
      "   State [0, 1, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 7]: 1 samples (0.1%)\n",
      "   State [0, 1, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 1, 8]: 1 samples (0.1%)\n",
      "   State [0, 1, 9]: 1 samples (0.1%)\n",
      "   State [0, 1]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 4]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 5]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 3, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 3]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 4]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 5]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 6]: 1 samples (0.1%)\n",
      "   State [0, 2, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 7]: 1 samples (0.1%)\n",
      "   State [0, 2, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 2, 8]: 1 samples (0.1%)\n",
      "   State [0, 2, 9]: 1 samples (0.1%)\n",
      "   State [0, 2]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 4]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 5]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 6]: 1 samples (0.1%)\n",
      "   State [0, 3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 7]: 1 samples (0.1%)\n",
      "   State [0, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 3, 8]: 1 samples (0.1%)\n",
      "   State [0, 3, 9]: 1 samples (0.1%)\n",
      "   State [0, 3]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 5]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 6]: 1 samples (0.1%)\n",
      "   State [0, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 7]: 1 samples (0.1%)\n",
      "   State [0, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 4, 8]: 1 samples (0.1%)\n",
      "   State [0, 4, 9]: 1 samples (0.1%)\n",
      "   State [0, 4]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 6]: 1 samples (0.1%)\n",
      "   State [0, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 7]: 1 samples (0.1%)\n",
      "   State [0, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 5, 8]: 1 samples (0.1%)\n",
      "   State [0, 5, 9]: 1 samples (0.1%)\n",
      "   State [0, 5]: 1 samples (0.1%)\n",
      "   State [0, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 6, 7]: 1 samples (0.1%)\n",
      "   State [0, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 6, 8]: 1 samples (0.1%)\n",
      "   State [0, 6, 9]: 1 samples (0.1%)\n",
      "   State [0, 6]: 1 samples (0.1%)\n",
      "   State [0, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 7, 8]: 1 samples (0.1%)\n",
      "   State [0, 7, 9]: 1 samples (0.1%)\n",
      "   State [0, 7]: 1 samples (0.1%)\n",
      "   State [0, 8, 9]: 1 samples (0.1%)\n",
      "   State [0, 8]: 1 samples (0.1%)\n",
      "   State [0, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 4]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 5]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 3, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 3]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 5]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 4, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 4]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 5]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 6]: 1 samples (0.1%)\n",
      "   State [1, 2, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 7]: 1 samples (0.1%)\n",
      "   State [1, 2, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 2, 8]: 1 samples (0.1%)\n",
      "   State [1, 2, 9]: 1 samples (0.1%)\n",
      "   State [1, 2]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 4]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 5]: 1 samples (0.1%)\n",
      "   State [1, 3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 6]: 1 samples (0.1%)\n",
      "   State [1, 3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 7]: 1 samples (0.1%)\n",
      "   State [1, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 3, 8]: 1 samples (0.1%)\n",
      "   State [1, 3, 9]: 1 samples (0.1%)\n",
      "   State [1, 3]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 5]: 1 samples (0.1%)\n",
      "   State [1, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 6]: 1 samples (0.1%)\n",
      "   State [1, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 7]: 1 samples (0.1%)\n",
      "   State [1, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 4, 8]: 1 samples (0.1%)\n",
      "   State [1, 4, 9]: 1 samples (0.1%)\n",
      "   State [1, 4]: 1 samples (0.1%)\n",
      "   State [1, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 5, 6]: 1 samples (0.1%)\n",
      "   State [1, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 5, 7]: 1 samples (0.1%)\n",
      "   State [1, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 5, 8]: 1 samples (0.1%)\n",
      "   State [1, 5, 9]: 1 samples (0.1%)\n",
      "   State [1, 5]: 1 samples (0.1%)\n",
      "   State [1, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 6, 7]: 1 samples (0.1%)\n",
      "   State [1, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 6, 8]: 1 samples (0.1%)\n",
      "   State [1, 6, 9]: 1 samples (0.1%)\n",
      "   State [1, 6]: 1 samples (0.1%)\n",
      "   State [1, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 7, 8]: 1 samples (0.1%)\n",
      "   State [1, 7, 9]: 1 samples (0.1%)\n",
      "   State [1, 7]: 1 samples (0.1%)\n",
      "   State [1, 8, 9]: 1 samples (0.1%)\n",
      "   State [1, 8]: 1 samples (0.1%)\n",
      "   State [1, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 5]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 6]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 4, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 4]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 6]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 5, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 5]: 1 samples (0.1%)\n",
      "   State [2, 3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 6]: 1 samples (0.1%)\n",
      "   State [2, 3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 7]: 1 samples (0.1%)\n",
      "   State [2, 3, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 3, 8]: 1 samples (0.1%)\n",
      "   State [2, 3, 9]: 1 samples (0.1%)\n",
      "   State [2, 3]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 5]: 1 samples (0.1%)\n",
      "   State [2, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 6]: 1 samples (0.1%)\n",
      "   State [2, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 7]: 1 samples (0.1%)\n",
      "   State [2, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 4, 8]: 1 samples (0.1%)\n",
      "   State [2, 4, 9]: 1 samples (0.1%)\n",
      "   State [2, 4]: 1 samples (0.1%)\n",
      "   State [2, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 5, 6]: 1 samples (0.1%)\n",
      "   State [2, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 5, 7]: 1 samples (0.1%)\n",
      "   State [2, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 5, 8]: 1 samples (0.1%)\n",
      "   State [2, 5, 9]: 1 samples (0.1%)\n",
      "   State [2, 5]: 1 samples (0.1%)\n",
      "   State [2, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 6, 7]: 1 samples (0.1%)\n",
      "   State [2, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 6, 8]: 1 samples (0.1%)\n",
      "   State [2, 6, 9]: 1 samples (0.1%)\n",
      "   State [2, 6]: 1 samples (0.1%)\n",
      "   State [2, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 7, 8]: 1 samples (0.1%)\n",
      "   State [2, 7, 9]: 1 samples (0.1%)\n",
      "   State [2, 7]: 1 samples (0.1%)\n",
      "   State [2, 8, 9]: 1 samples (0.1%)\n",
      "   State [2, 8]: 1 samples (0.1%)\n",
      "   State [2, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 6]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 7]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 5, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 5]: 1 samples (0.1%)\n",
      "   State [3, 4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 6, 7]: 1 samples (0.1%)\n",
      "   State [3, 4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 6, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 6, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 6]: 1 samples (0.1%)\n",
      "   State [3, 4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 7]: 1 samples (0.1%)\n",
      "   State [3, 4, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 4, 8]: 1 samples (0.1%)\n",
      "   State [3, 4, 9]: 1 samples (0.1%)\n",
      "   State [3, 4]: 1 samples (0.1%)\n",
      "   State [3, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [3, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [3, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [3, 5, 6]: 1 samples (0.1%)\n",
      "   State [3, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 5, 7]: 1 samples (0.1%)\n",
      "   State [3, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 5, 8]: 1 samples (0.1%)\n",
      "   State [3, 5, 9]: 1 samples (0.1%)\n",
      "   State [3, 5]: 1 samples (0.1%)\n",
      "   State [3, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 6, 7]: 1 samples (0.1%)\n",
      "   State [3, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 6, 8]: 1 samples (0.1%)\n",
      "   State [3, 6, 9]: 1 samples (0.1%)\n",
      "   State [3, 6]: 1 samples (0.1%)\n",
      "   State [3, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 7, 8]: 1 samples (0.1%)\n",
      "   State [3, 7, 9]: 1 samples (0.1%)\n",
      "   State [3, 7]: 1 samples (0.1%)\n",
      "   State [3, 8, 9]: 1 samples (0.1%)\n",
      "   State [3, 8]: 1 samples (0.1%)\n",
      "   State [3, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [4, 5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 6, 7]: 1 samples (0.1%)\n",
      "   State [4, 5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 6, 8]: 1 samples (0.1%)\n",
      "   State [4, 5, 6, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 6]: 1 samples (0.1%)\n",
      "   State [4, 5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 7, 8]: 1 samples (0.1%)\n",
      "   State [4, 5, 7, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 7]: 1 samples (0.1%)\n",
      "   State [4, 5, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 5, 8]: 1 samples (0.1%)\n",
      "   State [4, 5, 9]: 1 samples (0.1%)\n",
      "   State [4, 5]: 1 samples (0.1%)\n",
      "   State [4, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [4, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [4, 6, 7]: 1 samples (0.1%)\n",
      "   State [4, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 6, 8]: 1 samples (0.1%)\n",
      "   State [4, 6, 9]: 1 samples (0.1%)\n",
      "   State [4, 6]: 1 samples (0.1%)\n",
      "   State [4, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 7, 8]: 1 samples (0.1%)\n",
      "   State [4, 7, 9]: 1 samples (0.1%)\n",
      "   State [4, 7]: 1 samples (0.1%)\n",
      "   State [4, 8, 9]: 1 samples (0.1%)\n",
      "   State [4, 8]: 1 samples (0.1%)\n",
      "   State [4, 9]: 1 samples (0.1%)\n",
      "   State [5, 6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [5, 6, 7, 8]: 1 samples (0.1%)\n",
      "   State [5, 6, 7, 9]: 1 samples (0.1%)\n",
      "   State [5, 6, 7]: 1 samples (0.1%)\n",
      "   State [5, 6, 8, 9]: 1 samples (0.1%)\n",
      "   State [5, 6, 8]: 1 samples (0.1%)\n",
      "   State [5, 6, 9]: 1 samples (0.1%)\n",
      "   State [5, 6]: 1 samples (0.1%)\n",
      "   State [5, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [5, 7, 8]: 1 samples (0.1%)\n",
      "   State [5, 7, 9]: 1 samples (0.1%)\n",
      "   State [5, 7]: 1 samples (0.1%)\n",
      "   State [5, 8, 9]: 1 samples (0.1%)\n",
      "   State [5, 8]: 1 samples (0.1%)\n",
      "   State [5, 9]: 1 samples (0.1%)\n",
      "   State [6, 7, 8, 9]: 1 samples (0.1%)\n",
      "   State [6, 7, 8]: 1 samples (0.1%)\n",
      "   State [6, 7, 9]: 1 samples (0.1%)\n",
      "   State [6, 7]: 1 samples (0.1%)\n",
      "   State [6, 8, 9]: 1 samples (0.1%)\n",
      "   State [6, 8]: 1 samples (0.1%)\n",
      "   State [6, 9]: 1 samples (0.1%)\n",
      "   State [7, 8, 9]: 1 samples (0.1%)\n",
      "   State [7, 8]: 1 samples (0.1%)\n",
      "   State [7, 9]: 1 samples (0.1%)\n",
      "   State [8, 9]: 1 samples (0.1%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Single Initialization Data Loading for HMMR Analysis\n",
    "print(\"=== Single Initialization Data Loading for HMMR Analysis ===\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Set up paths\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "EXPERIMENTS_DIR = ROOT / \"Experiments\"\n",
    "RESULTS_DIR = ROOT / \"notebooks_sandbox\" / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "\n",
    "def load_hmmr_single_init_data(csv_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    Load and prepare data for HMMR analysis with single initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Loading HMMR data from: {csv_path}\")\n",
    "    \n",
    "    # Check file existence\n",
    "    if not Path(csv_path).exists():\n",
    "        print(f\"‚ùå File not found: {csv_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load the data\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Loaded {len(df)} rows\")\n",
    "        \n",
    "        # Apply sample limit for HMMR computational efficiency\n",
    "        if max_samples and len(df) > max_samples:\n",
    "            df = df.head(max_samples)\n",
    "            print(f\"üìä Limited to {max_samples} samples for HMMR efficiency\")\n",
    "        \n",
    "        # Identify column types\n",
    "        weight_cols = [col for col in df.columns if col.startswith('weight ') or col.startswith('bias ')]\n",
    "        metadata_cols = [col for col in df.columns if not col.startswith('weight ') and not col.startswith('bias ')]\n",
    "        \n",
    "        print(f\"üî¢ Found {len(weight_cols)} weight columns\")\n",
    "        print(f\"üìã Found {len(metadata_cols)} metadata columns\")\n",
    "        \n",
    "        # Single initialization mode: identify primary activation\n",
    "        activation_cols = [col for col in df.columns if col in [\"silu\", \"gelu\", \"relu\", \"leakyrelu\", \"sigmoid\", \"tanh\"]]\n",
    "        active_activations = [col for col in activation_cols if df[col].sum() > 0]\n",
    "        \n",
    "        if len(active_activations) > 0:\n",
    "            # Focus on the most common activation\n",
    "            activation_counts = {col: df[col].sum() for col in active_activations}\n",
    "            primary_activation = max(activation_counts, key=activation_counts.get)\n",
    "            \n",
    "            print(f\"üéØ Single initialization mode: {primary_activation}\")\n",
    "            print(f\"   Samples with {primary_activation}: {activation_counts[primary_activation]}\")\n",
    "            \n",
    "            # Filter to primary activation\n",
    "            df_filtered = df[df[primary_activation] == 1].copy()\n",
    "            print(f\"üìä Filtered to {len(df_filtered)} samples\")\n",
    "        else:\n",
    "            df_filtered = df.copy()\n",
    "            primary_activation = \"mixed\"\n",
    "            print(f\"üìä Using all {len(df_filtered)} samples (mixed activations)\")\n",
    "        \n",
    "        # HMMR-specific data preparation\n",
    "        print(f\"\\nüîß HMMR Data Preparation:\")\n",
    "        \n",
    "        # Extract weight matrix for sequence construction\n",
    "        weight_matrix = df_filtered[weight_cols].values\n",
    "        print(f\"   Weight matrix shape: {weight_matrix.shape}\")\n",
    "        \n",
    "        # Extract labels for HMMR state definition\n",
    "        if 'label task 1' in df_filtered.columns:\n",
    "            labels = df_filtered['label task 1'].values\n",
    "        elif 'label' in df_filtered.columns:\n",
    "            labels = df_filtered['label'].values\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No label column found - using synthetic labels\")\n",
    "            labels = np.arange(len(df_filtered)) % 10  # Synthetic labels\n",
    "        \n",
    "        print(f\"   Labels shape: {labels.shape}\")\n",
    "        print(f\"   Unique labels: {sorted(np.unique(labels))}\")\n",
    "        \n",
    "        # Extract accuracy for confidence evolution sequences\n",
    "        if 'Accuracy task1' in df_filtered.columns:\n",
    "            accuracies = df_filtered['Accuracy task1'].values\n",
    "        elif 'Accuracy' in df_filtered.columns:\n",
    "            accuracies = df_filtered['Accuracy'].values\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No accuracy column found - using synthetic confidence\")\n",
    "            accuracies = np.random.uniform(0.7, 0.95, len(df_filtered))\n",
    "        \n",
    "        print(f\"   Accuracy range: {np.nanmin(accuracies):.3f} - {np.nanmax(accuracies):.3f}\")\n",
    "        \n",
    "        # Extract epochs for temporal sequence construction\n",
    "        if 'epochCNN' in df_filtered.columns:\n",
    "            epochs = df_filtered['epochCNN'].values\n",
    "        elif 'epoch' in df_filtered.columns:\n",
    "            epochs = df_filtered['epoch'].values\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No epoch column found - using synthetic epochs\")\n",
    "            epochs = np.random.choice([11, 16, 21, 26, 31, 36], len(df_filtered))\n",
    "        \n",
    "        print(f\"   Epoch range: {np.min(epochs)} - {np.max(epochs)}\")\n",
    "        \n",
    "        # Validate data for HMMR requirements\n",
    "        print(f\"\\nüîç HMMR Data Validation:\")\n",
    "        \n",
    "        # Check for sufficient samples per state (label)\n",
    "        label_counts = pd.Series(labels).value_counts()\n",
    "        min_samples_per_state = label_counts.min()\n",
    "        print(f\"   Minimum samples per state: {min_samples_per_state}\")\n",
    "        \n",
    "        if min_samples_per_state < 5:\n",
    "            print(f\"   ‚ö†Ô∏è Warning: Some states have very few samples\")\n",
    "        \n",
    "        # Check weight matrix properties\n",
    "        nan_weights = np.sum(np.isnan(weight_matrix))\n",
    "        inf_weights = np.sum(np.isinf(weight_matrix))\n",
    "        \n",
    "        print(f\"   NaN weights: {nan_weights}\")\n",
    "        print(f\"   Inf weights: {inf_weights}\")\n",
    "        \n",
    "        if nan_weights > 0 or inf_weights > 0:\n",
    "            print(f\"   ‚ö†Ô∏è Warning: Found problematic weight values\")\n",
    "            # Handle problematic values\n",
    "            weight_matrix = np.nan_to_num(weight_matrix, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            print(f\"   ‚úÖ Cleaned weight matrix\")\n",
    "        \n",
    "        # Prepare HMMR dataset\n",
    "        hmmr_data = {\n",
    "            'weights': weight_matrix,\n",
    "            'labels': labels,\n",
    "            'accuracies': accuracies,\n",
    "            'epochs': epochs,\n",
    "            'metadata': df_filtered[metadata_cols].to_dict('records'),\n",
    "            'sample_ids': df_filtered.index.tolist()\n",
    "        }\n",
    "        \n",
    "        hmmr_info = {\n",
    "            'n_samples': len(df_filtered),\n",
    "            'n_features': len(weight_cols),\n",
    "            'n_states': len(np.unique(labels)),\n",
    "            'primary_activation': primary_activation,\n",
    "            'weight_cols': weight_cols,\n",
    "            'metadata_cols': metadata_cols,\n",
    "            'label_distribution': label_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ HMMR data preparation complete!\")\n",
    "        print(f\"   Samples: {hmmr_info['n_samples']}\")\n",
    "        print(f\"   Features: {hmmr_info['n_features']}\")\n",
    "        print(f\"   States: {hmmr_info['n_states']}\")\n",
    "        print(f\"   Ready for temporal sequence construction\")\n",
    "        \n",
    "        return hmmr_data, hmmr_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading HMMR data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load data for HMMR analysis\n",
    "print(\"üîÑ Loading data for HMMR analysis...\")\n",
    "hmmr_data, hmmr_info = load_hmmr_single_init_data(\n",
    "    DATA_DIR / \"Merged zoo.csv\",\n",
    "    max_samples=800  # Optimized for HMMR computational efficiency\n",
    ")\n",
    "\n",
    "if hmmr_data is not None:\n",
    "    print(f\"\\nüéØ Single Initialization HMMR Analysis Ready:\")\n",
    "    print(f\"   ‚úÖ Data loaded and validated\")\n",
    "    print(f\"   ‚úÖ Primary activation: {hmmr_info['primary_activation']}\")\n",
    "    print(f\"   ‚úÖ Weight matrix prepared: {hmmr_data['weights'].shape}\")\n",
    "    print(f\"   ‚úÖ Labels extracted: {len(np.unique(hmmr_data['labels']))} unique states\")\n",
    "    print(f\"   ‚úÖ Ready for temporal sequence construction\")\n",
    "    \n",
    "    # Display sample distribution\n",
    "    print(f\"\\nüìä State (Label) Distribution:\")\n",
    "    for label, count in sorted(hmmr_info['label_distribution'].items()):\n",
    "        print(f\"   State {label}: {count} samples ({count/hmmr_info['n_samples']:.1%})\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚ùå Cannot proceed with HMMR analysis - no data available\")\n",
    "    hmmr_data = None\n",
    "    hmmr_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMMR_r package not available - will implement simplified version\n",
      "=== HMMR_r Analysis Setup ===\n",
      "Project root: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New\n",
      "Data directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/data\n",
      "Results directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results\n",
      "HMMR directory: /home/aymen/Documents/GitHub/Federated-Continual-learning-/New/notebooks_sandbox/results/hmmr_analysis\n",
      "PyTorch version: 2.7.1+cu128\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.3\n",
      "HMMR_r available: False\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"10_hmmr_application_to_testsets.ipynb\n",
    "\n",
    "## Implementation with Theoretical Justification\n",
    "\n",
    "This implementation addresses the methodological concerns raised above by:\n",
    "1. Constructing meaningful temporal sequences from CNN layer activations\n",
    "2. Defining hidden states as confidence evolution stages  \n",
    "3. Implementing proper federated evaluation with no data leakage\n",
    "4. Providing comprehensive baseline comparisons\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Check for HMMR_r availability\n",
    "try:\n",
    "    # Try to import HMMR_r (will need to be installed)\n",
    "    import hmmr\n",
    "    HMMR_AVAILABLE = True\n",
    "    print(\"HMMR_r package available\")\n",
    "except ImportError:\n",
    "    print(\"HMMR_r package not available - will implement simplified version\")\n",
    "    HMMR_AVAILABLE = False\n",
    "\n",
    "# Set up paths\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "RESULTS_DIR = ROOT / \"notebooks_sandbox\" / \"results\"\n",
    "HMMR_DIR = RESULTS_DIR / \"hmmr_analysis\"\n",
    "HMMR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"=== HMMR_r Analysis Setup ===\")\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"HMMR directory: {HMMR_DIR}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"HMMR_r available: {HMMR_AVAILABLE}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Theoretical Framework for Temporal Sequence Construction ===\n",
      "Temporal sequence constructor defined with theoretical justification\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Theoretical Framework for Temporal Sequence Construction\n",
    "print(\"=== Theoretical Framework for Temporal Sequence Construction ===\")\n",
    "\n",
    "class TemporalSequenceConstructor:\n",
    "    \"\"\"\n",
    "    Constructs meaningful temporal sequences from static images for HMMR_r analysis\n",
    "    \n",
    "    Theoretical justification:\n",
    "    - CNN layer activations represent a hierarchy of feature extraction\n",
    "    - This hierarchy can be interpreted as a temporal process (coarse to fine features)\n",
    "    - Hidden states in HMMR_r model confidence evolution across this hierarchy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cnn_model, device='cpu'):\n",
    "        self.cnn_model = cnn_model\n",
    "        self.device = device\n",
    "        self.cnn_model.eval()\n",
    "        \n",
    "        # Hook for capturing intermediate activations\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks to capture layer activations\"\"\"\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activations[name] = output.detach().cpu().numpy()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for key layers\n",
    "        for i, layer in enumerate(self.cnn_model.module_list):\n",
    "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "                layer.register_forward_hook(get_activation(f'layer_{i}'))\n",
    "    \n",
    "    def construct_temporal_sequence(self, image, sequence_type='hierarchical'):\n",
    "        \"\"\"\n",
    "        Construct temporal sequence from single image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image tensor\n",
    "            sequence_type: 'hierarchical', 'confidence_evolution', 'noise_robustness'\n",
    "        \n",
    "        Returns:\n",
    "            sequence: Temporal sequence as numpy array\n",
    "            metadata: Additional information about the sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if sequence_type == 'hierarchical':\n",
    "                return self._hierarchical_sequence(image)\n",
    "            elif sequence_type == 'confidence_evolution':\n",
    "                return self._confidence_evolution_sequence(image)\n",
    "            elif sequence_type == 'noise_robustness':\n",
    "                return self._noise_robustness_sequence(image)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown sequence type: {sequence_type}\")\n",
    "    \n",
    "    def _hierarchical_sequence(self, image):\n",
    "        \"\"\"\n",
    "        Construct sequence from CNN layer hierarchy (coarse to fine features)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Forward pass to capture activations\n",
    "        _ = self.cnn_model(image.to(self.device))\n",
    "        \n",
    "        sequence = []\n",
    "        layer_names = sorted(self.activations.keys())\n",
    "        \n",
    "        for layer_name in layer_names:\n",
    "            activation = self.activations[layer_name]\n",
    "            \n",
    "            # Flatten and normalize activation\n",
    "            if len(activation.shape) > 2:\n",
    "                activation = activation.reshape(activation.shape[0], -1)\n",
    "            \n",
    "            # Global average pooling to get feature vector\n",
    "            feature_vector = np.mean(activation, axis=1)\n",
    "            \n",
    "            # L2 normalization\n",
    "            feature_vector = feature_vector / (np.linalg.norm(feature_vector) + 1e-8)\n",
    "            \n",
    "            sequence.append(feature_vector)\n",
    "        \n",
    "        sequence = np.array(sequence)  # Shape: (n_layers, batch_size, n_features)\n",
    "        \n",
    "        metadata = {\n",
    "            'sequence_type': 'hierarchical',\n",
    "            'n_layers': len(layer_names),\n",
    "            'layer_names': layer_names\n",
    "        }\n",
    "        \n",
    "        return sequence, metadata\n",
    "    \n",
    "    def _confidence_evolution_sequence(self, image):\n",
    "        \"\"\"\n",
    "        Construct sequence from model confidence under different perturbations\n",
    "        \"\"\"\n",
    "        \n",
    "        perturbation_levels = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "        sequence = []\n",
    "        \n",
    "        for noise_level in perturbation_levels:\n",
    "            # Add Gaussian noise\n",
    "            noisy_image = image + torch.randn_like(image) * noise_level\n",
    "            noisy_image = torch.clamp(noisy_image, 0, 1)\n",
    "            \n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                logits = self.cnn_model(noisy_image.to(self.device))\n",
    "                probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            sequence.append(probabilities)\n",
    "        \n",
    "        sequence = np.array(sequence)  # Shape: (n_perturbations, batch_size, n_classes)\n",
    "        \n",
    "        metadata = {\n",
    "            'sequence_type': 'confidence_evolution',\n",
    "            'perturbation_levels': perturbation_levels,\n",
    "            'n_classes': sequence.shape[-1]\n",
    "        }\n",
    "        \n",
    "        return sequence, metadata\n",
    "    \n",
    "    def _noise_robustness_sequence(self, image):\n",
    "        \"\"\"\n",
    "        Construct sequence measuring robustness to different types of noise\n",
    "        \"\"\"\n",
    "        \n",
    "        noise_types = ['gaussian', 'uniform', 'salt_pepper', 'blur']\n",
    "        noise_levels = [0.0, 0.1, 0.2]\n",
    "        \n",
    "        sequence = []\n",
    "        \n",
    "        for noise_type in noise_types:\n",
    "            for noise_level in noise_levels:\n",
    "                # Apply specific noise type\n",
    "                if noise_type == 'gaussian':\n",
    "                    noisy_image = image + torch.randn_like(image) * noise_level\n",
    "                elif noise_type == 'uniform':\n",
    "                    noisy_image = image + (torch.rand_like(image) - 0.5) * 2 * noise_level\n",
    "                elif noise_type == 'salt_pepper':\n",
    "                    mask = torch.rand_like(image) < noise_level\n",
    "                    noisy_image = image.clone()\n",
    "                    noisy_image[mask] = torch.rand_like(noisy_image[mask])\n",
    "                elif noise_type == 'blur':\n",
    "                    # Simple blur using average pooling\n",
    "                    kernel_size = max(3, int(noise_level * 10))\n",
    "                    if kernel_size % 2 == 0:\n",
    "                        kernel_size += 1\n",
    "                    noisy_image = F.avg_pool2d(image, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
    "                \n",
    "                noisy_image = torch.clamp(noisy_image, 0, 1)\n",
    "                \n",
    "                # Get model predictions\n",
    "                with torch.no_grad():\n",
    "                    logits = self.cnn_model(noisy_image.to(self.device))\n",
    "                    probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                sequence.append(probabilities)\n",
    "        \n",
    "        sequence = np.array(sequence)\n",
    "        \n",
    "        metadata = {\n",
    "            'sequence_type': 'noise_robustness',\n",
    "            'noise_types': noise_types,\n",
    "            'noise_levels': noise_levels,\n",
    "            'n_classes': sequence.shape[-1]\n",
    "        }\n",
    "        \n",
    "        return sequence, metadata\n",
    "\n",
    "print(\"Temporal sequence constructor defined with theoretical justification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single Initialization HMMR Analysis Framework ===\n",
      "üöÄ Initializing Single Initialization HMMR Analyzer...\n",
      "üîß Initializing HMMR Analyzer:\n",
      "   Samples: 800\n",
      "   Features: 2464\n",
      "   States: 800\n",
      "   Primary activation: gelu\n",
      "üîÑ Constructing confidence_evolution temporal sequences...\n",
      "‚úÖ Constructed 1 sequences\n",
      "   Sequence shape: (1, 300)\n",
      "üîÑ Implementing simplified HMMR with 0 hidden states...\n",
      "‚ùå Error in HMMR analysis: n_components=50 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'\n",
      "‚ùå HMMR analysis failed\n",
      "\n",
      "üéØ Single Initialization HMMR Analysis Complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Single Initialization HMMR Analysis Framework\n",
    "print(\"=== Single Initialization HMMR Analysis Framework ===\")\n",
    "\n",
    "class SingleInitHMMRAnalyzer:\n",
    "    \"\"\"\n",
    "    HMMR analysis framework optimized for single initialization data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hmmr_data, hmmr_info, device='cpu'):\n",
    "        self.data = hmmr_data\n",
    "        self.info = hmmr_info\n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"üîß Initializing HMMR Analyzer:\")\n",
    "        print(f\"   Samples: {self.info['n_samples']}\")\n",
    "        print(f\"   Features: {self.info['n_features']}\")\n",
    "        print(f\"   States: {self.info['n_states']}\")\n",
    "        print(f\"   Primary activation: {self.info['primary_activation']}\")\n",
    "    \n",
    "    def construct_synthetic_temporal_sequences(self, sequence_type='confidence_evolution'):\n",
    "        \"\"\"\n",
    "        Construct synthetic temporal sequences from static weight data\n",
    "        \n",
    "        Since we don't have actual CNN activations, we construct meaningful sequences\n",
    "        from weight patterns and metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"üîÑ Constructing {sequence_type} temporal sequences...\")\n",
    "        \n",
    "        sequences = []\n",
    "        sequence_metadata = []\n",
    "        \n",
    "        weights = self.data['weights']\n",
    "        labels = self.data['labels']\n",
    "        accuracies = self.data['accuracies']\n",
    "        epochs = self.data['epochs']\n",
    "        \n",
    "        if sequence_type == 'confidence_evolution':\n",
    "            # Create sequences based on accuracy evolution across epochs\n",
    "            unique_epochs = np.sort(np.unique(epochs))\n",
    "            \n",
    "            for epoch in unique_epochs:\n",
    "                epoch_mask = epochs == epoch\n",
    "                epoch_weights = weights[epoch_mask]\n",
    "                epoch_labels = labels[epoch_mask]\n",
    "                epoch_accuracies = accuracies[epoch_mask]\n",
    "                \n",
    "                if len(epoch_weights) > 0:\n",
    "                    # Create sequence based on weight statistics\n",
    "                    sequence = []\n",
    "                    \n",
    "                    # Weight statistics as sequence elements\n",
    "                    weight_mean = np.mean(epoch_weights, axis=0)\n",
    "                    weight_std = np.std(epoch_weights, axis=0)\n",
    "                    weight_range = np.ptp(epoch_weights, axis=0)\n",
    "                    \n",
    "                    # Combine into sequence\n",
    "                    sequence_element = np.concatenate([\n",
    "                        weight_mean[:100],  # Limit for computational efficiency\n",
    "                        weight_std[:100],\n",
    "                        weight_range[:100]\n",
    "                    ])\n",
    "                    \n",
    "                    sequences.append(sequence_element)\n",
    "                    \n",
    "                    sequence_metadata.append({\n",
    "                        'epoch': epoch,\n",
    "                        'n_samples': len(epoch_weights),\n",
    "                        'mean_accuracy': np.mean(epoch_accuracies),\n",
    "                        'unique_labels': len(np.unique(epoch_labels))\n",
    "                    })\n",
    "        \n",
    "        elif sequence_type == 'weight_evolution':\n",
    "            # Create sequences based on weight similarity patterns\n",
    "            sample_indices = np.random.choice(len(weights), min(50, len(weights)), replace=False)\n",
    "            \n",
    "            for i in range(0, len(sample_indices), 5):\n",
    "                batch_indices = sample_indices[i:i+5]\n",
    "                batch_weights = weights[batch_indices]\n",
    "                batch_labels = labels[batch_indices]\n",
    "                \n",
    "                # Sequence based on pairwise weight similarities\n",
    "                sequence = []\n",
    "                for j in range(len(batch_weights)):\n",
    "                    # Compute similarity to other weights in batch\n",
    "                    similarities = []\n",
    "                    for k in range(len(batch_weights)):\n",
    "                        if j != k:\n",
    "                            similarity = np.corrcoef(batch_weights[j], batch_weights[k])[0, 1]\n",
    "                            if not np.isnan(similarity):\n",
    "                                similarities.append(similarity)\n",
    "                    \n",
    "                    if similarities:\n",
    "                        sequence.append([np.mean(similarities), np.std(similarities)])\n",
    "                \n",
    "                if sequence:\n",
    "                    sequences.append(np.array(sequence).flatten())\n",
    "                    sequence_metadata.append({\n",
    "                        'batch_id': i // 5,\n",
    "                        'n_samples': len(batch_weights),\n",
    "                        'unique_labels': len(np.unique(batch_labels))\n",
    "                    })\n",
    "        \n",
    "        sequences = np.array(sequences) if sequences else np.array([])\n",
    "        \n",
    "        print(f\"‚úÖ Constructed {len(sequences)} sequences\")\n",
    "        print(f\"   Sequence shape: {sequences.shape if len(sequences) > 0 else 'N/A'}\")\n",
    "        \n",
    "        return sequences, sequence_metadata\n",
    "    \n",
    "    def implement_simplified_hmmr(self, sequences, n_hidden_states=3):\n",
    "        \"\"\"\n",
    "        Implement simplified HMMR-like analysis since full HMMR_r may not be available\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(sequences) == 0:\n",
    "            print(\"‚ùå No sequences available for HMMR analysis\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üîÑ Implementing simplified HMMR with {n_hidden_states} hidden states...\")\n",
    "        \n",
    "        try:\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sklearn.mixture import GaussianMixture\n",
    "            from sklearn.decomposition import PCA\n",
    "            \n",
    "            # Reduce dimensionality for better clustering\n",
    "            if sequences.shape[1] > 50:\n",
    "                pca = PCA(n_components=50)\n",
    "                sequences_reduced = pca.fit_transform(sequences)\n",
    "                print(f\"   Reduced dimensionality: {sequences.shape[1]} -> {sequences_reduced.shape[1]}\")\n",
    "            else:\n",
    "                sequences_reduced = sequences\n",
    "                pca = None\n",
    "            \n",
    "            # Fit Gaussian Mixture Model (simplified HMM)\n",
    "            gmm = GaussianMixture(n_components=n_hidden_states, random_state=42)\n",
    "            hidden_states = gmm.fit_predict(sequences_reduced)\n",
    "            \n",
    "            # Analyze state properties\n",
    "            state_analysis = {}\n",
    "            for state in range(n_hidden_states):\n",
    "                state_mask = hidden_states == state\n",
    "                state_sequences = sequences_reduced[state_mask]\n",
    "                \n",
    "                state_analysis[state] = {\n",
    "                    'n_sequences': np.sum(state_mask),\n",
    "                    'mean_sequence': np.mean(state_sequences, axis=0),\n",
    "                    'std_sequence': np.std(state_sequences, axis=0),\n",
    "                    'centroid': gmm.means_[state]\n",
    "                }\n",
    "            \n",
    "            # Compute transition probabilities (simplified)\n",
    "            transition_matrix = np.zeros((n_hidden_states, n_hidden_states))\n",
    "            for i in range(len(hidden_states) - 1):\n",
    "                current_state = hidden_states[i]\n",
    "                next_state = hidden_states[i + 1]\n",
    "                transition_matrix[current_state, next_state] += 1\n",
    "            \n",
    "            # Normalize transition matrix\n",
    "            row_sums = transition_matrix.sum(axis=1)\n",
    "            transition_matrix = transition_matrix / (row_sums[:, np.newaxis] + 1e-8)\n",
    "            \n",
    "            results = {\n",
    "                'hidden_states': hidden_states,\n",
    "                'state_analysis': state_analysis,\n",
    "                'transition_matrix': transition_matrix,\n",
    "                'gmm_model': gmm,\n",
    "                'pca_model': pca,\n",
    "                'n_hidden_states': n_hidden_states,\n",
    "                'sequences': sequences,\n",
    "                'sequences_reduced': sequences_reduced\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Simplified HMMR analysis completed\")\n",
    "            print(f\"   State distribution: {np.bincount(hidden_states)}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in HMMR analysis: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_hmmr_results(self, hmmr_results, sequence_metadata):\n",
    "        \"\"\"\n",
    "        Analyze and interpret HMMR results\n",
    "        \"\"\"\n",
    "        \n",
    "        if hmmr_results is None:\n",
    "            print(\"‚ùå No HMMR results to analyze\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üìä Analyzing HMMR results...\")\n",
    "        \n",
    "        hidden_states = hmmr_results['hidden_states']\n",
    "        state_analysis = hmmr_results['state_analysis']\n",
    "        transition_matrix = hmmr_results['transition_matrix']\n",
    "        \n",
    "        # Analyze state characteristics\n",
    "        print(f\"\\nüéØ Hidden State Analysis:\")\n",
    "        for state, analysis in state_analysis.items():\n",
    "            print(f\"   State {state}: {analysis['n_sequences']} sequences\")\n",
    "            \n",
    "            # Correlate with metadata if available\n",
    "            if sequence_metadata:\n",
    "                state_indices = np.where(hidden_states == state)[0]\n",
    "                state_metadata = [sequence_metadata[i] for i in state_indices if i < len(sequence_metadata)]\n",
    "                \n",
    "                if state_metadata:\n",
    "                    mean_accuracies = [m.get('mean_accuracy', 0) for m in state_metadata if 'mean_accuracy' in m]\n",
    "                    if mean_accuracies:\n",
    "                        print(f\"      Mean accuracy: {np.mean(mean_accuracies):.3f}\")\n",
    "                    \n",
    "                    epochs = [m.get('epoch', 0) for m in state_metadata if 'epoch' in m]\n",
    "                    if epochs:\n",
    "                        print(f\"      Epoch range: {min(epochs)} - {max(epochs)}\")\n",
    "        \n",
    "        # Analyze transition patterns\n",
    "        print(f\"\\nüîÑ Transition Analysis:\")\n",
    "        for i in range(len(transition_matrix)):\n",
    "            for j in range(len(transition_matrix[i])):\n",
    "                if transition_matrix[i, j] > 0.1:  # Significant transitions\n",
    "                    print(f\"   State {i} -> State {j}: {transition_matrix[i, j]:.3f}\")\n",
    "        \n",
    "        # Create visualizations\n",
    "        self._visualize_hmmr_results(hmmr_results, sequence_metadata)\n",
    "        \n",
    "        return {\n",
    "            'state_analysis': state_analysis,\n",
    "            'transition_matrix': transition_matrix,\n",
    "            'interpretation': self._interpret_states(state_analysis, sequence_metadata)\n",
    "        }\n",
    "    \n",
    "    def _visualize_hmmr_results(self, hmmr_results, sequence_metadata):\n",
    "        \"\"\"\n",
    "        Create visualizations for HMMR results\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            hidden_states = hmmr_results['hidden_states']\n",
    "            transition_matrix = hmmr_results['transition_matrix']\n",
    "            sequences_reduced = hmmr_results['sequences_reduced']\n",
    "            \n",
    "            # 1. State distribution\n",
    "            axes[0,0].hist(hidden_states, bins=len(np.unique(hidden_states)), alpha=0.7)\n",
    "            axes[0,0].set_title('Hidden State Distribution')\n",
    "            axes[0,0].set_xlabel('Hidden State')\n",
    "            axes[0,0].set_ylabel('Frequency')\n",
    "            \n",
    "            # 2. Transition matrix heatmap\n",
    "            sns.heatmap(transition_matrix, annot=True, fmt='.3f', ax=axes[0,1], cmap='Blues')\n",
    "            axes[0,1].set_title('State Transition Matrix')\n",
    "            axes[0,1].set_xlabel('To State')\n",
    "            axes[0,1].set_ylabel('From State')\n",
    "            \n",
    "            # 3. Sequence visualization (first 2 dimensions if available)\n",
    "            if sequences_reduced.shape[1] >= 2:\n",
    "                scatter = axes[1,0].scatter(sequences_reduced[:, 0], sequences_reduced[:, 1], \n",
    "                                          c=hidden_states, cmap='viridis', alpha=0.6)\n",
    "                axes[1,0].set_title('Sequences in Reduced Space')\n",
    "                axes[1,0].set_xlabel('Dimension 1')\n",
    "                axes[1,0].set_ylabel('Dimension 2')\n",
    "                plt.colorbar(scatter, ax=axes[1,0])\n",
    "            \n",
    "            # 4. State evolution over time (if metadata available)\n",
    "            if sequence_metadata:\n",
    "                epochs = [m.get('epoch', i) for i, m in enumerate(sequence_metadata[:len(hidden_states)])]\n",
    "                axes[1,1].plot(epochs[:len(hidden_states)], hidden_states, 'o-', alpha=0.7)\n",
    "                axes[1,1].set_title('State Evolution Over Epochs')\n",
    "                axes[1,1].set_xlabel('Epoch')\n",
    "                axes[1,1].set_ylabel('Hidden State')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(HMMR_DIR / 'single_init_hmmr_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"‚úÖ Visualizations saved to {HMMR_DIR}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error creating visualizations: {e}\")\n",
    "    \n",
    "    def _interpret_states(self, state_analysis, sequence_metadata):\n",
    "        \"\"\"\n",
    "        Provide interpretation of hidden states\n",
    "        \"\"\"\n",
    "        \n",
    "        interpretations = {}\n",
    "        \n",
    "        for state, analysis in state_analysis.items():\n",
    "            interpretation = {\n",
    "                'state_id': state,\n",
    "                'sample_count': analysis['n_sequences'],\n",
    "                'characteristics': []\n",
    "            }\n",
    "            \n",
    "            # Simple interpretation based on sequence count\n",
    "            if analysis['n_sequences'] > len(state_analysis) * 0.4:\n",
    "                interpretation['characteristics'].append('Frequent/Dominant state')\n",
    "            elif analysis['n_sequences'] < len(state_analysis) * 0.2:\n",
    "                interpretation['characteristics'].append('Rare/Transition state')\n",
    "            else:\n",
    "                interpretation['characteristics'].append('Intermediate state')\n",
    "            \n",
    "            interpretations[state] = interpretation\n",
    "        \n",
    "        return interpretations\n",
    "\n",
    "# Initialize HMMR analyzer if data is available\n",
    "if hmmr_data is not None:\n",
    "    print(\"üöÄ Initializing Single Initialization HMMR Analyzer...\")\n",
    "    hmmr_analyzer = SingleInitHMMRAnalyzer(hmmr_data, hmmr_info)\n",
    "    \n",
    "    # Construct temporal sequences\n",
    "    sequences, sequence_metadata = hmmr_analyzer.construct_synthetic_temporal_sequences(\n",
    "        sequence_type='confidence_evolution'\n",
    "    )\n",
    "    \n",
    "    if len(sequences) > 0:\n",
    "        # Run simplified HMMR analysis\n",
    "        hmmr_results = hmmr_analyzer.implement_simplified_hmmr(\n",
    "            sequences, \n",
    "            n_hidden_states=min(3, len(sequences)//2)\n",
    "        )\n",
    "        \n",
    "        if hmmr_results is not None:\n",
    "            # Analyze results\n",
    "            analysis_results = hmmr_analyzer.analyze_hmmr_results(hmmr_results, sequence_metadata)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Single Initialization HMMR Analysis Complete!\")\n",
    "            print(f\"   Results saved to {HMMR_DIR}\")\n",
    "        else:\n",
    "            print(f\"‚ùå HMMR analysis failed\")\n",
    "    else:\n",
    "        print(f\"‚ùå No sequences constructed\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Cannot initialize HMMR analyzer - no data available\")\n",
    "    hmmr_analyzer = None\n",
    "\n",
    "print(f\"\\nüéØ Single Initialization HMMR Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
