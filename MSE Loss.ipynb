{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77eb910c-dcef-4911-af76-79db919f0ceb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77eb910c-dcef-4911-af76-79db919f0ceb",
    "outputId": "bd9643ab-841c-418d-8b9d-cef77acca616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: torcheval in /home/crns/anaconda3/lib/python3.9/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in /home/crns/anaconda3/lib/python3.9/site-packages (from torcheval) (4.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: numpy==1.23 in /home/crns/anaconda3/lib/python3.9/site-packages (1.23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: dill in /home/crns/anaconda3/lib/python3.9/site-packages (0.3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: torchinfo in /home/crns/anaconda3/lib/python3.9/site-packages (1.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install avalanche-lib==0.3.1\n",
    "!pip install torcheval\n",
    "#!pip install numba\n",
    "!pip install numpy==1.23\n",
    "!pip install dill\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67987ec9-75b3-44c6-b850-c5d1d4451b3c",
   "metadata": {
    "id": "67987ec9-75b3-44c6-b850-c5d1d4451b3c"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation ,FFMpegWriter ,PillowWriter\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07cf8e38-6b5f-4942-af5b-cd20e21394ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
      "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.126967  -0.043210  -0.231192   \n",
      "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.173608  -0.093011   0.303516   \n",
      "\n",
      "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
      "0   0.342562  -0.112175   0.279830  -0.136550  -0.327724  98.643333     36  \n",
      "1   0.819489  -0.519285  -0.212039   0.197068  -0.362670  98.906667     36  \n",
      "\n",
      "[2 rows x 2483 columns]\n",
      "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
      "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.132555  -0.076178  -0.208102   \n",
      "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.174280  -0.063854   0.268694   \n",
      "\n",
      "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
      "0   0.336691  -0.102071   0.272633  -0.127867  -0.293667  98.856667     31  \n",
      "1   0.757035  -0.461682  -0.193141   0.179911  -0.332172  98.795000     31  \n",
      "\n",
      "[2 rows x 2483 columns]\n",
      "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
      "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.116577  -0.138303  -0.155354   \n",
      "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.105676   0.021190   0.162796   \n",
      "\n",
      "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
      "0   0.271865  -0.049130   0.221364  -0.091457  -0.179646  98.573333     16  \n",
      "1   0.516581  -0.253635  -0.132210   0.122809  -0.235453  98.015000     16  \n",
      "\n",
      "[2 rows x 2483 columns]\n",
      "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
      "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.101165  -0.123394  -0.139609   \n",
      "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.062117   0.038516   0.119955   \n",
      "\n",
      "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
      "0   0.238313  -0.040772   0.191127  -0.071601  -0.135258  98.091667     11  \n",
      "1   0.415654  -0.163165  -0.104250   0.108712  -0.193884  97.706667     11  \n",
      "\n",
      "[2 rows x 2483 columns]\n",
      "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
      "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.128177  -0.101753  -0.193208   \n",
      "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.158810  -0.024862   0.240861   \n",
      "\n",
      "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
      "0   0.318145  -0.088461   0.253567  -0.111446  -0.257247  98.695000     26  \n",
      "1   0.688969  -0.389595  -0.174334   0.158978  -0.312399  98.661667     26  \n",
      "\n",
      "[2 rows x 2483 columns]\n",
      "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
      "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.120194  -0.124954  -0.172320   \n",
      "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.140234   0.000427   0.200429   \n",
      "\n",
      "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
      "0   0.299861  -0.070635   0.243260  -0.101398  -0.225274  98.658333     21  \n",
      "1   0.608320  -0.321982  -0.157072   0.136263  -0.274709  98.370000     21  \n",
      "\n",
      "[2 rows x 2483 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36468, 2483)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv_files = [file for file in os.listdir(\".\") if file.endswith('.csv')]\n",
    "# N= []\n",
    "# print(csv_files[0][-6:-4])\n",
    "# for file in csv_files:\n",
    "#     df=pd.read_csv(file)\n",
    "#     df[\"epoch\"]=file[-6:-4]\n",
    "#     print(df.head(2))\n",
    "#     N.append(df)\n",
    "# df = pd.concat(N,axis=0)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b85fdd-94f0-4077-b668-61f0eacd391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2456</th>\n",
       "      <th>bias 2457</th>\n",
       "      <th>bias 2458</th>\n",
       "      <th>bias 2459</th>\n",
       "      <th>bias 2460</th>\n",
       "      <th>bias 2461</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126967</td>\n",
       "      <td>-0.043210</td>\n",
       "      <td>-0.231192</td>\n",
       "      <td>0.342562</td>\n",
       "      <td>-0.112175</td>\n",
       "      <td>0.279830</td>\n",
       "      <td>-0.136550</td>\n",
       "      <td>-0.327724</td>\n",
       "      <td>98.643333</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173608</td>\n",
       "      <td>-0.093011</td>\n",
       "      <td>0.303516</td>\n",
       "      <td>0.819489</td>\n",
       "      <td>-0.519285</td>\n",
       "      <td>-0.212039</td>\n",
       "      <td>0.197068</td>\n",
       "      <td>-0.362670</td>\n",
       "      <td>98.906667</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 2483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  0  1  2  3  4  5  6  7  8  ...  bias 2456  bias 2457  bias 2458  \\\n",
       "0  [0, 1]  1  1  0  0  0  0  0  0  0  ...  -0.126967  -0.043210  -0.231192   \n",
       "1  [0, 2]  1  0  1  0  0  0  0  0  0  ...  -0.173608  -0.093011   0.303516   \n",
       "\n",
       "   bias 2459  bias 2460  bias 2461  bias 2462  bias 2463   Accuracy  epoch  \n",
       "0   0.342562  -0.112175   0.279830  -0.136550  -0.327724  98.643333     36  \n",
       "1   0.819489  -0.519285  -0.212039   0.197068  -0.362670  98.906667     36  \n",
       "\n",
       "[2 rows x 2483 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8323c49d-240a-4e46-98ec-c00e2e783a63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8323c49d-240a-4e46-98ec-c00e2e783a63",
    "outputId": "9229fdc7-630d-4e87-d54b-3771391ef44c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a555c2-ee7a-455d-99cf-0f36e327abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524bc7f5-5643-4ce7-9903-fcaa3dd072ed",
   "metadata": {
    "id": "524bc7f5-5643-4ce7-9903-fcaa3dd072ed"
   },
   "outputs": [],
   "source": [
    "#df.to_csv(\"./data/Merged zoo.csv\",index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655000ba-32ed-41d3-93a3-522336521239",
   "metadata": {
    "id": "655000ba-32ed-41d3-93a3-522336521239"
   },
   "source": [
    "# Trainloader code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e190d-0d8d-44fd-be0d-457b0adaccba",
   "metadata": {
    "id": "3c9e190d-0d8d-44fd-be0d-457b0adaccba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe5550b-3705-4a36-a575-5f0d1da80409",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfe5550b-3705-4a36-a575-5f0d1da80409",
    "outputId": "6b4af11a-ab4d-499f-da84-133d1f354bb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight 0',\n",
       " 'weight 1',\n",
       " 'weight 2',\n",
       " 'weight 3',\n",
       " 'weight 4',\n",
       " 'weight 5',\n",
       " 'weight 6',\n",
       " 'weight 7',\n",
       " 'weight 8',\n",
       " 'weight 9']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=pd.read_csv(\"zoo epoch11.csv\")\n",
    "#params_cols=list(df.columns[17:-2])\n",
    "#params_cols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b4950f-64db-4129-abb0-ac1014946610",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "73b4950f-64db-4129-abb0-ac1014946610",
    "outputId": "e42d68e7-bc1f-45f5-958c-b5491c6876ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2456</th>\n",
       "      <th>bias 2457</th>\n",
       "      <th>bias 2458</th>\n",
       "      <th>bias 2459</th>\n",
       "      <th>bias 2460</th>\n",
       "      <th>bias 2461</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126967</td>\n",
       "      <td>-0.043210</td>\n",
       "      <td>-0.231192</td>\n",
       "      <td>0.342562</td>\n",
       "      <td>-0.112175</td>\n",
       "      <td>0.279830</td>\n",
       "      <td>-0.136550</td>\n",
       "      <td>-0.327724</td>\n",
       "      <td>98.643333</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173608</td>\n",
       "      <td>-0.093011</td>\n",
       "      <td>0.303516</td>\n",
       "      <td>0.819489</td>\n",
       "      <td>-0.519285</td>\n",
       "      <td>-0.212039</td>\n",
       "      <td>0.197068</td>\n",
       "      <td>-0.362670</td>\n",
       "      <td>98.906667</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215478</td>\n",
       "      <td>-0.561090</td>\n",
       "      <td>-0.173014</td>\n",
       "      <td>-0.151766</td>\n",
       "      <td>-0.098772</td>\n",
       "      <td>0.597230</td>\n",
       "      <td>0.088529</td>\n",
       "      <td>-0.206130</td>\n",
       "      <td>98.376667</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152755</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>-0.191886</td>\n",
       "      <td>0.263275</td>\n",
       "      <td>-0.203316</td>\n",
       "      <td>0.037706</td>\n",
       "      <td>-0.203713</td>\n",
       "      <td>-0.283887</td>\n",
       "      <td>98.911667</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212532</td>\n",
       "      <td>-0.178356</td>\n",
       "      <td>0.269947</td>\n",
       "      <td>-0.030196</td>\n",
       "      <td>-0.511167</td>\n",
       "      <td>-0.002527</td>\n",
       "      <td>0.088270</td>\n",
       "      <td>-0.004224</td>\n",
       "      <td>98.868333</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6073</th>\n",
       "      <td>[0, 1, 2, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101830</td>\n",
       "      <td>-0.144904</td>\n",
       "      <td>-0.117668</td>\n",
       "      <td>0.225659</td>\n",
       "      <td>0.035038</td>\n",
       "      <td>0.308250</td>\n",
       "      <td>-0.235618</td>\n",
       "      <td>-0.178583</td>\n",
       "      <td>98.363333</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>[0, 1, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.192891</td>\n",
       "      <td>-0.107484</td>\n",
       "      <td>0.367572</td>\n",
       "      <td>-0.155872</td>\n",
       "      <td>-0.013740</td>\n",
       "      <td>0.085309</td>\n",
       "      <td>-0.171841</td>\n",
       "      <td>98.335000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6075</th>\n",
       "      <td>[0, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188059</td>\n",
       "      <td>-0.038179</td>\n",
       "      <td>-0.014452</td>\n",
       "      <td>0.211741</td>\n",
       "      <td>0.194827</td>\n",
       "      <td>0.106201</td>\n",
       "      <td>-0.510387</td>\n",
       "      <td>-0.169262</td>\n",
       "      <td>98.366667</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230161</td>\n",
       "      <td>-0.049153</td>\n",
       "      <td>-0.121743</td>\n",
       "      <td>0.162385</td>\n",
       "      <td>0.205859</td>\n",
       "      <td>0.166138</td>\n",
       "      <td>-0.227936</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>98.335000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198345</td>\n",
       "      <td>-0.186030</td>\n",
       "      <td>-0.136558</td>\n",
       "      <td>0.638662</td>\n",
       "      <td>-0.356857</td>\n",
       "      <td>0.128831</td>\n",
       "      <td>0.073418</td>\n",
       "      <td>0.102498</td>\n",
       "      <td>98.196667</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36468 rows × 2483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               label  0  1  2  3  4  5  6  7  8  ...  \\\n",
       "0                             [0, 1]  1  1  0  0  0  0  0  0  0  ...   \n",
       "1                             [0, 2]  1  0  1  0  0  0  0  0  0  ...   \n",
       "2                             [0, 3]  1  0  0  1  0  0  0  0  0  ...   \n",
       "3                             [0, 4]  1  0  0  0  1  0  0  0  0  ...   \n",
       "4                             [0, 5]  1  0  0  0  0  1  0  0  0  ...   \n",
       "...                              ... .. .. .. .. .. .. .. .. ..  ...   \n",
       "6073     [0, 1, 2, 4, 5, 6, 7, 8, 9]  1  1  1  0  1  1  1  1  1  ...   \n",
       "6074     [0, 1, 3, 4, 5, 6, 7, 8, 9]  1  1  0  1  1  1  1  1  1  ...   \n",
       "6075     [0, 2, 3, 4, 5, 6, 7, 8, 9]  1  0  1  1  1  1  1  1  1  ...   \n",
       "6076     [1, 2, 3, 4, 5, 6, 7, 8, 9]  0  1  1  1  1  1  1  1  1  ...   \n",
       "6077  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  1  1  1  1  1  1  1  1  1  ...   \n",
       "\n",
       "      bias 2456  bias 2457  bias 2458  bias 2459  bias 2460  bias 2461  \\\n",
       "0     -0.126967  -0.043210  -0.231192   0.342562  -0.112175   0.279830   \n",
       "1     -0.173608  -0.093011   0.303516   0.819489  -0.519285  -0.212039   \n",
       "2     -0.215478  -0.561090  -0.173014  -0.151766  -0.098772   0.597230   \n",
       "3      0.152755   0.408451  -0.191886   0.263275  -0.203316   0.037706   \n",
       "4     -0.212532  -0.178356   0.269947  -0.030196  -0.511167  -0.002527   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "6073  -0.101830  -0.144904  -0.117668   0.225659   0.035038   0.308250   \n",
       "6074   0.004528   0.192891  -0.107484   0.367572  -0.155872  -0.013740   \n",
       "6075   0.188059  -0.038179  -0.014452   0.211741   0.194827   0.106201   \n",
       "6076  -0.230161  -0.049153  -0.121743   0.162385   0.205859   0.166138   \n",
       "6077  -0.198345  -0.186030  -0.136558   0.638662  -0.356857   0.128831   \n",
       "\n",
       "      bias 2462  bias 2463   Accuracy  epoch  \n",
       "0     -0.136550  -0.327724  98.643333     36  \n",
       "1      0.197068  -0.362670  98.906667     36  \n",
       "2      0.088529  -0.206130  98.376667     36  \n",
       "3     -0.203713  -0.283887  98.911667     36  \n",
       "4      0.088270  -0.004224  98.868333     36  \n",
       "...         ...        ...        ...    ...  \n",
       "6073  -0.235618  -0.178583  98.363333     21  \n",
       "6074   0.085309  -0.171841  98.335000     21  \n",
       "6075  -0.510387  -0.169262  98.366667     21  \n",
       "6076  -0.227936   0.025500  98.335000     21  \n",
       "6077   0.073418   0.102498  98.196667     21  \n",
       "\n",
       "[36468 rows x 2483 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e612addf-8dfb-468b-8d03-47befe820d92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e612addf-8dfb-468b-8d03-47befe820d92",
    "outputId": "3c25d1e9-3193-44b4-d2bf-5758770123d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 23436/23436 [00:00<00:00, 290260.00it/s]\n"
     ]
    }
   ],
   "source": [
    "Pairs_exp=list()\n",
    "All=list(range(10))\n",
    "for sample in range(2,9,1):\n",
    "    S=combinations(range(10), sample)\n",
    "    #All=list(range(10))\n",
    "    for i in S :\n",
    "        L1=list(i)\n",
    "        L2=[k for k in All if k not in L1]\n",
    "        for sample2 in range(2,9,1):\n",
    "            S2=combinations(L2, sample2)\n",
    "            for j in S2:\n",
    "                pair=[]\n",
    "                pair.append(L1)\n",
    "                sub=list(j)\n",
    "                pair.append(sub)\n",
    "                pair.sort()\n",
    "                if pair not in Pairs_exp :\n",
    "                    Pairs_exp.append(pair)\n",
    "print(len(Pairs_exp))\n",
    "train_pair=[]\n",
    "train_tgt=[]\n",
    "\n",
    "val_pair=[]\n",
    "val_tgt=[]\n",
    "\n",
    "test_pair=[]\n",
    "test_tgt=[]\n",
    "\n",
    "for pair in tqdm(Pairs_exp):\n",
    "    if (len(pair[0])==5 and len(pair[1])==5) or(len(pair[0])==4 and len(pair[1])==4) or(len(pair[0])==3 and len(pair[1])==3) or (len(pair[0])==2 and len(pair[1])==2):\n",
    "        test_pair.append(pair)\n",
    "        tg=(pair[0]+pair[1]).sort()\n",
    "        test_tgt.append(tg)\n",
    "    elif (len(pair[0])==7 and len(pair[1])==2) or (len(pair[0])==2 and len(pair[1])==7) or (len(pair[0])==6 and len(pair[1])==3) or (len(pair[0])==3 and len(pair[1])==6) or (len(pair[0])==4 and len(pair[1])==5) or (len(pair[0])==5 and len(pair[1])==4):\n",
    "        val_pair.append(pair)\n",
    "        tg=(pair[0]+pair[1]).sort()\n",
    "        val_tgt.append(tg)\n",
    "    else :\n",
    "        train_pair.append(pair)\n",
    "        tg=(pair[0]+pair[1]).sort()\n",
    "        train_tgt.append(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85be4507-0074-4bc9-b6d1-452d19b44061",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_192701/1008660192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b9c9f0-2573-48d1-af10-51fb1286111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2456</th>\n",
       "      <th>bias 2457</th>\n",
       "      <th>bias 2458</th>\n",
       "      <th>bias 2459</th>\n",
       "      <th>bias 2460</th>\n",
       "      <th>bias 2461</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 2483 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, gelu, relu, silu, tanh, sigmoid, leakyrelu, weight 0, weight 1, weight 2, weight 3, weight 4, weight 5, weight 6, weight 7, weight 8, weight 9, weight 10, weight 11, weight 12, weight 13, weight 14, weight 15, weight 16, weight 17, weight 18, weight 19, weight 20, weight 21, weight 22, weight 23, weight 24, weight 25, weight 26, weight 27, weight 28, weight 29, weight 30, weight 31, weight 32, weight 33, weight 34, weight 35, weight 36, weight 37, weight 38, weight 39, weight 40, weight 41, weight 42, weight 43, weight 44, weight 45, weight 46, weight 47, weight 48, weight 49, weight 50, weight 51, weight 52, weight 53, weight 54, weight 55, weight 56, weight 57, weight 58, weight 59, weight 60, weight 61, weight 62, weight 63, weight 64, weight 65, weight 66, weight 67, weight 68, weight 69, weight 70, weight 71, weight 72, weight 73, weight 74, weight 75, weight 76, weight 77, weight 78, weight 79, weight 80, weight 81, weight 82, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 2483 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"label\"]=='{}'.format([6,7,8,9]))&(df[\"epoch\"]==31)&(df[\"tanh\"]==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ac6c5-ce0c-439c-ba7a-b43b292b5ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c05e93-3693-461f-835b-f2531e98a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4435e-3031-4b42-bf4c-0dbf927e9c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374f7aa1-ed9d-4144-8d36-2a5a56d70a56",
   "metadata": {
    "id": "374f7aa1-ed9d-4144-8d36-2a5a56d70a56"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84569999-9515-4669-9ea3-30a6b66e5805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db08af9-f50c-40a5-b575-9ce7b075c802",
   "metadata": {
    "id": "2db08af9-f50c-40a5-b575-9ce7b075c802"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "\n",
    "import random\n",
    "import ast\n",
    "        \n",
    "class CustomDataset(TensorDataset): #L_activations=[\"gelu\",\"relu\",\"silu\",\"leakyrelu\",\"sigmoid\",\"tanh\"]\n",
    "    def __init__(self,L_exp,Index,nb_batches=3000,df_path=\"./data/Merged zoo.csv\"):\n",
    "\n",
    "        L_Stream1=[]\n",
    "        L_Stream2=[]\n",
    "        tgt=[]\n",
    "        self.df_path = df_path\n",
    "        self.Index=Index\n",
    "        self.df=pd.read_csv(df_path)\n",
    "        self.L_exp=L_exp\n",
    "        params_cols=list(self.df.columns[17:-2])\n",
    "        \n",
    "        \n",
    "        numeric_columns = self.df.columns[1:]\n",
    "        self.df[numeric_columns] = self.df[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "        L_Exp=[]\n",
    "        L_ACC=[]\n",
    "        L_indexes=[]\n",
    "        \n",
    "        num_rows = len(self.df)\n",
    "        random.seed(42)\n",
    "        # Define the number of sublists and their sizes\n",
    "        self.num_sublists =nb_batches\n",
    "        sublist_size = num_rows // self.num_sublists\n",
    "\n",
    "        # Create a list of indices\n",
    "        indices = list(range(num_rows))\n",
    "        \n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Create sublists of shuffled indices\n",
    "        Batches = [indices[i * sublist_size:(i + 1) * sublist_size] for i in range(self.num_sublists)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        Batch_df = self.df.iloc[Batches[self.Index]]\n",
    "        Batch_df  = Batch_df.dropna()\n",
    "        #self.transform = transforms.ToTensor()\n",
    "        labels=[ ast.literal_eval(s) for s in np.unique(Batch_df[\"label\"]) ]\n",
    "        for index in list(Batch_df.index):\n",
    "            for pair in L_exp:\n",
    "                label=ast.literal_eval(self.df.at[index,\"label\"])\n",
    "                if (pair[0]== label):\n",
    "                    \n",
    "                    L_Stream1.append(torch.from_numpy(Batch_df.loc[index][params_cols].to_numpy().astype('float64')))\n",
    "                    ACC1=Batch_df.loc[index][\"Accuracy\"]\n",
    "                    \n",
    "                    epoch=Batch_df.at[index,\"epoch\"]\n",
    "                    selected_row = self.df.iloc[index, 11:17]  \n",
    "                    columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "                    activ=columns_with_one\n",
    "                    \n",
    "                    \n",
    "                    rowk=self.df[(self.df[\"label\"]=='{}'.format(pair[1]))&(self.df[\"epoch\"]==int(epoch))&(self.df[activ[0]]==float(1))]\n",
    "                    L_Stream2.append(torch.from_numpy(rowk[params_cols].to_numpy().astype('float64')))\n",
    "                    ACC2=float(rowk[\"Accuracy\"].values)\n",
    "                    ind2=int(rowk.index[0])\n",
    "                    tg=pair[0]+pair[1]\n",
    "                    tg.sort()\n",
    "                    \n",
    "                    rowk=self.df[(self.df[\"label\"]=='{}'.format(tg))&\n",
    "                             (self.df[\"epoch\"]==int(epoch.tolist()))\n",
    "                             &(self.df[activ[0]]==float(1))]\n",
    "                    tgt.append(torch.from_numpy(rowk[params_cols].to_numpy().astype('float64')))\n",
    "                    ACC3=float(rowk[\"Accuracy\"].values)\n",
    "                    ind3=int(rowk.index[0])\n",
    "                    \n",
    "                    \n",
    "                    L_Exp.append([pair[0],pair[1]])\n",
    "                    L_ACC.append([ACC1,ACC2,ACC3])\n",
    "                    L_indexes.append([index,ind2,ind3])\n",
    "        Stream1=torch.stack(L_Stream1)\n",
    "        Stream2=torch.stack(L_Stream2)\n",
    "        target=torch.stack(tgt)\n",
    "        \n",
    "        Stream2=Stream2.reshape((int(Stream2.shape[0]), int(Stream2.shape[2])))\n",
    "        target=target.reshape((target.shape[0], target.shape[2]))\n",
    "        \n",
    "        train_dataset = TensorDataset(Stream1,Stream2, target)\n",
    "        self.loaded=train_dataset\n",
    "        self.exp=L_Exp\n",
    "        self.ACC=L_ACC\n",
    "        self.batch_indices=L_indexes\n",
    "        self.artifacts= self.loaded ,self.exp,self.ACC,self.batch_indices\n",
    "    def __len__(self):\n",
    "        return len(self.exp)\n",
    "    \n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         q=0\n",
    "#         for x1,x2,tg in self.loaded:\n",
    "            \n",
    "#             if q==idx and isinstance(x1, type(None))and isinstance(x2, type(None))and isinstance(tg, type(None))and (len(self.ACC)>0) and (len(self.batch_indices)>0):\n",
    "#                 print(x1,x2,tg,[self.exp[q],self.ACC[q],self.batch_indices[q]])\n",
    "#                 return x1,x2,tg,[self.exp[q],self.ACC[q],self.batch_indices[q]]\n",
    "\n",
    "#             else:\n",
    "#                 continue\n",
    "#             q=q+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528288f7-d903-4c3f-af78-136599ed3ce9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "528288f7-d903-4c3f-af78-136599ed3ce9",
    "outputId": "bb6a4f9c-d8b3-4be5-b0dd-6433aa047b8d"
   },
   "outputs": [],
   "source": [
    "cs=CustomDataset(val_pair,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2492e7ac-d3ba-4500-878d-ff5985223814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a268ce95-235d-4221-92cd-c2eb743b51a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a268ce95-235d-4221-92cd-c2eb743b51a1",
    "outputId": "e8fa2c03-20fa-456c-f9f0-48c8aa141716"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#cs=CustomDataset(val_pair,0,30)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x1,x2,tg \u001b[38;5;129;01min\u001b[39;00m DataLoader(\u001b[43mcs\u001b[49m\u001b[38;5;241m.\u001b[39mloaded,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m      \u001b[38;5;28mprint\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape,x2\u001b[38;5;241m.\u001b[39mshape,tg\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#cs=CustomDataset(val_pair,0,30)\n",
    "for x1,x2,tg in DataLoader(cs.loaded,batch_size=600, shuffle=False):\n",
    "     print(x1.shape,x2.shape,tg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693d0076-cd0f-47ec-a23c-f10cfde045cc",
   "metadata": {
    "id": "693d0076-cd0f-47ec-a23c-f10cfde045cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([97.7883], dtype=torch.float64),\n",
       "  tensor([97.4650], dtype=torch.float64),\n",
       "  tensor([98.0550], dtype=torch.float64)],\n",
       " [tensor([12415]), tensor([12772]), tensor([13167])])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ACC ,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc207f96-30cb-4314-a6fc-5b87497b1967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label        [0, 6, 9]\n",
       "0                    1\n",
       "1                    0\n",
       "2                    0\n",
       "3                    0\n",
       "               ...    \n",
       "bias 2461     0.206612\n",
       "bias 2462    -0.048973\n",
       "bias 2463     0.155417\n",
       "Accuracy     97.496667\n",
       "epoch               36\n",
       "Name: 4129, Length: 2483, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[4129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6113b58-7a3c-46f2-aa9d-33cd93694893",
   "metadata": {
    "id": "d6113b58-7a3c-46f2-aa9d-33cd93694893"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97.49666666666668, 97.46333333333334, 97.72333333333331)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[4129][\"Accuracy\"],df.iloc[4806][\"Accuracy\"],df.iloc[5055][\"Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e9ad19-0c94-4e75-9f2d-3efa0d2b1ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([12415]), tensor([12772]), tensor([13167])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045b5b9-750a-4236-b6d4-3cea89fdf1a2",
   "metadata": {
    "id": "f045b5b9-750a-4236-b6d4-3cea89fdf1a2"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387b2953-3794-42d9-a327-5e7cca27939d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "387b2953-3794-42d9-a327-5e7cca27939d",
    "outputId": "29fef800-a2d7-43d4-dafb-9fa69bda52ca"
   },
   "outputs": [],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e1b3c7-eae9-472b-8d5e-2fcade409e9b",
   "metadata": {
    "id": "f4e1b3c7-eae9-472b-8d5e-2fcade409e9b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "from einops import repeat\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# # Transformer Shared Layers\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80,device=\"cuda:2\"): #d\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "\n",
    "        # create constant 'pe' matrix with values dependant on\n",
    "        # pos and i\n",
    "        self.pe = self._generate_positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        # dynamically adjust positional encoding matrix based on sequence length\n",
    "        pe = self.pe[:, :seq_len]\n",
    "        pe=pe.to(self.device)\n",
    "        x=x.to(self.device)\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "    def _generate_positional_encoding(self, max_seq_len, d_model):\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7cb2ca-7fee-4e0e-ad6a-bfbd880f1d30",
   "metadata": {
    "id": "8f7cb2ca-7fee-4e0e-ad6a-bfbd880f1d30"
   },
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "\n",
    "    return output, scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores, sc = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output, sc\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, normalize=True, dropout=0.1, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self.norm_1 = Norm(d_model)\n",
    "            self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff=d_ff, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        if self.normalize:\n",
    "            x2 = self.norm_1(x)\n",
    "        else:\n",
    "            x2 = x.clone()\n",
    "        res, sc = self.attn(x2, x2, x2, mask)\n",
    "        # x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x = x + self.dropout_1(res)\n",
    "        if self.normalize:\n",
    "            x2 = self.norm_2(x)\n",
    "        else:\n",
    "            x2 = x.clone()\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        # return x\n",
    "        return x, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4cd349-fa4c-42c1-8330-6c8b77c63f72",
   "metadata": {
    "id": "6e4cd349-fa4c-42c1-8330-6c8b77c63f72"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = (\n",
    "            self.alpha\n",
    "            * (x - x.mean(dim=-1, keepdim=True))\n",
    "            / (x.std(dim=-1, keepdim=True) + self.eps)\n",
    "            + self.bias\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3bbd0b1-6ed0-4c53-9762-6e323034719a",
   "metadata": {
    "id": "f3bbd0b1-6ed0-4c53-9762-6e323034719a"
   },
   "outputs": [],
   "source": [
    "class EmbedderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, seed=22):\n",
    "        super().__init__()\n",
    "        #print(\"EmbedderNeuroneGroup\")\n",
    "        self.neuron_l1 = nn.Linear(200, d_model) #8\n",
    "        self.neuron_l2 = nn.Linear(72, d_model) #12\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        #print(\"multi-linear method\",v.shape)\n",
    "\n",
    "        l = []\n",
    "\n",
    "        for ndx in range(8):\n",
    "            idx_start = ndx * 200\n",
    "            idx_end = idx_start + 200\n",
    "            l.append(self.neuron_l1(v[:,idx_start:idx_end]))\n",
    "\n",
    "        # l2\n",
    "        for ndx in range(12):\n",
    "            idx_start = 200*8 + ndx * 72\n",
    "            idx_end = idx_start + 72\n",
    "            l.append(self.neuron_l2(v[:,idx_start:idx_end]))\n",
    "        #print(len(l))\n",
    "        #print(len(l[0]))\n",
    "        final = torch.stack(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbc54b7-041c-4dbc-a3cc-c22554a8c8f2",
   "metadata": {
    "id": "fdbc54b7-041c-4dbc-a3cc-c22554a8c8f2"
   },
   "outputs": [],
   "source": [
    "class EncoderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, N, heads, max_seq_len, dropout, d_ff):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = EmbedderNeuronGroup(d_model)\n",
    "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
    "        print(\"encoder droupout init\",dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, normalize=True,dropout=dropout, d_ff=d_ff), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        scores = []\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            self.layers[i] = self.layers[i].to(device)\n",
    "            x, sc = self.layers[i](x, mask)\n",
    "            scores.append(sc)\n",
    "        return self.norm(x), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c1a547-55ed-45b9-9b9b-6a3c9a5d2f6f",
   "metadata": {
    "id": "56c1a547-55ed-45b9-9b9b-6a3c9a5d2f6f"
   },
   "outputs": [],
   "source": [
    "# max_seq_len=176,\n",
    "# N=4\n",
    "# heads=3\n",
    "# d_model=900\n",
    "# d_ff=900\n",
    "# neck=700\n",
    "# dropout=0.1\n",
    "# # Enc=EncoderNeuronGroup(d_model=d_model, N=N, heads=heads, max_seq_len=max_seq_len, dropout=dropout,d_ff=d_ff)\n",
    "# # vec1 = torch.rand(1,2464)\n",
    "# # res,scores=Enc(vec1)\n",
    "# # res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1616a-7a25-4af5-bc73-e64a0612d0ad",
   "metadata": {
    "id": "05d1616a-7a25-4af5-bc73-e64a0612d0ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a844bdf8-de0a-4fb1-8d6e-bbbcd841b278",
   "metadata": {
    "id": "a844bdf8-de0a-4fb1-8d6e-bbbcd841b278"
   },
   "outputs": [],
   "source": [
    "# vec2neck = nn.Linear(d_ff*2, neck)\n",
    "# print(res.shape)\n",
    "# out3=torch.cat([res,res], dim=2)\n",
    "# print(\"neck input:\",out3.shape)\n",
    "# sum_r=torch.sum(out3, dim=1, keepdim=False)\n",
    "# vec2=vec2neck(sum_r)\n",
    "# print(len(vec2))\n",
    "# tanh = nn.Tanh()\n",
    "# neck_t=tanh(vec2)\n",
    "# print(\"neck shape:\",neck_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db4a2a5-8711-4948-8b8a-7e298d4502ac",
   "metadata": {
    "id": "5db4a2a5-8711-4948-8b8a-7e298d4502ac"
   },
   "outputs": [],
   "source": [
    "class Seq2Vec(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Define linear layers\n",
    "        self.linear = nn.Linear(max_seq_len * d_model, 2464)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)  # Flatten the sequence dimension\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Neck2Seq(nn.Module):\n",
    "    def __init__(self, d_model, neck,max_seq_length):\n",
    "        super().__init__()\n",
    "        self.neurons = nn.ModuleList([nn.Linear(neck, d_model) for _ in range(max_seq_length)])\n",
    "    def forward(self, x):\n",
    "        l = [neuron(x) for neuron in self.neurons]\n",
    "        final = torch.stack(l, dim=1)\n",
    "        return final\n",
    "class DecoderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, N, heads, max_seq_len, dropout, d_ff, neck):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Neck2Seq(d_model, neck,max_seq_len)\n",
    "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
    "        print(\"decoder droupout init\",dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads,normalize=True,dropout=dropout, d_ff=d_ff), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        self.lay = Seq2Vec(d_model=d_model,max_seq_len=max_seq_len)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        scores = []\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x, sc = self.layers[i](x, mask)\n",
    "            scores.append(sc)\n",
    "        return self.lay(self.norm(x)), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150e43a-f20a-4606-b1a8-5fb75ea2e1f4",
   "metadata": {
    "id": "4150e43a-f20a-4606-b1a8-5fb75ea2e1f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa74a80-dee8-4710-a221-a448b37758d1",
   "metadata": {
    "id": "efa74a80-dee8-4710-a221-a448b37758d1"
   },
   "outputs": [],
   "source": [
    "# Dec=DecoderNeuronGroup(d_model=d_model, N=N, heads=heads, max_seq_len=max_seq_len, dropout=dropout,d_ff=d_ff,neck=neck)\n",
    "# res,scores=Dec(neck_t)\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48474d-d348-4b5f-bdfb-3edabe5d46cf",
   "metadata": {
    "id": "da48474d-d348-4b5f-bdfb-3edabe5d46cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5c635-1abd-40e5-a4c3-70ac38d69bf5",
   "metadata": {
    "id": "ddb5c635-1abd-40e5-a4c3-70ac38d69bf5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2198b3a0-4243-4028-a1d1-c2992c56ab3d",
   "metadata": {
    "id": "2198b3a0-4243-4028-a1d1-c2992c56ab3d"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class TransformerAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_seq_len=10,\n",
    "        N=1,\n",
    "        heads=1,\n",
    "        d_model=100,\n",
    "        d_ff=100,\n",
    "        neck=20,\n",
    "        dropout=0.1,\n",
    "        **kwargs,):\n",
    "        super().__init__()\n",
    "        self.N=N\n",
    "        self.heads=heads\n",
    "        self.dropout=dropout\n",
    "        self.d_ff=d_ff\n",
    "        self.d_model=d_model\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.neck=neck\n",
    "\n",
    "\n",
    "        self.enc1 = EncoderNeuronGroup(d_model=self.d_model, N=self.N, heads=self.heads, max_seq_len=self.max_seq_len, dropout=self.dropout,d_ff=self.d_ff)\n",
    "        self.enc2 = EncoderNeuronGroup(d_model=self.d_model, N=self.N, heads=self.heads, max_seq_len=self.max_seq_len, dropout=self.dropout,d_ff=self.d_ff)\n",
    "        self.dec = DecoderNeuronGroup(d_model=self.d_model, N=self.N, heads=self.heads, max_seq_len=self.max_seq_len, dropout=self.dropout,d_ff=self.d_ff,neck=self.neck)\n",
    "        # Addition Approach\n",
    "        #print(\"Addition Approach!\")\n",
    "        self.vec2neck = nn.Linear(self.d_ff*2, self.neck)\n",
    "        # Stacking Approach\n",
    "        #print(\"Stack Approach!\")\n",
    "        #self.vec2neck = nn.Linear(2*self.d_ff * self.max_seq_len, self.neck)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Xavier Uniform Initialitzation\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, inp1,inp2):\n",
    "\n",
    "        # First Approach\n",
    "        out1, scEnc1 = self.enc1(inp1)\n",
    "        #print(\"Encoder 1 shape:\",out1.shape)\n",
    "        out2, scEnc2 = self.enc2(inp2)\n",
    "        #print(\"Encoder 2 shape:\",out2.shape)\n",
    "        out3=torch.cat([out1,out2], dim=2)\n",
    "\n",
    "        #print(\"neck input:\",out3.shape)\n",
    "        sum_r=torch.sum(out3, dim=1, keepdim=False)\n",
    "        vec2=self.vec2neck(sum_r)\n",
    "        #print(len(vec2))\n",
    "        tanh = nn.Tanh()\n",
    "        neck_t=tanh(vec2)\n",
    "        #print(\"neck shape:\",neck_t.shape)\n",
    "\n",
    "        out, scDec = self.dec(neck_t)\n",
    "        #print(\"decoder shape:\",out.shape)\n",
    "        return out, neck_t, scEnc1,scEnc2, scDec\n",
    "\n",
    "    def count_parameters(self, model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def numParams(self):\n",
    "        encNumParams = self.count_parameters(self.enc1)\n",
    "        neckNumParams = self.count_parameters(self.vec2neck)\n",
    "        decNumParams = self.count_parameters(self.dec)\n",
    "        modelParams = self.count_parameters(self)\n",
    "\n",
    "        return (\n",
    "            \"EncParams: {}, NeckParams: {}, DecParams: {}, || ModelParams: {} \".format(\n",
    "                encNumParams, neckNumParams, decNumParams, modelParams\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51fad956-1f6d-487c-a3a9-1003e7e55b65",
   "metadata": {
    "id": "51fad956-1f6d-487c-a3a9-1003e7e55b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 1], [2, 3, 4]], [[0, 1], [2, 3, 5]], [[0, 1], [2, 3, 6]], [[0, 1], [2, 3, 7]], [[0, 1], [2, 3, 8]], [[0, 1], [2, 3, 9]], [[0, 1], [2, 4, 5]], [[0, 1], [2, 4, 6]], [[0, 1], [2, 4, 7]], [[0, 1], [2, 4, 8]]]\n",
      "[[[0, 2, 3, 5], [7, 9]], [[1, 4], [3, 7, 8]], [[0, 3, 4], [1, 2, 6, 7, 9]], [[0, 5, 6, 8, 9], [3, 4, 7]], [[0, 2, 3, 5, 8], [1, 6, 9]], [[0, 6, 7, 8, 9], [2, 4, 5]], [[0, 4, 7, 8], [1, 2, 3, 5, 6, 9]], [[2, 5], [6, 8, 9]], [[0, 5, 6, 8], [3, 7, 9]], [[0, 1, 3, 4, 7], [6, 8, 9]]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(881)\n",
    "print(train_pair[:10])\n",
    "random.shuffle(test_pair)\n",
    "random.shuffle(train_pair)\n",
    "random.shuffle(val_pair)\n",
    "print(train_pair[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8ef6a42-2b3e-4fc8-babe-2d6f6ff37500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8ef6a42-2b3e-4fc8-babe-2d6f6ff37500",
    "outputId": "7301efd4-e73d-46ce-baa8-b3b12a63c900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4431, 16545, 2460, 4431, 16545, 2460)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pair),len(train_pair),len(val_pair),len(test_tgt),len(train_tgt),len(val_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e843a-7f62-4bdb-bfe2-c55960b6b649",
   "metadata": {
    "id": "5b6e843a-7f62-4bdb-bfe2-c55960b6b649"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619486cb-4a43-408f-976c-74c7d6a840d9",
   "metadata": {
    "id": "619486cb-4a43-408f-976c-74c7d6a840d9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207ffc08-3290-4087-bf01-1dfeae03df17",
   "metadata": {
    "id": "207ffc08-3290-4087-bf01-1dfeae03df17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder droupout init 0.12\n",
      "encoder droupout init 0.12\n",
      "decoder droupout init 0.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mod = TransformerAE(max_seq_len=20,\n",
    "#                     N=4,\n",
    "#                     heads=4,\n",
    "#                     d_model=900,\n",
    "#                     d_ff=900,\n",
    "#                     neck=400,\n",
    "#                     dropout=0.12\n",
    "#                    )\n",
    "\n",
    "mod = TransformerAE(max_seq_len=20,\n",
    "                    N=4,\n",
    "                    heads=4,\n",
    "                    d_model=300,\n",
    "                    d_ff=300,\n",
    "                    neck=50,\n",
    "                    dropout=0.12\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f35825-6623-41d4-9747-5e6c1d657c65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71f35825-6623-41d4-9747-5e6c1d657c65",
    "outputId": "9429472d-98a4-47db-c47e-af78aadfba49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncParams: 2254800, NeckParams: 30050, DecParams: 17265064, || ModelParams: 21804714 \n",
      "Output Shape:  torch.Size([1, 2464])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "TransformerAE                                 --\n",
       "├─EncoderNeuronGroup: 1-1                     --\n",
       "│    └─EmbedderNeuronGroup: 2-1               --\n",
       "│    │    └─Linear: 3-1                       60,300\n",
       "│    │    └─Linear: 3-2                       21,900\n",
       "│    └─PositionalEncoder: 2-2                 --\n",
       "│    └─ModuleList: 2-3                        --\n",
       "│    │    └─EncoderLayer: 3-3                 543,000\n",
       "│    │    └─EncoderLayer: 3-4                 543,000\n",
       "│    │    └─EncoderLayer: 3-5                 543,000\n",
       "│    │    └─EncoderLayer: 3-6                 543,000\n",
       "│    └─Norm: 2-4                              600\n",
       "├─EncoderNeuronGroup: 1-2                     --\n",
       "│    └─EmbedderNeuronGroup: 2-5               --\n",
       "│    │    └─Linear: 3-7                       60,300\n",
       "│    │    └─Linear: 3-8                       21,900\n",
       "│    └─PositionalEncoder: 2-6                 --\n",
       "│    └─ModuleList: 2-7                        --\n",
       "│    │    └─EncoderLayer: 3-9                 543,000\n",
       "│    │    └─EncoderLayer: 3-10                543,000\n",
       "│    │    └─EncoderLayer: 3-11                543,000\n",
       "│    │    └─EncoderLayer: 3-12                543,000\n",
       "│    └─Norm: 2-8                              600\n",
       "├─DecoderNeuronGroup: 1-3                     --\n",
       "│    └─Neck2Seq: 2-9                          --\n",
       "│    │    └─ModuleList: 3-13                  306,000\n",
       "│    └─PositionalEncoder: 2-10                --\n",
       "│    └─ModuleList: 2-11                       --\n",
       "│    │    └─EncoderLayer: 3-14                543,000\n",
       "│    │    └─EncoderLayer: 3-15                543,000\n",
       "│    │    └─EncoderLayer: 3-16                543,000\n",
       "│    │    └─EncoderLayer: 3-17                543,000\n",
       "│    └─Norm: 2-12                             600\n",
       "│    └─Seq2Vec: 2-13                          --\n",
       "│    │    └─Linear: 3-18                      14,786,464\n",
       "├─Linear: 1-4                                 30,050\n",
       "├─Tanh: 1-5                                   --\n",
       "======================================================================\n",
       "Total params: 21,804,714\n",
       "Trainable params: 21,804,714\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(mod.numParams())\n",
    "x1 = torch.rand(1,2464)\n",
    "x2 = torch.rand(1,2464)\n",
    "mod=mod.to(device)\n",
    "\n",
    "#x1=x1.to(torch.float32)\n",
    "#x2=x2.to(torch.float32)\n",
    "x1=x1.to(device)\n",
    "x2=x2.to(device)\n",
    "mod=mod.to(device)\n",
    "out = mod(x1,x2)\n",
    "print(\"Output Shape: \", out[0].shape)\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e0eefd-9bb1-4f3e-b722-78f4108efe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "class ClassSpecificImageFolder(datasets.DatasetFolder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root,\n",
    "            dropped_classes=[],\n",
    "            transform = None,\n",
    "            target_transform = None,\n",
    "            loader = datasets.folder.default_loader,\n",
    "            is_valid_file = None,\n",
    "    ):\n",
    "        self.dropped_classes = dropped_classes\n",
    "        super(ClassSpecificImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                                       transform=transform,\n",
    "                                                       target_transform=target_transform,\n",
    "                                                       is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "        classes = [c for c in classes if c not in self.dropped_classes]\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in,\n",
    "        nlin=\"leakyrelu\",\n",
    "        dropout=0.0,\n",
    "        init_type=\"uniform\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init module list\n",
    "        self.module_list = nn.ModuleList()\n",
    "        ### ASSUMES 28x28 image size\n",
    "        ## compose layer 1\n",
    "        self.module_list.append(nn.Conv2d(channels_in, 8, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        # apply dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 2\n",
    "        self.module_list.append(nn.Conv2d(8, 6, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 3\n",
    "        self.module_list.append(nn.Conv2d(6, 4, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add flatten layer\n",
    "        self.module_list.append(nn.Flatten())\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(3 * 3 * 4, 20))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(20, 10))\n",
    "\n",
    "        ### initialize weights with se methods\n",
    "        self.initialize_weights(init_type)\n",
    "\n",
    "    def initialize_weights(self, init_type):\n",
    "        # print(\"initialze model\")\n",
    "        for m in self.module_list:\n",
    "            if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "                if init_type == \"xavier_uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if init_type == \"xavier_normal\":\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                if init_type == \"uniform\":\n",
    "                    torch.nn.init.uniform_(m.weight)\n",
    "                if init_type == \"normal\":\n",
    "                    torch.nn.init.normal_(m.weight)\n",
    "                if init_type == \"kaiming_normal\":\n",
    "                    torch.nn.init.kaiming_normal_(m.weight)\n",
    "                if init_type == \"kaiming_uniform\":\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight)\n",
    "                # set bias to some small non-zero value\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    def get_nonlin(self, nlin):\n",
    "        # apply nonlinearity\n",
    "        if nlin == \"leakyrelu\":\n",
    "            return nn.LeakyReLU()\n",
    "        if nlin == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        if nlin == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        if nlin == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        if nlin == \"silu\":\n",
    "            return nn.SiLU()\n",
    "        if nlin == \"gelu\":\n",
    "            return nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward prop through module_list\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward_activations(self, x):\n",
    "        # forward prop through module_list\n",
    "        activations = []\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "            if (\n",
    "                isinstance(layer, nn.Tanh)\n",
    "                or isinstance(layer, nn.Sigmoid)\n",
    "                or isinstance(layer, nn.ReLU)\n",
    "                or isinstance(layer, nn.LeakyReLU)\n",
    "                or isinstance(layer, nn.SiLU)\n",
    "                or isinstance(layer, nn.GELU)\n",
    "                or isinstance(layer, ORU)\n",
    "                or isinstance(layer, ERU)\n",
    "            ):\n",
    "                activations.append(x)\n",
    "        return x, activations\n",
    "def train(model, trainloader, optimizer, criterion,nb_classes):\n",
    "    List_mx=[]\n",
    "    model.train()\n",
    "    #print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image\n",
    "        labels = labels\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        #mx=multiclass_confusion_matrix(preds ,labels,nb_classes,normalize=\"pred\")\n",
    "        #List_mx.append(mx)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc,List_mx\n",
    "\n",
    "\n",
    "def validate(model, testloader, criterion,nb_classes):\n",
    "    List_mx=[]\n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            if data is None:  # Skip None values\n",
    "                continue\n",
    "            counter += 1\n",
    "            \n",
    "            image, labels = data\n",
    "            image = image\n",
    "            labels = labels\n",
    "            # forward pass\n",
    "            outputs = model(image)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            # calculate the accuracy\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "            #mx=multiclass_confusion_matrix(preds ,labels,nb_classes,normalize=\"pred\")\n",
    "            #List_mx.append(mx)\n",
    "        \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss, epoch_acc,List_mx\n",
    "def create_frame(step,ax,data):\n",
    "    ax=ax.cla()\n",
    "    sns.heatmap(data[step][-1].cpu(),annot=True,cmap=\"cubehelix\",ax=ax,cbar=False)\n",
    "    plt.title('Epoch {} training {}'.format(step,exp)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5cf2e31-6295-42f1-9d0e-284ad545f94c",
   "metadata": {
    "id": "b5cf2e31-6295-42f1-9d0e-284ad545f94c"
   },
   "outputs": [],
   "source": [
    "#L_activations=[\"gelu\",\"relu\",\"silu\",\"leakyrelu\",\"sigmoid\",\"tanh\"]\n",
    "#csv_files,L_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c657276f-7578-45a1-a58b-db8a21914d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4932)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(2) in https://arxiv.org/pdf/2209.14733.pdf\n",
    "vec1 = torch.rand(1,2464)\n",
    "vec2 = torch.rand(1,2464)\n",
    "class LWLN_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LWLN_loss, self).__init__()\n",
    "    def forward(self, vec1,vec2):\n",
    "        loss = (torch.mean((vec1[:,0:208]-vec2[:,0:208])**2)/vec2[:,0:208].std() + \n",
    "                 torch.mean((vec1[:,208:1414]-vec2[:,208:1414])**2)/vec2[:,208:1414].std()+ \n",
    "                 torch.mean((vec1[:,1414:1514]-vec2[:,1414:1514])**2)/vec2[:,1414:1514].std()+\n",
    "                 torch.mean((vec1[:,1514:2254]-vec2[:,1514:2254])**2)/vec2[:,1514:2254].std()+\n",
    "                 torch.mean((vec1[:,2254:2464]-vec2[:,2254:2464])**2)/vec2[:,2254:2464].std())/(6)\n",
    "        \n",
    "        return loss\n",
    "LW=LWLN_loss()\n",
    "LW(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48684b12-575d-4354-84ed-04ce490df41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label task 1</th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy task1</th>\n",
       "      <th>label task 2</th>\n",
       "      <th>Accuracy task2</th>\n",
       "      <th>weight 0</th>\n",
       "      <th>weight 1</th>\n",
       "      <th>weight 2</th>\n",
       "      <th>weight 3</th>\n",
       "      <th>weight 4</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2459</th>\n",
       "      <th>bias 2460</th>\n",
       "      <th>bias 2461</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Loader Set</th>\n",
       "      <th>Reconstructed Accuracy ID</th>\n",
       "      <th>Actual Accuracy</th>\n",
       "      <th>Reconstructed Accuracy OOD</th>\n",
       "      <th>Transformer Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 2474 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label task 1, index, Accuracy task1, label task 2, Accuracy task2, weight 0, weight 1, weight 2, weight 3, weight 4, weight 5, weight 6, weight 7, weight 8, weight 9, weight 10, weight 11, weight 12, weight 13, weight 14, weight 15, weight 16, weight 17, weight 18, weight 19, weight 20, weight 21, weight 22, weight 23, weight 24, weight 25, weight 26, weight 27, weight 28, weight 29, weight 30, weight 31, weight 32, weight 33, weight 34, weight 35, weight 36, weight 37, weight 38, weight 39, weight 40, weight 41, weight 42, weight 43, weight 44, weight 45, weight 46, weight 47, weight 48, weight 49, weight 50, weight 51, weight 52, weight 53, weight 54, weight 55, weight 56, weight 57, weight 58, weight 59, weight 60, weight 61, weight 62, weight 63, weight 64, weight 65, weight 66, weight 67, weight 68, weight 69, weight 70, weight 71, weight 72, weight 73, weight 74, weight 75, weight 76, weight 77, weight 78, weight 79, weight 80, weight 81, weight 82, weight 83, weight 84, weight 85, weight 86, weight 87, weight 88, weight 89, weight 90, weight 91, weight 92, weight 93, weight 94, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 2474 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cols=[\"label task 1\",\"index\",\"Accuracy task1\",\\\n",
    "      \"label task 2\",\"Accuracy task2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Loader Set\",\"Reconstructed Accuracy ID\",\"Actual Accuracy\",\"Reconstructed Accuracy OOD\",\"Transformer Loss\"] \n",
    "\n",
    "print(len(Cols))\n",
    "predicted_Weights= pd.DataFrame(columns=Cols)\n",
    "\n",
    "# row=[\"\".format(task1),int(ind[0]),ACC[0],\"\".format(task2),ACC[1]]+vector_aux.to_list()+[\"train\",valid_epoch_acc0,ACC[2],valid_epoch_acc1,L_train[-1]]\n",
    "# predicted_Weights.append(row, ignore_index=True)\n",
    "predicted_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d1219-5d6c-4cc5-8322-b6fe33a49cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ind)\n",
    "# selected_row = df.iloc[int(ind[0]), 11:17]  \n",
    "# columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "# activ=columns_with_one\n",
    "# activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6653e61-bb82-427b-a7f9-354fd0856147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bf76a-72be-41fd-9a21-4433ff4c82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca949505-5643-4dbc-bdbd-3e53449ade57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN(1,activ[0],0.,\"kaiming_uniform\")\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66630c4-8ead-441b-bce4-00386ffee671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_tr.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922863a9-529d-438b-9ab2-a807666aa5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f56e3-8497-46b4-ba17-bbe91824b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce564cc0-f758-4760-900d-05f447a89d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 58, 58)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(EXP),len(ACC),len(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2f6c6f0-6f3a-42b1-8bb3-4c8ffd873bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 3, 5, 6], [2, 4, 7, 8, 9]],\n",
       " [97.735, 97.85333333333334, 98.01833333333332],\n",
       " [10405, 10730, 11141])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXP[0],ACC[0],U[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e18e7-f0fe-4dd9-880e-ad07dd25034e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86a29f26-8674-4646-a74f-dca82c8927d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['0', '2', '4', '5', '6', '7', '8', '9'], {'0': 0, '2': 1, '4': 2, '5': 3, '6': 4, '7': 5, '8': 6, '9': 7})\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "L2=[1,3]\n",
    "test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "print(test_IF0.find_classes(\"./data/SplitMnist/test/\"))\n",
    "Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=0, shuffle=True)\n",
    "print(len(Ts_DL0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71a45626-814b-4822-94f7-ea4c64bc2ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cec127-29d7-4d8a-b935-5d0f1dbcb2e5",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "866a3313-7926-4966-a1b9-5dc28e4c46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n",
    "import os\n",
    "# Set PYTORCH_CUDA_ALLOC_CONF environment variable\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e960c79-58b9-4f1d-93df-cbccebb9a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-08 16:03:42.200833\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f29dcbf0-ae9e-4b7b-889a-c6a87fbc9a74",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'type' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;28mlen\u001b[39m(EXP),\u001b[38;5;28mlen\u001b[39m(ACC),\u001b[38;5;28mlen\u001b[39m(U)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'type' has no len()"
     ]
    }
   ],
   "source": [
    "len(Dataset),len(EXP),len(ACC),len(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0110456-99c7-4c92-b5df-2e44664aea62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d0110456-99c7-4c92-b5df-2e44664aea62",
    "outputId": "02799af4-014c-4dab-ec46-33d719329174",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 2024-05-08 16:03:47.857075\n",
      "i: 0 2024-05-08 16:08:37.815565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 5, 7, 8, 9] 8060\n",
      "[4, 6] 1940\n",
      "epoch 0 \t 2.89485857137858 \t batch:  0 iteration : 0 /2907 \t Learning rate : 0.00000001\n",
      "[0, 2, 3, 4, 5, 6, 7, 9] 7891\n",
      "[1, 8] 2109\n",
      "epoch 0 \t 2858.6101713720122 \t batch:  0 iteration : 2 /2907 \t Learning rate : 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/300 [18:09:12<?, ?it/s]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f2232b5f340>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayman/anaconda3/envs/P38C117/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f2232b5f340>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayman/anaconda3/envs/P38C117/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m test_IF0\u001b[38;5;241m=\u001b[39mClassSpecificImageFolder( root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/SplitMnist/test/\u001b[39m\u001b[38;5;124m\"\u001b[39m,dropped_classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m L2],transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([ transforms\u001b[38;5;241m.\u001b[39mToTensor(),transforms\u001b[38;5;241m.\u001b[39mGrayscale(\u001b[38;5;241m1\u001b[39m)]))\n\u001b[1;32m    129\u001b[0m Ts_DL0 \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtest_IF0, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 131\u001b[0m _, valid_epoch_acc0,_\u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTs_DL0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcriterion_CNN0\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(task3)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m    133\u001b[0m     valid_epoch_acc1\u001b[38;5;241m=\u001b[39mvalid_epoch_acc0\n",
      "Cell \u001b[0;32mIn[16], line 185\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, testloader, criterion, nb_classes)\u001b[0m\n\u001b[1;32m    183\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# calculate the loss\u001b[39;00m\n\u001b[1;32m    187\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/P38C117/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 115\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# forward prop through module_list\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list:\n\u001b[0;32m--> 115\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/P38C117/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/P38C117/lib/python3.8/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/P38C117/lib/python3.8/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/P38C117/lib/python3.8/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "results_path=\"./model MSE/\"\n",
    "\n",
    "track=0\n",
    "lr=0.01\n",
    "criterion = nn.MSELoss()\n",
    "LW=LWLN_loss()\n",
    "\n",
    "optimizer = Adam(mod.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-8, max_lr=0.005, step_size_up=256, mode=\"triangular2\", cycle_momentum=False)\n",
    "num_epochs=300\n",
    "nb_batches=200\n",
    "\n",
    "Cols=[\"label task 1\",\"index\",\"Accuracy task1\",\\\n",
    "      \"label task 2\",\"Accuracy task2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Loader Set\",\"Reconstructed Accuracy ID\",\"Actual Accuracy\",\"Reconstructed Accuracy OOD\",\"Transformer Loss\",\"lr\",'epochCNN','ActivationCNN'] \n",
    "\n",
    "#print(len(Cols))\n",
    "#predicted_Weights= pd.DataFrame(columns=Cols)\n",
    "\n",
    "torch.save({'epoch':-1,'model_state_dict': mod.state_dict(),'optimizer_state_dict': optimizer.state_dict(),},results_path+'AE epoch {}.pth'.format(-1))\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    L_train=[]\n",
    "    L_val=[]\n",
    "    L_test=[]\n",
    "    \n",
    "    start_time_epoch = time.time()\n",
    "    for i in range(nb_batches):\n",
    "        print(f\"i: {i}\",datetime.datetime.now())\n",
    "        cs_tr=CustomDataset(train_pair,i,nb_batches=nb_batches)\n",
    "        print(f\"i: {i}\",datetime.datetime.now())\n",
    "        batch_loss=torch.tensor(0.0, device='cpu')\n",
    "        nb_samples=0\n",
    "        \n",
    "        # def artifacts(self):\n",
    "        # return self.loaded ,self.exp[q],self.ACC,self.batch_indices\n",
    "        Dataset,EXP,ACC,U = cs_tr.artifacts\n",
    "        P=0\n",
    "        for p ,(x1,x2,tg)in enumerate(DataLoader(Dataset,batch_size=346, shuffle=False)):\n",
    "            \n",
    "            has_nan1 = torch.isnan(x1).any().item()\n",
    "            has_nan2 = torch.isnan(x2).any().item()\n",
    "            try:\n",
    "\n",
    "                #x1=x1.to(torch.float32)\n",
    "                #x2=x2.to(torch.float32)\n",
    "                #tg=tg.to(torch.float32)\n",
    "\n",
    "                x1=x1.to(device)\n",
    "                x2=x2.to(device)\n",
    "                tg=tg.to(device)\n",
    "                mod=mod.to(device)\n",
    "\n",
    "                mod.train()\n",
    "                optimizer.zero_grad()\n",
    "                output = mod(x1,x2)\n",
    "                \n",
    "                \n",
    "\n",
    "                for vect in range(len(x1)):\n",
    "                    \n",
    "                    y_pred=torch.unsqueeze(output[0][vect], 0) \n",
    "                    y =torch.unsqueeze(tg[vect], 0) \n",
    "                    \n",
    "                    loss_tr = 0.5*criterion(y_pred,y) +0.5*LW(y_pred,y)\n",
    "                    loss_tr = loss_tr.detach().cpu()\n",
    "                    \n",
    "                    batch_loss=loss_tr+batch_loss\n",
    "                    nb_samples=nb_samples+1\n",
    "                    \n",
    "                    selected_row = cs_tr.df.iloc[int(U[P+vect][0]), 11:17]  \n",
    "                    columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "                    activ=columns_with_one\n",
    "                    epochCNN=cs_tr.df.loc[int(U[P+vect][0])]['epoch']\n",
    "                    \n",
    "                    \n",
    "                    checkpoint=OrderedDict()\n",
    "                    vector_aux= output[0][vect].detach()\n",
    "                    y_pred=vector_aux.cpu()\n",
    "    \n",
    "                    task1=[int(x) for x in EXP[P+vect][0]]\n",
    "                    task2=[int(x) for x in EXP[P+vect][1]]\n",
    "                    task3=sorted(task1+task2)\n",
    "    \n",
    "    \n",
    "                    All=list(range(10))\n",
    "                    L2=[k for k in All if k not in task3] #Classes to test on (In distribution)\n",
    "                    L_others=[k for k in All if k not in task3] # Out of distribution classes\n",
    "    \n",
    "                    checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(y_pred[0:200]).reshape([8, 1, 5, 5]))\n",
    "                    checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(y_pred[200:208]).reshape([8]))\n",
    "    \n",
    "                    checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(y_pred[208:1408]).reshape([6, 8, 5, 5]))\n",
    "                    checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(y_pred[1408:1414]).reshape([6]))\n",
    "    \n",
    "                    checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(y_pred[1414:1510]).reshape([4, 6, 2, 2]))\n",
    "                    checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(y_pred[1510:1514]).reshape([4]))\n",
    "    \n",
    "                    checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(y_pred[1514:2234]).reshape([20,36]))\n",
    "                    checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(y_pred[2234:2254]).reshape([20]))\n",
    "    \n",
    "                    checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(y_pred[2254:2454]).reshape([10,20]))\n",
    "                    checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(y_pred[2454:2464]).reshape([10]))\n",
    "    \n",
    "                    Brain = CNN(1,activ[0],0,\"kaiming_uniform\")\n",
    "    \n",
    "                    model=copy.deepcopy(Brain)\n",
    "                    model.load_state_dict(checkpoint)\n",
    "    \n",
    "                    criterion_CNN0=CrossEntropyLoss()\n",
    "    \n",
    "                    test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                    Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=4, shuffle=True)\n",
    "                    \n",
    "                    _, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "                    if len(task3)>8:\n",
    "                        valid_epoch_acc1=valid_epoch_acc0\n",
    "                        continue\n",
    "                    else:\n",
    "                        criterion_CNN1=CrossEntropyLoss()\n",
    "                        test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                        Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=60, num_workers=5, shuffle=True)\n",
    "                        \n",
    "                        valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n",
    "    \n",
    "    \n",
    "                    lr = optimizer.param_groups[0][\"lr\"]\n",
    "                    if (P+vect)%1000==0:\n",
    "                        print(task3,len(Ts_DL0.dataset))\n",
    "                        print(L2,len(Ts_DL1.dataset))\n",
    "                        time.sleep(5)\n",
    "                        print(\"epoch\" ,epoch, \"\\t\", batch_loss.detach().cpu().item(),\"\\t batch: \",i,\"iteration :\",p,f\"/{len(Dataset)}\",f\"\\t Learning rate :\" ,f\"{lr:.8f}\")\n",
    "                    \n",
    "                    vectstring=f\"{y_pred.tolist()}\".replace(\" \", \"\")[1:-1].split(',')\n",
    "                    row_part=f\"{task1};{int(U[P+vect][0])};{float(ACC[P+vect][0])};{task2};{float(ACC[P+vect][1])}\".split(\";\")\n",
    "                    row_part2=f\"train,{valid_epoch_acc0},{float(ACC[P+vect][2])},{valid_epoch_acc1},{float(loss_tr.detach().cpu())},{lr},{epochCNN},{activ[0]}\".split(\",\")\n",
    "                    row=row_part+vectstring+row_part2\n",
    "                    \n",
    "                    \n",
    "                    temp_file = os.path.join(results_path,f'AE Tracking {track}.csv')\n",
    "                \n",
    "                        \n",
    "                    if (P+vect)%300==0:\n",
    "                        track += 1\n",
    "                        temp_file = os.path.join(results_path, f'AE Tracking {track}.csv')\n",
    "                        # Write the column headers\n",
    "                        with open(temp_file, mode='a', newline='\\n') as file:\n",
    "                            writer = csv.writer(file)\n",
    "                            writer.writerow(Cols)\n",
    "                        # Write to the CSV file\n",
    "                    with open(temp_file, mode='a', newline='\\n') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        # Write your row data\n",
    "                        writer.writerow(row)\n",
    "\n",
    "                P=P+len(x1)\n",
    "            except Exception as e:\n",
    "                # Print the exception\n",
    "                print(\"An exception occurred:\", e)\n",
    "                traceback.print_exc()\n",
    "                # Continue with the loop\n",
    "                continue\n",
    "        batch_loss=batch_loss/nb_samples\n",
    "        batch_loss=batch_loss.to(device)\n",
    "        batch_loss.requires_grad=True\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        end_time_epoch = time.time()\n",
    "        execution_time = end_time_epoch - start_time_epoch\n",
    "        print(\"Batch Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    end_time_epoch = time.time()\n",
    "    execution_time = end_time_epoch - start_time_epoch\n",
    "    print(\"Epoch Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "    csv_files = [file for file in os.listdir(results_path) if file.endswith(\".csv\")]\n",
    "    \n",
    "    csv_files.sort(key=lambda x: int(x.split()[2][:-4]))\n",
    "    df = pd.read_csv(os.path.join(results_path, \"AE Tracking 1.csv\"),header=0)\n",
    "    res= pd.DataFrame(columns=df.columns)\n",
    "    for i,file in enumerate(csv_files[1:]):\n",
    "        df = pd.read_csv(os.path.join(results_path, file), header=[0])\n",
    "        res = res.append(df, ignore_index=True)\n",
    "    print(\"epoch\",epoch,'\\n','worst_CNN_ACC \\t',  float(min(res[[\"Reconstructed Accuracy ID\"]].values)),'\\n','best_CNN_ACC \\t',float(max(res[[\"Reconstructed Accuracy ID\"]].values)),'\\n','max_Transformer_loss \\t',float(max(res[[\"Transformer Loss\"]].values)),'\\n','min_CNN_ACC \\t',float(min(res[[\"Transformer Loss\"]].values)))\n",
    "\n",
    "        \n",
    "\n",
    "    if epoch%20==0 or (epoch in [1,2,3,4,5,10]):\n",
    "        torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'Batch Loss':batch_loss.detach().cpu().item(),'last_batch_CNN_ACC':valid_epoch_acc0,'worst_CNN_ACC':  float(min(res[[\"Reconstructed Accuracy ID\"]].values)),'best_CNN_ACC':float(max(res[[\"Reconstructed Accuracy ID\"]].values)),'max_Transformer_loss':float(max(res[[\"Transformer Loss\"]].values)),'min_CNN_ACC':float(min(res[[\"Transformer Loss\"]].values))},results_path+'AE epoch {}.pth'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11956ef2-7a65-4c5f-8ec9-3c73650dd32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3adde5d5-3b13-4da4-949b-9753dd2aaa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39, 2464])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e92e89c-aa40-4d37-9ad2-7d6b0d8d398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 2464]) torch.Size([600, 2464]) torch.Size([600, 2464])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([600, 2464]), 600, torch.Size([200]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x1.shape,x2.shape,tg.shape)\n",
    "vector_aux.shape, len(x1) , vector_aux[p,0:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21eca4-df55-4952-a7be-1db513ff76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tg=torch.unsqueeze(tg, 0)\n",
    "loss_tr = 0.2 * criterion(output[0],tg) +0.8 *LW(output[0],tg)\n",
    "loss_tr = loss_tr.detach().cpu()\n",
    "batch_loss=loss_tr+batch_loss\n",
    "nb_samples=nb_samples+len(x1)\n",
    "\n",
    "selected_row = cs_tr.df.iloc[int(U[p][0]), 11:17]  \n",
    "columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "activ=columns_with_one\n",
    "epochCNN=cs_tr.df.loc[int(U[p][0])]['epoch']\n",
    "\n",
    "\n",
    "checkpoint=OrderedDict()\n",
    "vector_aux= output[0].detach()\n",
    "vector_aux=vector_aux.cpu()\n",
    "\n",
    "task1=[int(x) for x in EXP[p][0]]\n",
    "task2=[int(x) for x in EXP[p][1]]\n",
    "task3=sorted(task1+task2)\n",
    "\n",
    "\n",
    "All=list(range(10))\n",
    "L2=[k for k in All if k not in task3] #Classes to test on (In distribution)\n",
    "L_others=[k for k in All if k not in task3] # Out of distribution classes\n",
    "\n",
    "checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(vector_aux[:,0:200]).reshape([8, 1, 5, 5]))\n",
    "checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(vector_aux[:,200:208]).reshape([8]))\n",
    "\n",
    "checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(vector_aux[:,208:1408]).reshape([6, 8, 5, 5]))\n",
    "checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(vector_aux[:,1408:1414]).reshape([6]))\n",
    "\n",
    "checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(vector_aux[:,1414:1510]).reshape([4, 6, 2, 2]))\n",
    "checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(vector_aux[:,1510:1514]).reshape([4]))\n",
    "\n",
    "checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(vector_aux[:,1514:2234]).reshape([20,36]))\n",
    "checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(vector_aux[:,2234:2254]).reshape([20]))\n",
    "\n",
    "checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(vector_aux[:,2254:2454]).reshape([10,20]))\n",
    "checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(vector_aux[:,2454:2464]).reshape([10]))\n",
    "\n",
    "Brain = CNN(1,activ[0],0,\"kaiming_uniform\")\n",
    "\n",
    "model=copy.deepcopy(Brain)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "criterion_CNN0=CrossEntropyLoss()\n",
    "\n",
    "test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=L2,transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=90, num_workers=0, shuffle=True)\n",
    "_, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "\n",
    "criterion_CNN1=CrossEntropyLoss()\n",
    "test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=task3,transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=90, num_workers=0, shuffle=True)\n",
    "valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1a724-248f-4c83-98b8-13e7dccda916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df29ecd-9235-45f4-951a-ecfe9cc1a2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616cff7-b29e-4984-9dc5-f8b13aec2643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73672723-0e7f-42b2-8c86-1dad689519f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633140e-1262-4495-b136-dceb763907f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a3a71a5-4cf0-42db-9886-e0099d6f5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'Mixed Loss':batch_loss.detach().cpu().item(),'CNN_ACC':valid_epoch_acc0},'./model/AE epoch {}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93aa579e-d4ab-49f9-90a0-cd54ad405921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(47223.6875, requires_grad=True), 21868)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss.requires_grad=True\n",
    "batch_loss,nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb3bfa8-06ae-4660-a448-352089868053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfa23f52-93fa-410f-918e-d9da64b9145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           EXP: 488.2 KiB\n",
      "                           ACC: 488.2 KiB\n",
      "                             U: 488.2 KiB\n",
      "                     Pairs_exp: 190.1 KiB\n",
      "                    train_pair: 133.4 KiB\n",
      "                     train_tgt: 133.4 KiB\n",
      "                     test_pair: 36.3 KiB\n",
      "                      test_tgt: 36.3 KiB\n",
      "                    vectstring: 20.1 KiB\n",
      "                      val_pair: 20.1 KiB\n"
     ]
    }
   ],
   "source": [
    "#https://discuss.pytorch.org/t/memory-management-using-pytorch-cuda-alloc-conf/157850\n",
    "#https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-can-i-set-max-split-size-mb\n",
    "\n",
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n",
    "                          locals().items())), key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f683513b-3230-405d-8eeb-6af177013302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 6, 7, 8, 9], [0, 2, 3, 5, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[0],task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c605d46-e9d9-40b6-9c4b-2d78b8393a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label task 1</th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy task1</th>\n",
       "      <th>label task 2</th>\n",
       "      <th>Accuracy task2</th>\n",
       "      <th>weight 0</th>\n",
       "      <th>weight 1</th>\n",
       "      <th>weight 2</th>\n",
       "      <th>weight 3</th>\n",
       "      <th>weight 4</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Loader Set</th>\n",
       "      <th>Reconstructed Accuracy ID</th>\n",
       "      <th>Actual Accuracy</th>\n",
       "      <th>Reconstructed Accuracy OOD</th>\n",
       "      <th>Transformer Loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochCNN</th>\n",
       "      <th>ActivationCNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>15266</td>\n",
       "      <td>98.333333</td>\n",
       "      <td></td>\n",
       "      <td>98.093333</td>\n",
       "      <td>1.693639</td>\n",
       "      <td>-0.393555</td>\n",
       "      <td>0.068379</td>\n",
       "      <td>0.583934</td>\n",
       "      <td>-1.534439</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.608331</td>\n",
       "      <td>0.621260</td>\n",
       "      <td>train</td>\n",
       "      <td>8.38</td>\n",
       "      <td>98.250000</td>\n",
       "      <td>8.38</td>\n",
       "      <td>2.000025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>15266</td>\n",
       "      <td>98.333333</td>\n",
       "      <td></td>\n",
       "      <td>97.883333</td>\n",
       "      <td>2.080894</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>0.173723</td>\n",
       "      <td>0.623434</td>\n",
       "      <td>-1.614910</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.388891</td>\n",
       "      <td>0.539818</td>\n",
       "      <td>train</td>\n",
       "      <td>7.79</td>\n",
       "      <td>97.948333</td>\n",
       "      <td>7.79</td>\n",
       "      <td>1.986340</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>15266</td>\n",
       "      <td>98.333333</td>\n",
       "      <td></td>\n",
       "      <td>98.410000</td>\n",
       "      <td>2.049980</td>\n",
       "      <td>-0.242580</td>\n",
       "      <td>0.318586</td>\n",
       "      <td>0.646433</td>\n",
       "      <td>-1.770652</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.287083</td>\n",
       "      <td>0.398330</td>\n",
       "      <td>train</td>\n",
       "      <td>9.31</td>\n",
       "      <td>98.070000</td>\n",
       "      <td>9.31</td>\n",
       "      <td>2.011771</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>15266</td>\n",
       "      <td>98.333333</td>\n",
       "      <td></td>\n",
       "      <td>98.286667</td>\n",
       "      <td>1.815176</td>\n",
       "      <td>-0.604665</td>\n",
       "      <td>-0.257339</td>\n",
       "      <td>0.312119</td>\n",
       "      <td>-1.143257</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.217142</td>\n",
       "      <td>0.706113</td>\n",
       "      <td>train</td>\n",
       "      <td>9.93</td>\n",
       "      <td>97.825000</td>\n",
       "      <td>9.93</td>\n",
       "      <td>2.031300</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>15266</td>\n",
       "      <td>98.333333</td>\n",
       "      <td></td>\n",
       "      <td>98.236667</td>\n",
       "      <td>1.697706</td>\n",
       "      <td>-0.546888</td>\n",
       "      <td>0.326315</td>\n",
       "      <td>0.452815</td>\n",
       "      <td>-1.507497</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.418500</td>\n",
       "      <td>0.340476</td>\n",
       "      <td>train</td>\n",
       "      <td>10.97</td>\n",
       "      <td>98.133333</td>\n",
       "      <td>10.97</td>\n",
       "      <td>2.037839</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td></td>\n",
       "      <td>15221</td>\n",
       "      <td>97.201667</td>\n",
       "      <td></td>\n",
       "      <td>98.041667</td>\n",
       "      <td>1.963520</td>\n",
       "      <td>-0.738335</td>\n",
       "      <td>-0.012039</td>\n",
       "      <td>0.313362</td>\n",
       "      <td>-1.397939</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.352349</td>\n",
       "      <td>0.799487</td>\n",
       "      <td>train</td>\n",
       "      <td>9.55</td>\n",
       "      <td>98.003333</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.002596</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td></td>\n",
       "      <td>20039</td>\n",
       "      <td>97.770000</td>\n",
       "      <td></td>\n",
       "      <td>97.316667</td>\n",
       "      <td>1.746097</td>\n",
       "      <td>-0.815134</td>\n",
       "      <td>-0.120588</td>\n",
       "      <td>0.589807</td>\n",
       "      <td>-1.498475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.946741</td>\n",
       "      <td>-0.098292</td>\n",
       "      <td>train</td>\n",
       "      <td>9.70</td>\n",
       "      <td>97.821667</td>\n",
       "      <td>9.70</td>\n",
       "      <td>1.936134</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td></td>\n",
       "      <td>20039</td>\n",
       "      <td>97.770000</td>\n",
       "      <td></td>\n",
       "      <td>97.650000</td>\n",
       "      <td>1.846137</td>\n",
       "      <td>-0.439110</td>\n",
       "      <td>0.093636</td>\n",
       "      <td>0.326424</td>\n",
       "      <td>-1.411710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.701774</td>\n",
       "      <td>0.514087</td>\n",
       "      <td>train</td>\n",
       "      <td>8.21</td>\n",
       "      <td>96.220000</td>\n",
       "      <td>8.21</td>\n",
       "      <td>1.968303</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td></td>\n",
       "      <td>20039</td>\n",
       "      <td>97.770000</td>\n",
       "      <td></td>\n",
       "      <td>97.646667</td>\n",
       "      <td>1.797259</td>\n",
       "      <td>-0.159525</td>\n",
       "      <td>-0.572254</td>\n",
       "      <td>0.156177</td>\n",
       "      <td>-1.160839</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.177182</td>\n",
       "      <td>0.417260</td>\n",
       "      <td>train</td>\n",
       "      <td>8.78</td>\n",
       "      <td>97.841667</td>\n",
       "      <td>8.78</td>\n",
       "      <td>1.933440</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td></td>\n",
       "      <td>438</td>\n",
       "      <td>98.756667</td>\n",
       "      <td></td>\n",
       "      <td>97.725000</td>\n",
       "      <td>1.757277</td>\n",
       "      <td>-0.962532</td>\n",
       "      <td>-0.738005</td>\n",
       "      <td>0.541748</td>\n",
       "      <td>-1.378104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.849312</td>\n",
       "      <td>0.724254</td>\n",
       "      <td>train</td>\n",
       "      <td>9.13</td>\n",
       "      <td>98.883333</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.953976</td>\n",
       "      <td>0.005</td>\n",
       "      <td>16</td>\n",
       "      <td>leakyrelu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 2477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label task 1  index  Accuracy task1 label task 2  Accuracy task2  \\\n",
       "0                 15266       98.333333                    98.093333   \n",
       "1                 15266       98.333333                    97.883333   \n",
       "2                 15266       98.333333                    98.410000   \n",
       "3                 15266       98.333333                    98.286667   \n",
       "4                 15266       98.333333                    98.236667   \n",
       "..           ...    ...             ...          ...             ...   \n",
       "115               15221       97.201667                    98.041667   \n",
       "116               20039       97.770000                    97.316667   \n",
       "117               20039       97.770000                    97.650000   \n",
       "118               20039       97.770000                    97.646667   \n",
       "119                 438       98.756667                    97.725000   \n",
       "\n",
       "     weight 0  weight 1  weight 2  weight 3  weight 4  ...  bias 2462  \\\n",
       "0    1.693639 -0.393555  0.068379  0.583934 -1.534439  ...  -1.608331   \n",
       "1    2.080894 -0.042077  0.173723  0.623434 -1.614910  ...  -1.388891   \n",
       "2    2.049980 -0.242580  0.318586  0.646433 -1.770652  ...  -1.287083   \n",
       "3    1.815176 -0.604665 -0.257339  0.312119 -1.143257  ...  -1.217142   \n",
       "4    1.697706 -0.546888  0.326315  0.452815 -1.507497  ...  -1.418500   \n",
       "..        ...       ...       ...       ...       ...  ...        ...   \n",
       "115  1.963520 -0.738335 -0.012039  0.313362 -1.397939  ...  -1.352349   \n",
       "116  1.746097 -0.815134 -0.120588  0.589807 -1.498475  ...  -0.946741   \n",
       "117  1.846137 -0.439110  0.093636  0.326424 -1.411710  ...  -0.701774   \n",
       "118  1.797259 -0.159525 -0.572254  0.156177 -1.160839  ...  -1.177182   \n",
       "119  1.757277 -0.962532 -0.738005  0.541748 -1.378104  ...  -0.849312   \n",
       "\n",
       "     bias 2463  Loader Set  Reconstructed Accuracy ID  Actual Accuracy  \\\n",
       "0     0.621260       train                       8.38        98.250000   \n",
       "1     0.539818       train                       7.79        97.948333   \n",
       "2     0.398330       train                       9.31        98.070000   \n",
       "3     0.706113       train                       9.93        97.825000   \n",
       "4     0.340476       train                      10.97        98.133333   \n",
       "..         ...         ...                        ...              ...   \n",
       "115   0.799487       train                       9.55        98.003333   \n",
       "116  -0.098292       train                       9.70        97.821667   \n",
       "117   0.514087       train                       8.21        96.220000   \n",
       "118   0.417260       train                       8.78        97.841667   \n",
       "119   0.724254       train                       9.13        98.883333   \n",
       "\n",
       "     Reconstructed Accuracy OOD  Transformer Loss     lr  epochCNN  \\\n",
       "0                          8.38          2.000025  0.005        16   \n",
       "1                          7.79          1.986340  0.005        16   \n",
       "2                          9.31          2.011771  0.005        16   \n",
       "3                          9.93          2.031300  0.005        16   \n",
       "4                         10.97          2.037839  0.005        16   \n",
       "..                          ...               ...    ...       ...   \n",
       "115                        9.55          2.002596  0.005        16   \n",
       "116                        9.70          1.936134  0.005        16   \n",
       "117                        8.21          1.968303  0.005        16   \n",
       "118                        8.78          1.933440  0.005        16   \n",
       "119                        9.13          1.953976  0.005        16   \n",
       "\n",
       "     ActivationCNN  \n",
       "0        leakyrelu  \n",
       "1        leakyrelu  \n",
       "2        leakyrelu  \n",
       "3        leakyrelu  \n",
       "4        leakyrelu  \n",
       "..             ...  \n",
       "115      leakyrelu  \n",
       "116      leakyrelu  \n",
       "117      leakyrelu  \n",
       "118      leakyrelu  \n",
       "119      leakyrelu  \n",
       "\n",
       "[120 rows x 2477 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11dea1c-4ef5-4e59-a650-3c2d100ab6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # if o%5==0 and p<5:\n",
    "        #     print(\"\\t\",[int(x) for x in labels[0]],\"\\t\",[int(x) for x in labels[1]],f\"\\t {loss_tr.item():.5f}\" ,\"\\t Epoch \",epoch,\"/300\",path[-11:-4],activ)\n",
    "        # L_train.append(loss_tr.item())\n",
    "        \n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.cuda()\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        if i%5==0:\n",
    "            print(\"\\t\",state['step'] , f\"\\t Learning rate :\" ,f\"{lr:.8f}\",\"\\t min loss in batch: \", f\"{min(L_train[-120:]):.7f}\" , \"\\t avg loss in batch\" ,f\"{sum(L_train[-120:])/len(L_train[-120:]):.7f}\" )\n",
    "\n",
    "            for muk in range(len(test_pair)//80):\n",
    "\n",
    "                start_index = muk * 80\n",
    "                end_index = min((muk + 1) * 80, len(test_pair))\n",
    "                print(start_index, end_index)\n",
    "                cs_ts=CustomDataset(test_pair,start_index,end_index,path,activ)\n",
    "                for x1,x2,tg,labels in DataLoader(cs_ts):\n",
    "                    x1=x1.to(torch.float32)\n",
    "                    x2=x2.to(torch.float32)\n",
    "                    tg=tg.to(torch.float32)\n",
    "                    \n",
    "                    x1=x1.to(device)\n",
    "                    x2=x2.to(device)\n",
    "                    tg=tg.to(device)\n",
    "                    mod=mod.to(device)\n",
    "                    \n",
    "                    mod.eval()\n",
    "                    output = mod(x1,x2)\n",
    "                    loss_ts = criterion(output[0],tg)\n",
    "                    print(\"Test--\",[int(x) for x in labels[0]],[int(x) for x in labels[1]],f\"{loss_ts.item():.5f}\" ,\"Epoch \",epoch,\"/300\",path,activ)\n",
    "                    L_test.append(loss_ts.item())\n",
    "            for muk in range(len(val_pair)//80):\n",
    "                start_index = muk * 80\n",
    "                end_index = min((muk + 1) * 80, len(val_pair))\n",
    "                cs_val=CustomDataset(val_pair,start_index ,end_index,path,activ)\n",
    "\n",
    "                for x1,x2,tg,labels in DataLoader(cs_val):\n",
    "                    x1=x1.to(torch.float32)\n",
    "                    x2=x2.to(torch.float32)\n",
    "                    tg=tg.to(torch.float32)\n",
    "                    \n",
    "                    x1=x1.to(device)\n",
    "                    x2=x2.to(device)\n",
    "                    tg=tg.to(device)\n",
    "                    mod=mod.to(device)\n",
    "                    \n",
    "                    mod.eval()\n",
    "                    optimizer.zero_grad()\n",
    "                    output = mod(x1,x2)\n",
    "                    loss_val = criterion(output[0],tg)\n",
    "                    print(\"Validation--\",[int(x) for x in labels[0]],[int(x) for x in labels[1]],\"MAE \",f\"{loss_val.item():.5f}\",\"Epoch \",epoch,\"/300\",path,activ)\n",
    "                    L_val.append(loss_val.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "        np.save(f'Train epoch{epoch}.npy',np.array(L_train))\n",
    "        np.save(f'Val epoch{epoch}.npy', np.array(L_val))\n",
    "        np.save(f'Test epoch{epoch}.npy', np.array(L_test))\n",
    "        Lt_epoch.append(np.array(L_train).mean())\n",
    "        Ls_epoch.append(np.array(L_test).mean())\n",
    "        Lv_epoch.append(np.array(L_val).mean())\n",
    "        if epoch%2==0:\n",
    "            torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),'optimizer_state_dict': optimizer.state_dict(),},'./model/AE epoch {} {}.pth'.format(epoch,path[-11:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4d35f-81a3-478e-a429-b7900929fde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d81995-b013-471d-a00d-b7f4eb8442e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab90e5d-6ba4-44ad-9bd0-30f04bae0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to your checkpoint\n",
    "checkpoint_path = \".//model//AE epoch 0 epoch36.pth\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path,pickle_module=dill,map_location=device)\n",
    "\n",
    "# Create your model instance\n",
    "model = TransformerAE(max_seq_len=176,\n",
    "                    N=4,\n",
    "                    heads=4,\n",
    "                    d_model=700,\n",
    "                    d_ff=700,\n",
    "                    neck=200,\n",
    "                    dropout=0.1\n",
    "                   )\n",
    "\n",
    "# Load the model state_dict\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load other components as needed\n",
    "# For example, optimizer state_dict, epoch, loss, etc.\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#len(test_pair),len(train_pair),len(val_pair)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    if epoch==0:\n",
    "        L_train=[]\n",
    "        L_val=[]\n",
    "        L_test=[]\n",
    "        for path in csv_files:\n",
    "\n",
    "            for activ in L_activations:\n",
    "                for muk in range(len(test_pair)//800):\n",
    "\n",
    "                    start_index = muk * 800\n",
    "                    end_index = min((muk + 1) * 800, len(test_pair))\n",
    "                    print(start_index, end_index)\n",
    "                    cs_ts=CustomDataset(test_pair,start_index,end_index,path,activ)\n",
    "                    for x1,x2,tg,labels in DataLoader(cs_ts):\n",
    "                        x1=x1.to(torch.float32)\n",
    "                        x2=x2.to(torch.float32)\n",
    "                        tg=tg.to(torch.float32)\n",
    "\n",
    "                        x1=x1.to(device)\n",
    "                        x2=x2.to(device)\n",
    "                        tg=tg.to(device)\n",
    "                        mod=mod.to(device)\n",
    "\n",
    "                        mod.eval()\n",
    "                        output = mod(x1,x2)\n",
    "                        loss_ts = criterion(output[0],tg)\n",
    "\n",
    "                        L_test.append(loss_ts.item())\n",
    "                    print(\"Test--\",[int(x) for x in labels[0]],[int(x) for x in labels[1]],f\"{loss_ts.item():.5f}\" ,sum(L_test[-800:])/len(L_test[-800:]))\n",
    "                for muk in range(len(val_pair)//800):\n",
    "                    start_index = muk * 800\n",
    "                    end_index = min((muk + 1) * 800, len(val_pair))\n",
    "                    cs_val=CustomDataset(val_pair,start_index ,end_index,path,activ)\n",
    "\n",
    "                    for x1,x2,tg,labels in DataLoader(cs_val):\n",
    "                        x1=x1.to(torch.float32)\n",
    "                        x2=x2.to(torch.float32)\n",
    "                        tg=tg.to(torch.float32)\n",
    "\n",
    "                        x1=x1.to(device)\n",
    "                        x2=x2.to(device)\n",
    "                        tg=tg.to(device)\n",
    "                        mod=mod.to(device)\n",
    "\n",
    "                        mod.eval()\n",
    "                        optimizer.zero_grad()\n",
    "                        output = mod(x1,x2)\n",
    "                        loss_val = criterion(output[0],tg)\n",
    "                        L_val.append(loss_val.item())\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf05334-85d1-444c-9ed1-01cc6aaa1921",
   "metadata": {
    "id": "fdf05334-85d1-444c-9ed1-01cc6aaa1921"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c6f484-af6d-422c-970e-c9d19800ede0",
   "metadata": {
    "id": "b9c6f484-af6d-422c-970e-c9d19800ede0"
   },
   "source": [
    "# reconstruct Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81562e-069c-4d1b-be73-05b95149994c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80087ab3-c319-4285-baf7-27063468b639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7b764-981e-4faf-81d0-7f5592b555b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape ,activ,task3,\"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7981adb-33bb-4b75-a8ab-e2180dacea14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630fb9b-bf05-43f9-95d7-323a211e4fb9",
   "metadata": {
    "id": "c630fb9b-bf05-43f9-95d7-323a211e4fb9"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint=OrderedDict()\n",
    "vector_aux= output[0].detach()\n",
    "vector_aux=vector_aux.cpu()\n",
    "\n",
    "task1=[int(x) for x in labels[0]]\n",
    "task2=[int(x) for x in labels[1]]\n",
    "task3=sorted(task1+task2)\n",
    "\n",
    "\n",
    "All=list(range(10))\n",
    "L2=[k for k in All if k not in task3]\n",
    "L_others=[k for k in All if k not in L2]\n",
    "\n",
    "checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(vector_aux[:,0:200]).reshape([8, 1, 5, 5]))\n",
    "checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(vector_aux[:,200:208]).reshape([8]))\n",
    "\n",
    "checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(vector_aux[:,208:1408]).reshape([6, 8, 5, 5]))\n",
    "checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(vector_aux[:,1408:1414]).reshape([6]))\n",
    "\n",
    "checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(vector_aux[:,1414:1510]).reshape([4, 6, 2, 2]))\n",
    "checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(vector_aux[:,1510:1514]).reshape([4]))\n",
    "\n",
    "checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(vector_aux[:,1514:2234]).reshape([20,36]))\n",
    "checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(vector_aux[:,2234:2254]).reshape([20]))\n",
    "\n",
    "checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(vector_aux[:,2254:2454]).reshape([10,20]))\n",
    "checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(vector_aux[:,2454:2464]).reshape([10]))\n",
    "\n",
    "model = CNN(1,activ,0,\"kaiming_uniform\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "criterion_CNN=CrossEntropyLoss()\n",
    "\n",
    "test_IF=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=L2,transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL = DataLoader(dataset=test_IF, batch_size=90, num_workers=0, shuffle=True)\n",
    "valid_epoch_loss0, valid_epoch_acc0,L_mx= validate(model, Ts_DL,  criterion_CNN,10)\n",
    "print(valid_epoch_loss0, valid_epoch_acc0)\n",
    "\n",
    "test_IF=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=L_others,transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL = DataLoader(dataset=test_IF, batch_size=90, num_workers=0, shuffle=True)\n",
    "valid_epoch_loss0, valid_epoch_acc0,L_mx= validate(model, Ts_DL,  criterion_CNN,10)\n",
    "print(valid_epoch_loss0, valid_epoch_acc0)\n",
    "\n",
    "#if not(os.path.isdir('./checkpoints/')):\n",
    "#    os.mkdir('./checkpoints/')\n",
    "#torch.save(model.state_dict(), './checkpoints/Cumulative/{}/{}/{}/1/Reconsturcted checkpoint.pth'.format(ListExperiences[class_order_idx][:5],list(i)[0],list(i)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb50f6-1c94-4b90-a2fa-3fa495285493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ad9dc-d6f9-48ef-9b04-da0541e6d458",
   "metadata": {
    "id": "c58ad9dc-d6f9-48ef-9b04-da0541e6d458"
   },
   "outputs": [],
   "source": [
    "vector_aux[0:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e7aae-ae9f-427c-b0de-c97dccf5a42b",
   "metadata": {
    "id": "7f8e7aae-ae9f-427c-b0de-c97dccf5a42b"
   },
   "outputs": [],
   "source": [
    "#per epoch:\n",
    "    #Checkpoint AE\n",
    "    #dataframe of weights + respective performance\n",
    "    #dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8770aa-e5e1-40a5-94bb-25d9adf77af5",
   "metadata": {
    "id": "4e8770aa-e5e1-40a5-94bb-25d9adf77af5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1f0d1-0159-4154-8ad9-c110697e971b",
   "metadata": {
    "id": "44b1f0d1-0159-4154-8ad9-c110697e971b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30512354-7922-4443-bd1c-e7bab2a72ac0",
   "metadata": {
    "id": "30512354-7922-4443-bd1c-e7bab2a72ac0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf755c23-ef0c-48d6-9fbf-e8ed8862f35f",
   "metadata": {
    "id": "bf755c23-ef0c-48d6-9fbf-e8ed8862f35f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c386ba-8d07-4db9-b682-98c6949e37d8",
   "metadata": {
    "id": "e1c386ba-8d07-4db9-b682-98c6949e37d8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ec2f5-0222-48bc-8011-01542cd27baa",
   "metadata": {
    "id": "208ec2f5-0222-48bc-8011-01542cd27baa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d6e0b-b674-4c11-883c-b077e17b3d87",
   "metadata": {
    "id": "be3d6e0b-b674-4c11-883c-b077e17b3d87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca690d7-667d-44d6-8a25-1aba073df4b2",
   "metadata": {
    "id": "4ca690d7-667d-44d6-8a25-1aba073df4b2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4caf193-9fff-49c3-8fd9-309fbfd3f0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddeded-fc34-4edb-8d82-0797efdc1b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c3f75-a2ce-4257-b992-45f33293ec9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0984f7-48ed-4462-9cc8-273668c5a4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbe062-9220-40bc-b8f5-d0d5fe162f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb28fc4-22de-4f3b-860a-2f19167da1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8d57d-55f2-4def-ac5e-b12a98dc1ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5bf33-2638-446b-8204-c8683d156a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716280ed-6d86-4a3b-b10a-720b73da5232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888d77a-4164-4f0d-ab72-1221ec5a9638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a53cf6-02fb-494b-9134-83948b749250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c290b-c556-4972-9c25-48ac1c62a922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10925c-4a7f-4d04-b0c9-692f75f85949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec0bdb-014f-49d6-9de3-66e7ba65c46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda3d30-fd75-4680-8fa0-a1e3e2641d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409794f-b60a-4fb6-a02f-801cdd486ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52937f3a-c64d-492c-8711-779ced48b4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297bb0e-f9f2-40b4-93db-976e0f295a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1043382-3f7a-4759-b297-0d2ac33f9523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d4b97-9ebc-4853-9a48-077b802a9eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112a640-c9aa-4469-ae01-597fe1fa9cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395ebbb-7eae-4d4f-b71a-78d7601b5161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac117a5d-9c3d-4881-adb5-3a8a590be114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bdb67-400a-4dc4-9c02-39c737df2cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1ff9b-e7a3-425e-a90c-41fc3ba12e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b770c1-61c7-4c82-9eca-911b59df8f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bba8a6-580a-4b21-9cd2-90f62f9a66f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b614d-5146-46c5-8393-247daade63f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5cabf-691c-4a2e-878e-d67ac1fda23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024e11e-c636-4c8c-ad23-ddbfd0f6840e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13318076-07b8-4cc9-9161-64e3dd77f5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de99196-3531-4461-a24e-f571dbddb797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31fb41-0fa9-4840-aa42-ddf8129e20bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb368d60-ebbf-41b6-9c76-3df12b63d59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980173a1-2b86-4054-b452-1e6aa23c02e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ce906-315e-4bbf-ac0f-1dcb68b1143e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a127c8-0072-4f07-9fa6-c8a260fcc2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9af7f-1b50-4590-9d4d-add4d9102cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb8473-ef7d-4145-8aba-5577e2e0e22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d55b9-c1ee-4445-87c1-43d1630ee4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acd0cb-104a-4696-a74a-257093035566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59939d-48d2-4ae1-8f6a-84a5fe07b112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f4eb0-a393-4e7a-8bcb-aff97ec4bf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac5353-8531-446d-aa2f-e84bffafe1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adbe1b8-17ba-4a26-9879-9a362843e9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70c6f7-35ca-40a3-b6fc-cdf79007f8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4731b71-d500-405f-8d0a-b2d9d16a6421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975e80d-0aaa-47e6-aded-a8c7eadb8959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbce7e-98b4-4658-a767-99a7c3bd35bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba4be3-6994-4589-8113-97ccaf5d4629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703f28f-624a-41fb-a360-653b6680df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b9212-5bef-4c7d-a757-a35a2e544e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae331862-c512-49ef-b486-3832d41bb46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758d763-9dcf-4f4e-ae4e-c41573c2d486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce752a5d-11b6-4677-8827-3e734cc5ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae3099-64eb-4e94-b5c1-83cb6c1ec048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ec93d-1f46-4af6-b75e-ff42ae938334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb427b27-edf0-4cfa-a5ac-d5d7c130f732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfc685-30ea-4464-8e5f-bcef74acaf34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
