{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00831a0d-1a6c-4d30-bd87-ec7e3f50fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 0 [0/60000(0%)]\t Loss: 544.012024\n",
      "Train epoch: 0 [1280/60000(2%)]\t Loss: 533.547913\n",
      "Train epoch: 0 [2560/60000(4%)]\t Loss: 518.113708\n",
      "Train epoch: 0 [3840/60000(6%)]\t Loss: 491.408417\n",
      "Train epoch: 0 [5120/60000(9%)]\t Loss: 453.439240\n",
      "Train epoch: 0 [6400/60000(11%)]\t Loss: 411.114532\n",
      "Train epoch: 0 [7680/60000(13%)]\t Loss: 356.117096\n",
      "Train epoch: 0 [8960/60000(15%)]\t Loss: 313.968628\n",
      "Train epoch: 0 [10240/60000(17%)]\t Loss: 285.281555\n",
      "Train epoch: 0 [11520/60000(19%)]\t Loss: 260.408936\n",
      "Train epoch: 0 [12800/60000(21%)]\t Loss: 247.898102\n",
      "Train epoch: 0 [14080/60000(23%)]\t Loss: 237.535690\n",
      "Train epoch: 0 [15360/60000(26%)]\t Loss: 234.287872\n",
      "Train epoch: 0 [16640/60000(28%)]\t Loss: 234.198395\n",
      "Train epoch: 0 [17920/60000(30%)]\t Loss: 229.211929\n",
      "Train epoch: 0 [19200/60000(32%)]\t Loss: 222.824326\n",
      "Train epoch: 0 [20480/60000(34%)]\t Loss: 221.319931\n",
      "Train epoch: 0 [21760/60000(36%)]\t Loss: 220.680130\n",
      "Train epoch: 0 [23040/60000(38%)]\t Loss: 212.756989\n",
      "Train epoch: 0 [24320/60000(41%)]\t Loss: 218.504715\n",
      "Train epoch: 0 [25600/60000(43%)]\t Loss: 208.050354\n",
      "Train epoch: 0 [26880/60000(45%)]\t Loss: 205.002991\n",
      "Train epoch: 0 [28160/60000(47%)]\t Loss: 203.133804\n",
      "Train epoch: 0 [29440/60000(49%)]\t Loss: 203.879593\n",
      "Train epoch: 0 [30720/60000(51%)]\t Loss: 207.656845\n",
      "Train epoch: 0 [32000/60000(53%)]\t Loss: 197.401459\n",
      "Train epoch: 0 [33280/60000(55%)]\t Loss: 196.118866\n",
      "Train epoch: 0 [34560/60000(58%)]\t Loss: 193.236694\n",
      "Train epoch: 0 [35840/60000(60%)]\t Loss: 187.689880\n",
      "Train epoch: 0 [37120/60000(62%)]\t Loss: 190.318436\n",
      "Train epoch: 0 [38400/60000(64%)]\t Loss: 189.457840\n",
      "Train epoch: 0 [39680/60000(66%)]\t Loss: 190.187469\n",
      "Train epoch: 0 [40960/60000(68%)]\t Loss: 182.984863\n",
      "Train epoch: 0 [42240/60000(70%)]\t Loss: 185.120621\n",
      "Train epoch: 0 [43520/60000(72%)]\t Loss: 177.622910\n",
      "Train epoch: 0 [44800/60000(75%)]\t Loss: 186.258987\n",
      "Train epoch: 0 [46080/60000(77%)]\t Loss: 176.940521\n",
      "Train epoch: 0 [47360/60000(79%)]\t Loss: 176.630829\n",
      "Train epoch: 0 [48640/60000(81%)]\t Loss: 176.156448\n",
      "Train epoch: 0 [49920/60000(83%)]\t Loss: 175.037613\n",
      "Train epoch: 0 [51200/60000(85%)]\t Loss: 170.929443\n",
      "Train epoch: 0 [52480/60000(87%)]\t Loss: 172.884949\n",
      "Train epoch: 0 [53760/60000(90%)]\t Loss: 171.644684\n",
      "Train epoch: 0 [55040/60000(92%)]\t Loss: 168.477646\n",
      "Train epoch: 0 [56320/60000(94%)]\t Loss: 170.812729\n",
      "Train epoch: 0 [57600/60000(96%)]\t Loss: 160.425476\n",
      "Train epoch: 0 [58880/60000(98%)]\t Loss: 157.986557\n",
      "====> Epoch: 0 Average loss: 239.4672\n",
      "Train epoch: 1 [0/60000(0%)]\t Loss: 160.937500\n",
      "Train epoch: 1 [1280/60000(2%)]\t Loss: 156.990311\n",
      "Train epoch: 1 [2560/60000(4%)]\t Loss: 157.221252\n",
      "Train epoch: 1 [3840/60000(6%)]\t Loss: 155.707321\n",
      "Train epoch: 1 [5120/60000(9%)]\t Loss: 154.427582\n",
      "Train epoch: 1 [6400/60000(11%)]\t Loss: 154.099258\n",
      "Train epoch: 1 [7680/60000(13%)]\t Loss: 154.551041\n",
      "Train epoch: 1 [8960/60000(15%)]\t Loss: 158.796341\n",
      "Train epoch: 1 [10240/60000(17%)]\t Loss: 154.453033\n",
      "Train epoch: 1 [11520/60000(19%)]\t Loss: 157.107574\n",
      "Train epoch: 1 [12800/60000(21%)]\t Loss: 154.180969\n",
      "Train epoch: 1 [14080/60000(23%)]\t Loss: 151.006485\n",
      "Train epoch: 1 [15360/60000(26%)]\t Loss: 149.406281\n",
      "Train epoch: 1 [16640/60000(28%)]\t Loss: 147.843430\n",
      "Train epoch: 1 [17920/60000(30%)]\t Loss: 144.145111\n",
      "Train epoch: 1 [19200/60000(32%)]\t Loss: 145.154739\n",
      "Train epoch: 1 [20480/60000(34%)]\t Loss: 148.048965\n",
      "Train epoch: 1 [21760/60000(36%)]\t Loss: 141.290436\n",
      "Train epoch: 1 [23040/60000(38%)]\t Loss: 146.001450\n",
      "Train epoch: 1 [24320/60000(41%)]\t Loss: 145.897827\n",
      "Train epoch: 1 [25600/60000(43%)]\t Loss: 145.119431\n",
      "Train epoch: 1 [26880/60000(45%)]\t Loss: 144.532715\n",
      "Train epoch: 1 [28160/60000(47%)]\t Loss: 144.723587\n",
      "Train epoch: 1 [29440/60000(49%)]\t Loss: 143.064255\n",
      "Train epoch: 1 [30720/60000(51%)]\t Loss: 142.544922\n",
      "Train epoch: 1 [32000/60000(53%)]\t Loss: 140.367264\n",
      "Train epoch: 1 [33280/60000(55%)]\t Loss: 141.731689\n",
      "Train epoch: 1 [34560/60000(58%)]\t Loss: 146.128357\n",
      "Train epoch: 1 [35840/60000(60%)]\t Loss: 136.030533\n",
      "Train epoch: 1 [37120/60000(62%)]\t Loss: 138.457886\n",
      "Train epoch: 1 [38400/60000(64%)]\t Loss: 142.432190\n",
      "Train epoch: 1 [39680/60000(66%)]\t Loss: 136.368637\n",
      "Train epoch: 1 [40960/60000(68%)]\t Loss: 137.986404\n",
      "Train epoch: 1 [42240/60000(70%)]\t Loss: 142.046829\n",
      "Train epoch: 1 [43520/60000(72%)]\t Loss: 135.715698\n",
      "Train epoch: 1 [44800/60000(75%)]\t Loss: 136.521622\n",
      "Train epoch: 1 [46080/60000(77%)]\t Loss: 137.454803\n",
      "Train epoch: 1 [47360/60000(79%)]\t Loss: 133.856216\n",
      "Train epoch: 1 [48640/60000(81%)]\t Loss: 137.076813\n",
      "Train epoch: 1 [49920/60000(83%)]\t Loss: 137.755371\n",
      "Train epoch: 1 [51200/60000(85%)]\t Loss: 137.418640\n",
      "Train epoch: 1 [52480/60000(87%)]\t Loss: 126.348320\n",
      "Train epoch: 1 [53760/60000(90%)]\t Loss: 132.532501\n",
      "Train epoch: 1 [55040/60000(92%)]\t Loss: 128.266403\n",
      "Train epoch: 1 [56320/60000(94%)]\t Loss: 134.976105\n",
      "Train epoch: 1 [57600/60000(96%)]\t Loss: 132.908417\n",
      "Train epoch: 1 [58880/60000(98%)]\t Loss: 129.130417\n",
      "====> Epoch: 1 Average loss: 144.7671\n",
      "Train epoch: 2 [0/60000(0%)]\t Loss: 129.807770\n",
      "Train epoch: 2 [1280/60000(2%)]\t Loss: 140.811920\n",
      "Train epoch: 2 [2560/60000(4%)]\t Loss: 134.092087\n",
      "Train epoch: 2 [3840/60000(6%)]\t Loss: 129.485031\n",
      "Train epoch: 2 [5120/60000(9%)]\t Loss: 132.613205\n",
      "Train epoch: 2 [6400/60000(11%)]\t Loss: 126.158203\n",
      "Train epoch: 2 [7680/60000(13%)]\t Loss: 127.857277\n",
      "Train epoch: 2 [8960/60000(15%)]\t Loss: 123.941589\n",
      "Train epoch: 2 [10240/60000(17%)]\t Loss: 126.344452\n",
      "Train epoch: 2 [11520/60000(19%)]\t Loss: 129.564575\n",
      "Train epoch: 2 [12800/60000(21%)]\t Loss: 124.907745\n",
      "Train epoch: 2 [14080/60000(23%)]\t Loss: 132.742523\n",
      "Train epoch: 2 [15360/60000(26%)]\t Loss: 118.263084\n",
      "Train epoch: 2 [16640/60000(28%)]\t Loss: 125.028801\n",
      "Train epoch: 2 [17920/60000(30%)]\t Loss: 126.777916\n",
      "Train epoch: 2 [19200/60000(32%)]\t Loss: 119.375626\n",
      "Train epoch: 2 [20480/60000(34%)]\t Loss: 121.997574\n",
      "Train epoch: 2 [21760/60000(36%)]\t Loss: 119.794266\n",
      "Train epoch: 2 [23040/60000(38%)]\t Loss: 128.865723\n",
      "Train epoch: 2 [24320/60000(41%)]\t Loss: 123.153984\n",
      "Train epoch: 2 [25600/60000(43%)]\t Loss: 125.859779\n",
      "Train epoch: 2 [26880/60000(45%)]\t Loss: 121.002548\n",
      "Train epoch: 2 [28160/60000(47%)]\t Loss: 122.145706\n",
      "Train epoch: 2 [29440/60000(49%)]\t Loss: 127.310051\n",
      "Train epoch: 2 [30720/60000(51%)]\t Loss: 119.312920\n",
      "Train epoch: 2 [32000/60000(53%)]\t Loss: 127.628647\n",
      "Train epoch: 2 [33280/60000(55%)]\t Loss: 119.742340\n",
      "Train epoch: 2 [34560/60000(58%)]\t Loss: 124.270706\n",
      "Train epoch: 2 [35840/60000(60%)]\t Loss: 116.638878\n",
      "Train epoch: 2 [37120/60000(62%)]\t Loss: 125.758430\n",
      "Train epoch: 2 [38400/60000(64%)]\t Loss: 127.042427\n",
      "Train epoch: 2 [39680/60000(66%)]\t Loss: 120.586990\n",
      "Train epoch: 2 [40960/60000(68%)]\t Loss: 115.326073\n",
      "Train epoch: 2 [42240/60000(70%)]\t Loss: 122.319656\n",
      "Train epoch: 2 [43520/60000(72%)]\t Loss: 113.827843\n",
      "Train epoch: 2 [44800/60000(75%)]\t Loss: 122.581940\n",
      "Train epoch: 2 [46080/60000(77%)]\t Loss: 125.584396\n",
      "Train epoch: 2 [47360/60000(79%)]\t Loss: 117.979614\n",
      "Train epoch: 2 [48640/60000(81%)]\t Loss: 115.297577\n",
      "Train epoch: 2 [49920/60000(83%)]\t Loss: 115.765976\n",
      "Train epoch: 2 [51200/60000(85%)]\t Loss: 118.214340\n",
      "Train epoch: 2 [52480/60000(87%)]\t Loss: 112.633560\n",
      "Train epoch: 2 [53760/60000(90%)]\t Loss: 116.056389\n",
      "Train epoch: 2 [55040/60000(92%)]\t Loss: 120.026100\n",
      "Train epoch: 2 [56320/60000(94%)]\t Loss: 113.767189\n",
      "Train epoch: 2 [57600/60000(96%)]\t Loss: 113.817467\n",
      "Train epoch: 2 [58880/60000(98%)]\t Loss: 118.374207\n",
      "====> Epoch: 2 Average loss: 122.6848\n",
      "Train epoch: 3 [0/60000(0%)]\t Loss: 115.995995\n",
      "Train epoch: 3 [1280/60000(2%)]\t Loss: 112.168694\n",
      "Train epoch: 3 [2560/60000(4%)]\t Loss: 116.612236\n",
      "Train epoch: 3 [3840/60000(6%)]\t Loss: 113.768356\n",
      "Train epoch: 3 [5120/60000(9%)]\t Loss: 111.380447\n",
      "Train epoch: 3 [6400/60000(11%)]\t Loss: 110.261833\n",
      "Train epoch: 3 [7680/60000(13%)]\t Loss: 115.498817\n",
      "Train epoch: 3 [8960/60000(15%)]\t Loss: 107.547234\n",
      "Train epoch: 3 [10240/60000(17%)]\t Loss: 111.527611\n",
      "Train epoch: 3 [11520/60000(19%)]\t Loss: 113.663429\n",
      "Train epoch: 3 [12800/60000(21%)]\t Loss: 110.030014\n",
      "Train epoch: 3 [14080/60000(23%)]\t Loss: 114.206795\n",
      "Train epoch: 3 [15360/60000(26%)]\t Loss: 114.825310\n",
      "Train epoch: 3 [16640/60000(28%)]\t Loss: 114.136459\n",
      "Train epoch: 3 [17920/60000(30%)]\t Loss: 111.047089\n",
      "Train epoch: 3 [19200/60000(32%)]\t Loss: 108.802315\n",
      "Train epoch: 3 [20480/60000(34%)]\t Loss: 119.330688\n",
      "Train epoch: 3 [21760/60000(36%)]\t Loss: 115.044510\n",
      "Train epoch: 3 [23040/60000(38%)]\t Loss: 108.408340\n",
      "Train epoch: 3 [24320/60000(41%)]\t Loss: 108.463455\n",
      "Train epoch: 3 [25600/60000(43%)]\t Loss: 112.077202\n",
      "Train epoch: 3 [26880/60000(45%)]\t Loss: 108.455238\n",
      "Train epoch: 3 [28160/60000(47%)]\t Loss: 108.614265\n",
      "Train epoch: 3 [29440/60000(49%)]\t Loss: 114.057426\n",
      "Train epoch: 3 [30720/60000(51%)]\t Loss: 107.633430\n",
      "Train epoch: 3 [32000/60000(53%)]\t Loss: 105.695862\n",
      "Train epoch: 3 [33280/60000(55%)]\t Loss: 108.576881\n",
      "Train epoch: 3 [34560/60000(58%)]\t Loss: 112.027840\n",
      "Train epoch: 3 [35840/60000(60%)]\t Loss: 104.776428\n",
      "Train epoch: 3 [37120/60000(62%)]\t Loss: 103.757072\n",
      "Train epoch: 3 [38400/60000(64%)]\t Loss: 108.978645\n",
      "Train epoch: 3 [39680/60000(66%)]\t Loss: 108.016365\n",
      "Train epoch: 3 [40960/60000(68%)]\t Loss: 107.489891\n",
      "Train epoch: 3 [42240/60000(70%)]\t Loss: 103.232056\n",
      "Train epoch: 3 [43520/60000(72%)]\t Loss: 105.008423\n",
      "Train epoch: 3 [44800/60000(75%)]\t Loss: 104.436989\n",
      "Train epoch: 3 [46080/60000(77%)]\t Loss: 108.532867\n",
      "Train epoch: 3 [47360/60000(79%)]\t Loss: 105.025314\n",
      "Train epoch: 3 [48640/60000(81%)]\t Loss: 109.717018\n",
      "Train epoch: 3 [49920/60000(83%)]\t Loss: 102.540382\n",
      "Train epoch: 3 [51200/60000(85%)]\t Loss: 104.340111\n",
      "Train epoch: 3 [52480/60000(87%)]\t Loss: 107.842224\n",
      "Train epoch: 3 [53760/60000(90%)]\t Loss: 110.137436\n",
      "Train epoch: 3 [55040/60000(92%)]\t Loss: 103.307228\n",
      "Train epoch: 3 [56320/60000(94%)]\t Loss: 100.172470\n",
      "Train epoch: 3 [57600/60000(96%)]\t Loss: 103.348190\n",
      "Train epoch: 3 [58880/60000(98%)]\t Loss: 107.201790\n",
      "====> Epoch: 3 Average loss: 108.7309\n",
      "Train epoch: 4 [0/60000(0%)]\t Loss: 106.477287\n",
      "Train epoch: 4 [1280/60000(2%)]\t Loss: 101.186890\n",
      "Train epoch: 4 [2560/60000(4%)]\t Loss: 104.025162\n",
      "Train epoch: 4 [3840/60000(6%)]\t Loss: 100.485107\n",
      "Train epoch: 4 [5120/60000(9%)]\t Loss: 97.554398\n",
      "Train epoch: 4 [6400/60000(11%)]\t Loss: 103.578690\n",
      "Train epoch: 4 [7680/60000(13%)]\t Loss: 97.063354\n",
      "Train epoch: 4 [8960/60000(15%)]\t Loss: 102.370262\n",
      "Train epoch: 4 [10240/60000(17%)]\t Loss: 96.887352\n",
      "Train epoch: 4 [11520/60000(19%)]\t Loss: 104.148247\n",
      "Train epoch: 4 [12800/60000(21%)]\t Loss: 101.889839\n",
      "Train epoch: 4 [14080/60000(23%)]\t Loss: 102.080765\n",
      "Train epoch: 4 [15360/60000(26%)]\t Loss: 98.659363\n",
      "Train epoch: 4 [16640/60000(28%)]\t Loss: 101.085739\n",
      "Train epoch: 4 [17920/60000(30%)]\t Loss: 101.941727\n",
      "Train epoch: 4 [19200/60000(32%)]\t Loss: 100.101875\n",
      "Train epoch: 4 [20480/60000(34%)]\t Loss: 97.121216\n",
      "Train epoch: 4 [21760/60000(36%)]\t Loss: 97.622787\n",
      "Train epoch: 4 [23040/60000(38%)]\t Loss: 97.669701\n",
      "Train epoch: 4 [24320/60000(41%)]\t Loss: 98.602753\n",
      "Train epoch: 4 [25600/60000(43%)]\t Loss: 101.623505\n",
      "Train epoch: 4 [26880/60000(45%)]\t Loss: 100.331764\n",
      "Train epoch: 4 [28160/60000(47%)]\t Loss: 104.805595\n",
      "Train epoch: 4 [29440/60000(49%)]\t Loss: 95.815666\n",
      "Train epoch: 4 [30720/60000(51%)]\t Loss: 97.851120\n",
      "Train epoch: 4 [32000/60000(53%)]\t Loss: 100.625328\n",
      "Train epoch: 4 [33280/60000(55%)]\t Loss: 98.155991\n",
      "Train epoch: 4 [34560/60000(58%)]\t Loss: 97.904877\n",
      "Train epoch: 4 [35840/60000(60%)]\t Loss: 94.780388\n",
      "Train epoch: 4 [37120/60000(62%)]\t Loss: 94.671791\n",
      "Train epoch: 4 [38400/60000(64%)]\t Loss: 94.385689\n",
      "Train epoch: 4 [39680/60000(66%)]\t Loss: 97.646271\n",
      "Train epoch: 4 [40960/60000(68%)]\t Loss: 95.519783\n",
      "Train epoch: 4 [42240/60000(70%)]\t Loss: 98.021545\n",
      "Train epoch: 4 [43520/60000(72%)]\t Loss: 96.467415\n",
      "Train epoch: 4 [44800/60000(75%)]\t Loss: 98.481575\n",
      "Train epoch: 4 [46080/60000(77%)]\t Loss: 97.465668\n",
      "Train epoch: 4 [47360/60000(79%)]\t Loss: 95.040192\n",
      "Train epoch: 4 [48640/60000(81%)]\t Loss: 90.454926\n",
      "Train epoch: 4 [49920/60000(83%)]\t Loss: 95.283829\n",
      "Train epoch: 4 [51200/60000(85%)]\t Loss: 93.308792\n",
      "Train epoch: 4 [52480/60000(87%)]\t Loss: 95.225861\n",
      "Train epoch: 4 [53760/60000(90%)]\t Loss: 95.747116\n",
      "Train epoch: 4 [55040/60000(92%)]\t Loss: 95.297333\n",
      "Train epoch: 4 [56320/60000(94%)]\t Loss: 96.001495\n",
      "Train epoch: 4 [57600/60000(96%)]\t Loss: 93.808601\n",
      "Train epoch: 4 [58880/60000(98%)]\t Loss: 92.855652\n",
      "====> Epoch: 4 Average loss: 98.6613\n",
      "Train epoch: 5 [0/60000(0%)]\t Loss: 96.036140\n",
      "Train epoch: 5 [1280/60000(2%)]\t Loss: 95.104637\n",
      "Train epoch: 5 [2560/60000(4%)]\t Loss: 92.719704\n",
      "Train epoch: 5 [3840/60000(6%)]\t Loss: 90.376411\n",
      "Train epoch: 5 [5120/60000(9%)]\t Loss: 91.805672\n",
      "Train epoch: 5 [6400/60000(11%)]\t Loss: 89.756096\n",
      "Train epoch: 5 [7680/60000(13%)]\t Loss: 95.156517\n",
      "Train epoch: 5 [8960/60000(15%)]\t Loss: 92.267593\n",
      "Train epoch: 5 [10240/60000(17%)]\t Loss: 97.055443\n",
      "Train epoch: 5 [11520/60000(19%)]\t Loss: 96.456757\n",
      "Train epoch: 5 [12800/60000(21%)]\t Loss: 92.558929\n",
      "Train epoch: 5 [14080/60000(23%)]\t Loss: 91.329674\n",
      "Train epoch: 5 [15360/60000(26%)]\t Loss: 94.381836\n",
      "Train epoch: 5 [16640/60000(28%)]\t Loss: 93.720291\n",
      "Train epoch: 5 [17920/60000(30%)]\t Loss: 88.741936\n",
      "Train epoch: 5 [19200/60000(32%)]\t Loss: 94.770927\n",
      "Train epoch: 5 [20480/60000(34%)]\t Loss: 93.712738\n",
      "Train epoch: 5 [21760/60000(36%)]\t Loss: 94.897858\n",
      "Train epoch: 5 [23040/60000(38%)]\t Loss: 89.646935\n",
      "Train epoch: 5 [24320/60000(41%)]\t Loss: 89.851822\n",
      "Train epoch: 5 [25600/60000(43%)]\t Loss: 86.113853\n",
      "Train epoch: 5 [26880/60000(45%)]\t Loss: 91.631104\n",
      "Train epoch: 5 [28160/60000(47%)]\t Loss: 91.697807\n",
      "Train epoch: 5 [29440/60000(49%)]\t Loss: 91.547630\n",
      "Train epoch: 5 [30720/60000(51%)]\t Loss: 92.704742\n",
      "Train epoch: 5 [32000/60000(53%)]\t Loss: 90.803925\n",
      "Train epoch: 5 [33280/60000(55%)]\t Loss: 92.080170\n",
      "Train epoch: 5 [34560/60000(58%)]\t Loss: 89.043396\n",
      "Train epoch: 5 [35840/60000(60%)]\t Loss: 88.107307\n",
      "Train epoch: 5 [37120/60000(62%)]\t Loss: 89.577385\n",
      "Train epoch: 5 [38400/60000(64%)]\t Loss: 91.288864\n",
      "Train epoch: 5 [39680/60000(66%)]\t Loss: 92.150612\n",
      "Train epoch: 5 [40960/60000(68%)]\t Loss: 91.863182\n",
      "Train epoch: 5 [42240/60000(70%)]\t Loss: 89.077377\n",
      "Train epoch: 5 [43520/60000(72%)]\t Loss: 90.285904\n",
      "Train epoch: 5 [44800/60000(75%)]\t Loss: 87.279549\n",
      "Train epoch: 5 [46080/60000(77%)]\t Loss: 88.481567\n",
      "Train epoch: 5 [47360/60000(79%)]\t Loss: 92.945869\n",
      "Train epoch: 5 [48640/60000(81%)]\t Loss: 87.344536\n",
      "Train epoch: 5 [49920/60000(83%)]\t Loss: 88.104935\n",
      "Train epoch: 5 [51200/60000(85%)]\t Loss: 87.739151\n",
      "Train epoch: 5 [52480/60000(87%)]\t Loss: 91.928268\n",
      "Train epoch: 5 [53760/60000(90%)]\t Loss: 84.581726\n",
      "Train epoch: 5 [55040/60000(92%)]\t Loss: 89.194016\n",
      "Train epoch: 5 [56320/60000(94%)]\t Loss: 87.433136\n",
      "Train epoch: 5 [57600/60000(96%)]\t Loss: 84.074356\n",
      "Train epoch: 5 [58880/60000(98%)]\t Loss: 84.950020\n",
      "====> Epoch: 5 Average loss: 91.0449\n",
      "Train epoch: 6 [0/60000(0%)]\t Loss: 89.297157\n",
      "Train epoch: 6 [1280/60000(2%)]\t Loss: 89.318619\n",
      "Train epoch: 6 [2560/60000(4%)]\t Loss: 86.097565\n",
      "Train epoch: 6 [3840/60000(6%)]\t Loss: 87.086800\n",
      "Train epoch: 6 [5120/60000(9%)]\t Loss: 91.057289\n",
      "Train epoch: 6 [6400/60000(11%)]\t Loss: 84.663986\n",
      "Train epoch: 6 [7680/60000(13%)]\t Loss: 88.504677\n",
      "Train epoch: 6 [8960/60000(15%)]\t Loss: 85.351990\n",
      "Train epoch: 6 [10240/60000(17%)]\t Loss: 86.777481\n",
      "Train epoch: 6 [11520/60000(19%)]\t Loss: 88.682571\n",
      "Train epoch: 6 [12800/60000(21%)]\t Loss: 84.309914\n",
      "Train epoch: 6 [14080/60000(23%)]\t Loss: 87.796059\n",
      "Train epoch: 6 [15360/60000(26%)]\t Loss: 88.418587\n",
      "Train epoch: 6 [16640/60000(28%)]\t Loss: 87.236778\n",
      "Train epoch: 6 [17920/60000(30%)]\t Loss: 86.004173\n",
      "Train epoch: 6 [19200/60000(32%)]\t Loss: 89.886276\n",
      "Train epoch: 6 [20480/60000(34%)]\t Loss: 88.427437\n",
      "Train epoch: 6 [21760/60000(36%)]\t Loss: 87.416344\n",
      "Train epoch: 6 [23040/60000(38%)]\t Loss: 89.161278\n",
      "Train epoch: 6 [24320/60000(41%)]\t Loss: 82.275742\n",
      "Train epoch: 6 [25600/60000(43%)]\t Loss: 84.101738\n",
      "Train epoch: 6 [26880/60000(45%)]\t Loss: 85.612572\n",
      "Train epoch: 6 [28160/60000(47%)]\t Loss: 87.479874\n",
      "Train epoch: 6 [29440/60000(49%)]\t Loss: 85.111008\n",
      "Train epoch: 6 [30720/60000(51%)]\t Loss: 86.774498\n",
      "Train epoch: 6 [32000/60000(53%)]\t Loss: 86.190117\n",
      "Train epoch: 6 [33280/60000(55%)]\t Loss: 87.347275\n",
      "Train epoch: 6 [34560/60000(58%)]\t Loss: 85.467598\n",
      "Train epoch: 6 [35840/60000(60%)]\t Loss: 85.222847\n",
      "Train epoch: 6 [37120/60000(62%)]\t Loss: 81.626457\n",
      "Train epoch: 6 [38400/60000(64%)]\t Loss: 83.721138\n",
      "Train epoch: 6 [39680/60000(66%)]\t Loss: 83.243538\n",
      "Train epoch: 6 [40960/60000(68%)]\t Loss: 83.164391\n",
      "Train epoch: 6 [42240/60000(70%)]\t Loss: 83.664673\n",
      "Train epoch: 6 [43520/60000(72%)]\t Loss: 82.353012\n",
      "Train epoch: 6 [44800/60000(75%)]\t Loss: 85.429451\n",
      "Train epoch: 6 [46080/60000(77%)]\t Loss: 85.525009\n",
      "Train epoch: 6 [47360/60000(79%)]\t Loss: 83.909554\n",
      "Train epoch: 6 [48640/60000(81%)]\t Loss: 83.439384\n",
      "Train epoch: 6 [49920/60000(83%)]\t Loss: 86.771698\n",
      "Train epoch: 6 [51200/60000(85%)]\t Loss: 82.199181\n",
      "Train epoch: 6 [52480/60000(87%)]\t Loss: 80.518478\n",
      "Train epoch: 6 [53760/60000(90%)]\t Loss: 86.866570\n",
      "Train epoch: 6 [55040/60000(92%)]\t Loss: 83.572075\n",
      "Train epoch: 6 [56320/60000(94%)]\t Loss: 85.014076\n",
      "Train epoch: 6 [57600/60000(96%)]\t Loss: 79.526398\n",
      "Train epoch: 6 [58880/60000(98%)]\t Loss: 84.806541\n",
      "====> Epoch: 6 Average loss: 85.2243\n",
      "Train epoch: 7 [0/60000(0%)]\t Loss: 84.053108\n",
      "Train epoch: 7 [1280/60000(2%)]\t Loss: 80.688400\n",
      "Train epoch: 7 [2560/60000(4%)]\t Loss: 82.826118\n",
      "Train epoch: 7 [3840/60000(6%)]\t Loss: 80.506630\n",
      "Train epoch: 7 [5120/60000(9%)]\t Loss: 82.767555\n",
      "Train epoch: 7 [6400/60000(11%)]\t Loss: 77.506317\n",
      "Train epoch: 7 [7680/60000(13%)]\t Loss: 82.360382\n",
      "Train epoch: 7 [8960/60000(15%)]\t Loss: 81.476776\n",
      "Train epoch: 7 [10240/60000(17%)]\t Loss: 81.686707\n",
      "Train epoch: 7 [11520/60000(19%)]\t Loss: 80.853218\n",
      "Train epoch: 7 [12800/60000(21%)]\t Loss: 78.883987\n",
      "Train epoch: 7 [14080/60000(23%)]\t Loss: 82.827583\n",
      "Train epoch: 7 [15360/60000(26%)]\t Loss: 80.778275\n",
      "Train epoch: 7 [16640/60000(28%)]\t Loss: 84.112785\n",
      "Train epoch: 7 [17920/60000(30%)]\t Loss: 77.278992\n",
      "Train epoch: 7 [19200/60000(32%)]\t Loss: 81.300789\n",
      "Train epoch: 7 [20480/60000(34%)]\t Loss: 80.319733\n",
      "Train epoch: 7 [21760/60000(36%)]\t Loss: 85.142258\n",
      "Train epoch: 7 [23040/60000(38%)]\t Loss: 80.475494\n",
      "Train epoch: 7 [24320/60000(41%)]\t Loss: 80.427292\n",
      "Train epoch: 7 [25600/60000(43%)]\t Loss: 84.016602\n",
      "Train epoch: 7 [26880/60000(45%)]\t Loss: 84.470825\n",
      "Train epoch: 7 [28160/60000(47%)]\t Loss: 84.876190\n",
      "Train epoch: 7 [29440/60000(49%)]\t Loss: 78.232552\n",
      "Train epoch: 7 [30720/60000(51%)]\t Loss: 81.492493\n",
      "Train epoch: 7 [32000/60000(53%)]\t Loss: 77.917412\n",
      "Train epoch: 7 [33280/60000(55%)]\t Loss: 81.739334\n",
      "Train epoch: 7 [34560/60000(58%)]\t Loss: 77.295647\n",
      "Train epoch: 7 [35840/60000(60%)]\t Loss: 78.278313\n",
      "Train epoch: 7 [37120/60000(62%)]\t Loss: 78.890640\n",
      "Train epoch: 7 [38400/60000(64%)]\t Loss: 77.957932\n",
      "Train epoch: 7 [39680/60000(66%)]\t Loss: 79.164589\n",
      "Train epoch: 7 [40960/60000(68%)]\t Loss: 78.335846\n",
      "Train epoch: 7 [42240/60000(70%)]\t Loss: 81.425125\n",
      "Train epoch: 7 [43520/60000(72%)]\t Loss: 80.713730\n",
      "Train epoch: 7 [44800/60000(75%)]\t Loss: 80.047432\n",
      "Train epoch: 7 [46080/60000(77%)]\t Loss: 81.801445\n",
      "Train epoch: 7 [47360/60000(79%)]\t Loss: 82.977005\n",
      "Train epoch: 7 [48640/60000(81%)]\t Loss: 79.781517\n",
      "Train epoch: 7 [49920/60000(83%)]\t Loss: 79.253387\n",
      "Train epoch: 7 [51200/60000(85%)]\t Loss: 82.276100\n",
      "Train epoch: 7 [52480/60000(87%)]\t Loss: 76.540840\n",
      "Train epoch: 7 [53760/60000(90%)]\t Loss: 74.863533\n",
      "Train epoch: 7 [55040/60000(92%)]\t Loss: 79.683472\n",
      "Train epoch: 7 [56320/60000(94%)]\t Loss: 77.831528\n",
      "Train epoch: 7 [57600/60000(96%)]\t Loss: 82.837189\n",
      "Train epoch: 7 [58880/60000(98%)]\t Loss: 76.630707\n",
      "====> Epoch: 7 Average loss: 80.6985\n",
      "Train epoch: 8 [0/60000(0%)]\t Loss: 81.164650\n",
      "Train epoch: 8 [1280/60000(2%)]\t Loss: 77.451607\n",
      "Train epoch: 8 [2560/60000(4%)]\t Loss: 76.730278\n",
      "Train epoch: 8 [3840/60000(6%)]\t Loss: 77.066086\n",
      "Train epoch: 8 [5120/60000(9%)]\t Loss: 77.865738\n",
      "Train epoch: 8 [6400/60000(11%)]\t Loss: 76.476303\n",
      "Train epoch: 8 [7680/60000(13%)]\t Loss: 79.973763\n",
      "Train epoch: 8 [8960/60000(15%)]\t Loss: 76.100533\n",
      "Train epoch: 8 [10240/60000(17%)]\t Loss: 80.618423\n",
      "Train epoch: 8 [11520/60000(19%)]\t Loss: 79.594200\n",
      "Train epoch: 8 [12800/60000(21%)]\t Loss: 81.388512\n",
      "Train epoch: 8 [14080/60000(23%)]\t Loss: 75.529594\n",
      "Train epoch: 8 [15360/60000(26%)]\t Loss: 76.832497\n",
      "Train epoch: 8 [16640/60000(28%)]\t Loss: 78.256264\n",
      "Train epoch: 8 [17920/60000(30%)]\t Loss: 77.293709\n",
      "Train epoch: 8 [19200/60000(32%)]\t Loss: 77.628906\n",
      "Train epoch: 8 [20480/60000(34%)]\t Loss: 75.125031\n",
      "Train epoch: 8 [21760/60000(36%)]\t Loss: 81.161522\n",
      "Train epoch: 8 [23040/60000(38%)]\t Loss: 77.653374\n",
      "Train epoch: 8 [24320/60000(41%)]\t Loss: 75.709709\n",
      "Train epoch: 8 [25600/60000(43%)]\t Loss: 73.939865\n",
      "Train epoch: 8 [26880/60000(45%)]\t Loss: 75.836510\n",
      "Train epoch: 8 [28160/60000(47%)]\t Loss: 76.906387\n",
      "Train epoch: 8 [29440/60000(49%)]\t Loss: 76.006348\n",
      "Train epoch: 8 [30720/60000(51%)]\t Loss: 78.574356\n",
      "Train epoch: 8 [32000/60000(53%)]\t Loss: 79.299896\n",
      "Train epoch: 8 [33280/60000(55%)]\t Loss: 71.435143\n",
      "Train epoch: 8 [34560/60000(58%)]\t Loss: 75.860031\n",
      "Train epoch: 8 [35840/60000(60%)]\t Loss: 78.251030\n",
      "Train epoch: 8 [37120/60000(62%)]\t Loss: 79.926933\n",
      "Train epoch: 8 [38400/60000(64%)]\t Loss: 71.240585\n",
      "Train epoch: 8 [39680/60000(66%)]\t Loss: 74.903526\n",
      "Train epoch: 8 [40960/60000(68%)]\t Loss: 75.951477\n",
      "Train epoch: 8 [42240/60000(70%)]\t Loss: 80.721664\n",
      "Train epoch: 8 [43520/60000(72%)]\t Loss: 75.472649\n",
      "Train epoch: 8 [44800/60000(75%)]\t Loss: 76.845993\n",
      "Train epoch: 8 [46080/60000(77%)]\t Loss: 74.042740\n",
      "Train epoch: 8 [47360/60000(79%)]\t Loss: 81.506920\n",
      "Train epoch: 8 [48640/60000(81%)]\t Loss: 76.224770\n",
      "Train epoch: 8 [49920/60000(83%)]\t Loss: 77.698708\n",
      "Train epoch: 8 [51200/60000(85%)]\t Loss: 75.164528\n",
      "Train epoch: 8 [52480/60000(87%)]\t Loss: 75.297676\n",
      "Train epoch: 8 [53760/60000(90%)]\t Loss: 74.607346\n",
      "Train epoch: 8 [55040/60000(92%)]\t Loss: 75.286453\n",
      "Train epoch: 8 [56320/60000(94%)]\t Loss: 75.074432\n",
      "Train epoch: 8 [57600/60000(96%)]\t Loss: 75.536873\n",
      "Train epoch: 8 [58880/60000(98%)]\t Loss: 75.930016\n",
      "====> Epoch: 8 Average loss: 77.1137\n",
      "Train epoch: 9 [0/60000(0%)]\t Loss: 75.426941\n",
      "Train epoch: 9 [1280/60000(2%)]\t Loss: 74.048470\n",
      "Train epoch: 9 [2560/60000(4%)]\t Loss: 77.505356\n",
      "Train epoch: 9 [3840/60000(6%)]\t Loss: 78.304657\n",
      "Train epoch: 9 [5120/60000(9%)]\t Loss: 74.442917\n",
      "Train epoch: 9 [6400/60000(11%)]\t Loss: 73.634430\n",
      "Train epoch: 9 [7680/60000(13%)]\t Loss: 74.023125\n",
      "Train epoch: 9 [8960/60000(15%)]\t Loss: 74.070267\n",
      "Train epoch: 9 [10240/60000(17%)]\t Loss: 73.349449\n",
      "Train epoch: 9 [11520/60000(19%)]\t Loss: 77.056877\n",
      "Train epoch: 9 [12800/60000(21%)]\t Loss: 74.897713\n",
      "Train epoch: 9 [14080/60000(23%)]\t Loss: 75.482529\n",
      "Train epoch: 9 [15360/60000(26%)]\t Loss: 72.627403\n",
      "Train epoch: 9 [16640/60000(28%)]\t Loss: 72.665108\n",
      "Train epoch: 9 [17920/60000(30%)]\t Loss: 73.499672\n",
      "Train epoch: 9 [19200/60000(32%)]\t Loss: 77.469452\n",
      "Train epoch: 9 [20480/60000(34%)]\t Loss: 75.196259\n",
      "Train epoch: 9 [21760/60000(36%)]\t Loss: 73.298302\n",
      "Train epoch: 9 [23040/60000(38%)]\t Loss: 76.095047\n",
      "Train epoch: 9 [24320/60000(41%)]\t Loss: 76.243416\n",
      "Train epoch: 9 [25600/60000(43%)]\t Loss: 76.024887\n",
      "Train epoch: 9 [26880/60000(45%)]\t Loss: 74.113922\n",
      "Train epoch: 9 [28160/60000(47%)]\t Loss: 73.162857\n",
      "Train epoch: 9 [29440/60000(49%)]\t Loss: 77.100967\n",
      "Train epoch: 9 [30720/60000(51%)]\t Loss: 72.199379\n",
      "Train epoch: 9 [32000/60000(53%)]\t Loss: 72.450867\n",
      "Train epoch: 9 [33280/60000(55%)]\t Loss: 75.444092\n",
      "Train epoch: 9 [34560/60000(58%)]\t Loss: 72.755707\n",
      "Train epoch: 9 [35840/60000(60%)]\t Loss: 75.822662\n",
      "Train epoch: 9 [37120/60000(62%)]\t Loss: 73.084457\n",
      "Train epoch: 9 [38400/60000(64%)]\t Loss: 74.027496\n",
      "Train epoch: 9 [39680/60000(66%)]\t Loss: 73.835533\n",
      "Train epoch: 9 [40960/60000(68%)]\t Loss: 72.398827\n",
      "Train epoch: 9 [42240/60000(70%)]\t Loss: 72.243698\n",
      "Train epoch: 9 [43520/60000(72%)]\t Loss: 73.042458\n",
      "Train epoch: 9 [44800/60000(75%)]\t Loss: 76.248528\n",
      "Train epoch: 9 [46080/60000(77%)]\t Loss: 72.174835\n",
      "Train epoch: 9 [47360/60000(79%)]\t Loss: 73.620575\n",
      "Train epoch: 9 [48640/60000(81%)]\t Loss: 73.026260\n",
      "Train epoch: 9 [49920/60000(83%)]\t Loss: 76.832413\n",
      "Train epoch: 9 [51200/60000(85%)]\t Loss: 73.032761\n",
      "Train epoch: 9 [52480/60000(87%)]\t Loss: 73.974030\n",
      "Train epoch: 9 [53760/60000(90%)]\t Loss: 74.478012\n",
      "Train epoch: 9 [55040/60000(92%)]\t Loss: 72.914803\n",
      "Train epoch: 9 [56320/60000(94%)]\t Loss: 73.170914\n",
      "Train epoch: 9 [57600/60000(96%)]\t Loss: 72.525391\n",
      "Train epoch: 9 [58880/60000(98%)]\t Loss: 71.437576\n",
      "====> Epoch: 9 Average loss: 74.2029\n",
      "Train epoch: 10 [0/60000(0%)]\t Loss: 73.259071\n",
      "Train epoch: 10 [1280/60000(2%)]\t Loss: 72.145790\n",
      "Train epoch: 10 [2560/60000(4%)]\t Loss: 73.487999\n",
      "Train epoch: 10 [3840/60000(6%)]\t Loss: 70.100861\n",
      "Train epoch: 10 [5120/60000(9%)]\t Loss: 70.864380\n",
      "Train epoch: 10 [6400/60000(11%)]\t Loss: 77.841248\n",
      "Train epoch: 10 [7680/60000(13%)]\t Loss: 74.674026\n",
      "Train epoch: 10 [8960/60000(15%)]\t Loss: 71.517616\n",
      "Train epoch: 10 [10240/60000(17%)]\t Loss: 70.403564\n",
      "Train epoch: 10 [11520/60000(19%)]\t Loss: 70.267685\n",
      "Train epoch: 10 [12800/60000(21%)]\t Loss: 72.235016\n",
      "Train epoch: 10 [14080/60000(23%)]\t Loss: 68.627434\n",
      "Train epoch: 10 [15360/60000(26%)]\t Loss: 75.023438\n",
      "Train epoch: 10 [16640/60000(28%)]\t Loss: 76.368546\n",
      "Train epoch: 10 [17920/60000(30%)]\t Loss: 71.442078\n",
      "Train epoch: 10 [19200/60000(32%)]\t Loss: 72.261299\n",
      "Train epoch: 10 [20480/60000(34%)]\t Loss: 68.088898\n",
      "Train epoch: 10 [21760/60000(36%)]\t Loss: 69.226311\n",
      "Train epoch: 10 [23040/60000(38%)]\t Loss: 73.647736\n",
      "Train epoch: 10 [24320/60000(41%)]\t Loss: 74.968758\n",
      "Train epoch: 10 [25600/60000(43%)]\t Loss: 70.732506\n",
      "Train epoch: 10 [26880/60000(45%)]\t Loss: 71.283325\n",
      "Train epoch: 10 [28160/60000(47%)]\t Loss: 70.752342\n",
      "Train epoch: 10 [29440/60000(49%)]\t Loss: 71.506432\n",
      "Train epoch: 10 [30720/60000(51%)]\t Loss: 71.113609\n",
      "Train epoch: 10 [32000/60000(53%)]\t Loss: 74.069069\n",
      "Train epoch: 10 [33280/60000(55%)]\t Loss: 68.297005\n",
      "Train epoch: 10 [34560/60000(58%)]\t Loss: 74.530457\n",
      "Train epoch: 10 [35840/60000(60%)]\t Loss: 69.745827\n",
      "Train epoch: 10 [37120/60000(62%)]\t Loss: 70.951347\n",
      "Train epoch: 10 [38400/60000(64%)]\t Loss: 73.178017\n",
      "Train epoch: 10 [39680/60000(66%)]\t Loss: 72.930824\n",
      "Train epoch: 10 [40960/60000(68%)]\t Loss: 69.620956\n",
      "Train epoch: 10 [42240/60000(70%)]\t Loss: 72.696327\n",
      "Train epoch: 10 [43520/60000(72%)]\t Loss: 73.333092\n",
      "Train epoch: 10 [44800/60000(75%)]\t Loss: 70.218216\n",
      "Train epoch: 10 [46080/60000(77%)]\t Loss: 71.274925\n",
      "Train epoch: 10 [47360/60000(79%)]\t Loss: 73.456604\n",
      "Train epoch: 10 [48640/60000(81%)]\t Loss: 70.737175\n",
      "Train epoch: 10 [49920/60000(83%)]\t Loss: 71.790482\n",
      "Train epoch: 10 [51200/60000(85%)]\t Loss: 71.613686\n",
      "Train epoch: 10 [52480/60000(87%)]\t Loss: 68.079353\n",
      "Train epoch: 10 [53760/60000(90%)]\t Loss: 72.434662\n",
      "Train epoch: 10 [55040/60000(92%)]\t Loss: 70.983749\n",
      "Train epoch: 10 [56320/60000(94%)]\t Loss: 69.991272\n",
      "Train epoch: 10 [57600/60000(96%)]\t Loss: 70.459473\n",
      "Train epoch: 10 [58880/60000(98%)]\t Loss: 71.981789\n",
      "====> Epoch: 10 Average loss: 71.8013\n",
      "Train epoch: 11 [0/60000(0%)]\t Loss: 68.157494\n",
      "Train epoch: 11 [1280/60000(2%)]\t Loss: 76.340744\n",
      "Train epoch: 11 [2560/60000(4%)]\t Loss: 71.424423\n",
      "Train epoch: 11 [3840/60000(6%)]\t Loss: 69.581551\n",
      "Train epoch: 11 [5120/60000(9%)]\t Loss: 71.300728\n",
      "Train epoch: 11 [6400/60000(11%)]\t Loss: 69.767967\n",
      "Train epoch: 11 [7680/60000(13%)]\t Loss: 71.336319\n",
      "Train epoch: 11 [8960/60000(15%)]\t Loss: 69.782700\n",
      "Train epoch: 11 [10240/60000(17%)]\t Loss: 74.637604\n",
      "Train epoch: 11 [11520/60000(19%)]\t Loss: 71.542061\n",
      "Train epoch: 11 [12800/60000(21%)]\t Loss: 72.577011\n",
      "Train epoch: 11 [14080/60000(23%)]\t Loss: 69.075569\n",
      "Train epoch: 11 [15360/60000(26%)]\t Loss: 73.135925\n",
      "Train epoch: 11 [16640/60000(28%)]\t Loss: 70.490273\n",
      "Train epoch: 11 [17920/60000(30%)]\t Loss: 69.719856\n",
      "Train epoch: 11 [19200/60000(32%)]\t Loss: 70.414703\n",
      "Train epoch: 11 [20480/60000(34%)]\t Loss: 69.937721\n",
      "Train epoch: 11 [21760/60000(36%)]\t Loss: 68.498749\n",
      "Train epoch: 11 [23040/60000(38%)]\t Loss: 72.210320\n",
      "Train epoch: 11 [24320/60000(41%)]\t Loss: 72.314133\n",
      "Train epoch: 11 [25600/60000(43%)]\t Loss: 68.280159\n",
      "Train epoch: 11 [26880/60000(45%)]\t Loss: 69.348984\n",
      "Train epoch: 11 [28160/60000(47%)]\t Loss: 71.393379\n",
      "Train epoch: 11 [29440/60000(49%)]\t Loss: 70.533730\n",
      "Train epoch: 11 [30720/60000(51%)]\t Loss: 72.645264\n",
      "Train epoch: 11 [32000/60000(53%)]\t Loss: 65.987900\n",
      "Train epoch: 11 [33280/60000(55%)]\t Loss: 71.628532\n",
      "Train epoch: 11 [34560/60000(58%)]\t Loss: 69.492126\n",
      "Train epoch: 11 [35840/60000(60%)]\t Loss: 70.442375\n",
      "Train epoch: 11 [37120/60000(62%)]\t Loss: 68.416725\n",
      "Train epoch: 11 [38400/60000(64%)]\t Loss: 70.431648\n",
      "Train epoch: 11 [39680/60000(66%)]\t Loss: 68.658348\n",
      "Train epoch: 11 [40960/60000(68%)]\t Loss: 76.282089\n",
      "Train epoch: 11 [42240/60000(70%)]\t Loss: 66.092010\n",
      "Train epoch: 11 [43520/60000(72%)]\t Loss: 68.698769\n",
      "Train epoch: 11 [44800/60000(75%)]\t Loss: 68.832069\n",
      "Train epoch: 11 [46080/60000(77%)]\t Loss: 70.012337\n",
      "Train epoch: 11 [47360/60000(79%)]\t Loss: 66.463814\n",
      "Train epoch: 11 [48640/60000(81%)]\t Loss: 69.126907\n",
      "Train epoch: 11 [49920/60000(83%)]\t Loss: 68.349693\n",
      "Train epoch: 11 [51200/60000(85%)]\t Loss: 69.596855\n",
      "Train epoch: 11 [52480/60000(87%)]\t Loss: 70.290215\n",
      "Train epoch: 11 [53760/60000(90%)]\t Loss: 70.865730\n",
      "Train epoch: 11 [55040/60000(92%)]\t Loss: 71.603447\n",
      "Train epoch: 11 [56320/60000(94%)]\t Loss: 66.738403\n",
      "Train epoch: 11 [57600/60000(96%)]\t Loss: 68.561554\n",
      "Train epoch: 11 [58880/60000(98%)]\t Loss: 69.088043\n",
      "====> Epoch: 11 Average loss: 69.7963\n",
      "Train epoch: 12 [0/60000(0%)]\t Loss: 67.562302\n",
      "Train epoch: 12 [1280/60000(2%)]\t Loss: 71.504082\n",
      "Train epoch: 12 [2560/60000(4%)]\t Loss: 67.002182\n",
      "Train epoch: 12 [3840/60000(6%)]\t Loss: 68.765060\n",
      "Train epoch: 12 [5120/60000(9%)]\t Loss: 67.159470\n",
      "Train epoch: 12 [6400/60000(11%)]\t Loss: 69.913773\n",
      "Train epoch: 12 [7680/60000(13%)]\t Loss: 69.553154\n",
      "Train epoch: 12 [8960/60000(15%)]\t Loss: 68.565399\n",
      "Train epoch: 12 [10240/60000(17%)]\t Loss: 66.844917\n",
      "Train epoch: 12 [11520/60000(19%)]\t Loss: 67.656464\n",
      "Train epoch: 12 [12800/60000(21%)]\t Loss: 65.215683\n",
      "Train epoch: 12 [14080/60000(23%)]\t Loss: 65.527740\n",
      "Train epoch: 12 [15360/60000(26%)]\t Loss: 68.603142\n",
      "Train epoch: 12 [16640/60000(28%)]\t Loss: 67.896973\n",
      "Train epoch: 12 [17920/60000(30%)]\t Loss: 65.074852\n",
      "Train epoch: 12 [19200/60000(32%)]\t Loss: 69.023109\n",
      "Train epoch: 12 [20480/60000(34%)]\t Loss: 67.331665\n",
      "Train epoch: 12 [21760/60000(36%)]\t Loss: 67.785774\n",
      "Train epoch: 12 [23040/60000(38%)]\t Loss: 71.225594\n",
      "Train epoch: 12 [24320/60000(41%)]\t Loss: 68.517456\n",
      "Train epoch: 12 [25600/60000(43%)]\t Loss: 70.511253\n",
      "Train epoch: 12 [26880/60000(45%)]\t Loss: 67.796776\n",
      "Train epoch: 12 [28160/60000(47%)]\t Loss: 70.204277\n",
      "Train epoch: 12 [29440/60000(49%)]\t Loss: 67.295525\n",
      "Train epoch: 12 [30720/60000(51%)]\t Loss: 66.408249\n",
      "Train epoch: 12 [32000/60000(53%)]\t Loss: 63.150963\n",
      "Train epoch: 12 [33280/60000(55%)]\t Loss: 67.586540\n",
      "Train epoch: 12 [34560/60000(58%)]\t Loss: 66.436806\n",
      "Train epoch: 12 [35840/60000(60%)]\t Loss: 67.402718\n",
      "Train epoch: 12 [37120/60000(62%)]\t Loss: 67.158234\n",
      "Train epoch: 12 [38400/60000(64%)]\t Loss: 69.322685\n",
      "Train epoch: 12 [39680/60000(66%)]\t Loss: 67.320679\n",
      "Train epoch: 12 [40960/60000(68%)]\t Loss: 71.650490\n",
      "Train epoch: 12 [42240/60000(70%)]\t Loss: 66.568680\n",
      "Train epoch: 12 [43520/60000(72%)]\t Loss: 67.331573\n",
      "Train epoch: 12 [44800/60000(75%)]\t Loss: 67.868881\n",
      "Train epoch: 12 [46080/60000(77%)]\t Loss: 69.772194\n",
      "Train epoch: 12 [47360/60000(79%)]\t Loss: 69.093575\n",
      "Train epoch: 12 [48640/60000(81%)]\t Loss: 65.953781\n",
      "Train epoch: 12 [49920/60000(83%)]\t Loss: 66.292236\n",
      "Train epoch: 12 [51200/60000(85%)]\t Loss: 67.131416\n",
      "Train epoch: 12 [52480/60000(87%)]\t Loss: 70.416862\n",
      "Train epoch: 12 [53760/60000(90%)]\t Loss: 70.722443\n",
      "Train epoch: 12 [55040/60000(92%)]\t Loss: 67.735146\n",
      "Train epoch: 12 [56320/60000(94%)]\t Loss: 67.603882\n",
      "Train epoch: 12 [57600/60000(96%)]\t Loss: 69.017097\n",
      "Train epoch: 12 [58880/60000(98%)]\t Loss: 68.306969\n",
      "====> Epoch: 12 Average loss: 68.0868\n",
      "Train epoch: 13 [0/60000(0%)]\t Loss: 67.432152\n",
      "Train epoch: 13 [1280/60000(2%)]\t Loss: 65.309830\n",
      "Train epoch: 13 [2560/60000(4%)]\t Loss: 67.194550\n",
      "Train epoch: 13 [3840/60000(6%)]\t Loss: 64.928894\n",
      "Train epoch: 13 [5120/60000(9%)]\t Loss: 67.673660\n",
      "Train epoch: 13 [6400/60000(11%)]\t Loss: 67.224937\n",
      "Train epoch: 13 [7680/60000(13%)]\t Loss: 66.267342\n",
      "Train epoch: 13 [8960/60000(15%)]\t Loss: 67.451889\n",
      "Train epoch: 13 [10240/60000(17%)]\t Loss: 71.021606\n",
      "Train epoch: 13 [11520/60000(19%)]\t Loss: 67.693779\n",
      "Train epoch: 13 [12800/60000(21%)]\t Loss: 66.229088\n",
      "Train epoch: 13 [14080/60000(23%)]\t Loss: 66.921623\n",
      "Train epoch: 13 [15360/60000(26%)]\t Loss: 66.162842\n",
      "Train epoch: 13 [16640/60000(28%)]\t Loss: 65.181458\n",
      "Train epoch: 13 [17920/60000(30%)]\t Loss: 66.582573\n",
      "Train epoch: 13 [19200/60000(32%)]\t Loss: 67.004349\n",
      "Train epoch: 13 [20480/60000(34%)]\t Loss: 65.525864\n",
      "Train epoch: 13 [21760/60000(36%)]\t Loss: 67.367470\n",
      "Train epoch: 13 [23040/60000(38%)]\t Loss: 66.716240\n",
      "Train epoch: 13 [24320/60000(41%)]\t Loss: 67.458771\n",
      "Train epoch: 13 [25600/60000(43%)]\t Loss: 65.002586\n",
      "Train epoch: 13 [26880/60000(45%)]\t Loss: 68.214081\n",
      "Train epoch: 13 [28160/60000(47%)]\t Loss: 65.941711\n",
      "Train epoch: 13 [29440/60000(49%)]\t Loss: 64.759514\n",
      "Train epoch: 13 [30720/60000(51%)]\t Loss: 67.815399\n",
      "Train epoch: 13 [32000/60000(53%)]\t Loss: 65.626190\n",
      "Train epoch: 13 [33280/60000(55%)]\t Loss: 68.645126\n",
      "Train epoch: 13 [34560/60000(58%)]\t Loss: 68.735802\n",
      "Train epoch: 13 [35840/60000(60%)]\t Loss: 63.690750\n",
      "Train epoch: 13 [37120/60000(62%)]\t Loss: 65.774994\n",
      "Train epoch: 13 [38400/60000(64%)]\t Loss: 68.632050\n",
      "Train epoch: 13 [39680/60000(66%)]\t Loss: 66.337685\n",
      "Train epoch: 13 [40960/60000(68%)]\t Loss: 65.712692\n",
      "Train epoch: 13 [42240/60000(70%)]\t Loss: 67.237503\n",
      "Train epoch: 13 [43520/60000(72%)]\t Loss: 66.328941\n",
      "Train epoch: 13 [44800/60000(75%)]\t Loss: 63.871910\n",
      "Train epoch: 13 [46080/60000(77%)]\t Loss: 70.249245\n",
      "Train epoch: 13 [47360/60000(79%)]\t Loss: 67.173927\n",
      "Train epoch: 13 [48640/60000(81%)]\t Loss: 65.820030\n",
      "Train epoch: 13 [49920/60000(83%)]\t Loss: 64.975929\n",
      "Train epoch: 13 [51200/60000(85%)]\t Loss: 65.319046\n",
      "Train epoch: 13 [52480/60000(87%)]\t Loss: 65.579155\n",
      "Train epoch: 13 [53760/60000(90%)]\t Loss: 64.989151\n",
      "Train epoch: 13 [55040/60000(92%)]\t Loss: 66.574181\n",
      "Train epoch: 13 [56320/60000(94%)]\t Loss: 65.965332\n",
      "Train epoch: 13 [57600/60000(96%)]\t Loss: 64.909027\n",
      "Train epoch: 13 [58880/60000(98%)]\t Loss: 64.014763\n",
      "====> Epoch: 13 Average loss: 66.6253\n",
      "Train epoch: 14 [0/60000(0%)]\t Loss: 64.914162\n",
      "Train epoch: 14 [1280/60000(2%)]\t Loss: 67.331650\n",
      "Train epoch: 14 [2560/60000(4%)]\t Loss: 67.884003\n",
      "Train epoch: 14 [3840/60000(6%)]\t Loss: 66.201553\n",
      "Train epoch: 14 [5120/60000(9%)]\t Loss: 65.925537\n",
      "Train epoch: 14 [6400/60000(11%)]\t Loss: 67.877693\n",
      "Train epoch: 14 [7680/60000(13%)]\t Loss: 65.159142\n",
      "Train epoch: 14 [8960/60000(15%)]\t Loss: 66.272743\n",
      "Train epoch: 14 [10240/60000(17%)]\t Loss: 67.459747\n",
      "Train epoch: 14 [11520/60000(19%)]\t Loss: 63.645714\n",
      "Train epoch: 14 [12800/60000(21%)]\t Loss: 68.700905\n",
      "Train epoch: 14 [14080/60000(23%)]\t Loss: 66.976006\n",
      "Train epoch: 14 [15360/60000(26%)]\t Loss: 67.618988\n",
      "Train epoch: 14 [16640/60000(28%)]\t Loss: 63.465572\n",
      "Train epoch: 14 [17920/60000(30%)]\t Loss: 64.634628\n",
      "Train epoch: 14 [19200/60000(32%)]\t Loss: 65.167336\n",
      "Train epoch: 14 [20480/60000(34%)]\t Loss: 65.488220\n",
      "Train epoch: 14 [21760/60000(36%)]\t Loss: 66.605721\n",
      "Train epoch: 14 [23040/60000(38%)]\t Loss: 64.637733\n",
      "Train epoch: 14 [24320/60000(41%)]\t Loss: 65.876480\n",
      "Train epoch: 14 [25600/60000(43%)]\t Loss: 66.468826\n",
      "Train epoch: 14 [26880/60000(45%)]\t Loss: 62.688152\n",
      "Train epoch: 14 [28160/60000(47%)]\t Loss: 65.186684\n",
      "Train epoch: 14 [29440/60000(49%)]\t Loss: 64.660126\n",
      "Train epoch: 14 [30720/60000(51%)]\t Loss: 66.241867\n",
      "Train epoch: 14 [32000/60000(53%)]\t Loss: 62.452026\n",
      "Train epoch: 14 [33280/60000(55%)]\t Loss: 67.187553\n",
      "Train epoch: 14 [34560/60000(58%)]\t Loss: 66.766640\n",
      "Train epoch: 14 [35840/60000(60%)]\t Loss: 63.938126\n",
      "Train epoch: 14 [37120/60000(62%)]\t Loss: 63.696156\n",
      "Train epoch: 14 [38400/60000(64%)]\t Loss: 66.250839\n",
      "Train epoch: 14 [39680/60000(66%)]\t Loss: 65.335686\n",
      "Train epoch: 14 [40960/60000(68%)]\t Loss: 65.100487\n",
      "Train epoch: 14 [42240/60000(70%)]\t Loss: 65.711617\n",
      "Train epoch: 14 [43520/60000(72%)]\t Loss: 64.356453\n",
      "Train epoch: 14 [44800/60000(75%)]\t Loss: 64.567841\n",
      "Train epoch: 14 [46080/60000(77%)]\t Loss: 65.165497\n",
      "Train epoch: 14 [47360/60000(79%)]\t Loss: 64.865257\n",
      "Train epoch: 14 [48640/60000(81%)]\t Loss: 65.207169\n",
      "Train epoch: 14 [49920/60000(83%)]\t Loss: 65.841660\n",
      "Train epoch: 14 [51200/60000(85%)]\t Loss: 64.538582\n",
      "Train epoch: 14 [52480/60000(87%)]\t Loss: 65.090874\n",
      "Train epoch: 14 [53760/60000(90%)]\t Loss: 63.264584\n",
      "Train epoch: 14 [55040/60000(92%)]\t Loss: 64.647964\n",
      "Train epoch: 14 [56320/60000(94%)]\t Loss: 65.285187\n",
      "Train epoch: 14 [57600/60000(96%)]\t Loss: 65.067459\n",
      "Train epoch: 14 [58880/60000(98%)]\t Loss: 63.058365\n",
      "====> Epoch: 14 Average loss: 65.3576\n",
      "Train epoch: 15 [0/60000(0%)]\t Loss: 64.164635\n",
      "Train epoch: 15 [1280/60000(2%)]\t Loss: 65.587791\n",
      "Train epoch: 15 [2560/60000(4%)]\t Loss: 66.343269\n",
      "Train epoch: 15 [3840/60000(6%)]\t Loss: 67.134811\n",
      "Train epoch: 15 [5120/60000(9%)]\t Loss: 62.915737\n",
      "Train epoch: 15 [6400/60000(11%)]\t Loss: 63.869640\n",
      "Train epoch: 15 [7680/60000(13%)]\t Loss: 64.338425\n",
      "Train epoch: 15 [8960/60000(15%)]\t Loss: 64.476196\n",
      "Train epoch: 15 [10240/60000(17%)]\t Loss: 64.739113\n",
      "Train epoch: 15 [11520/60000(19%)]\t Loss: 66.653664\n",
      "Train epoch: 15 [12800/60000(21%)]\t Loss: 65.641586\n",
      "Train epoch: 15 [14080/60000(23%)]\t Loss: 63.720371\n",
      "Train epoch: 15 [15360/60000(26%)]\t Loss: 63.653149\n",
      "Train epoch: 15 [16640/60000(28%)]\t Loss: 61.579258\n",
      "Train epoch: 15 [17920/60000(30%)]\t Loss: 65.063545\n",
      "Train epoch: 15 [19200/60000(32%)]\t Loss: 65.772507\n",
      "Train epoch: 15 [20480/60000(34%)]\t Loss: 64.066277\n",
      "Train epoch: 15 [21760/60000(36%)]\t Loss: 66.125038\n",
      "Train epoch: 15 [23040/60000(38%)]\t Loss: 66.103065\n",
      "Train epoch: 15 [24320/60000(41%)]\t Loss: 65.803772\n",
      "Train epoch: 15 [25600/60000(43%)]\t Loss: 66.332367\n",
      "Train epoch: 15 [26880/60000(45%)]\t Loss: 66.975784\n",
      "Train epoch: 15 [28160/60000(47%)]\t Loss: 64.386902\n",
      "Train epoch: 15 [29440/60000(49%)]\t Loss: 67.280487\n",
      "Train epoch: 15 [30720/60000(51%)]\t Loss: 65.481781\n",
      "Train epoch: 15 [32000/60000(53%)]\t Loss: 63.573093\n",
      "Train epoch: 15 [33280/60000(55%)]\t Loss: 66.213806\n",
      "Train epoch: 15 [34560/60000(58%)]\t Loss: 65.636101\n",
      "Train epoch: 15 [35840/60000(60%)]\t Loss: 62.815536\n",
      "Train epoch: 15 [37120/60000(62%)]\t Loss: 62.681019\n",
      "Train epoch: 15 [38400/60000(64%)]\t Loss: 64.858360\n",
      "Train epoch: 15 [39680/60000(66%)]\t Loss: 67.178787\n",
      "Train epoch: 15 [40960/60000(68%)]\t Loss: 66.234077\n",
      "Train epoch: 15 [42240/60000(70%)]\t Loss: 66.101227\n",
      "Train epoch: 15 [43520/60000(72%)]\t Loss: 62.380508\n",
      "Train epoch: 15 [44800/60000(75%)]\t Loss: 66.623749\n",
      "Train epoch: 15 [46080/60000(77%)]\t Loss: 63.784821\n",
      "Train epoch: 15 [47360/60000(79%)]\t Loss: 63.274014\n",
      "Train epoch: 15 [48640/60000(81%)]\t Loss: 63.213535\n",
      "Train epoch: 15 [49920/60000(83%)]\t Loss: 61.846138\n",
      "Train epoch: 15 [51200/60000(85%)]\t Loss: 63.983643\n",
      "Train epoch: 15 [52480/60000(87%)]\t Loss: 62.689533\n",
      "Train epoch: 15 [53760/60000(90%)]\t Loss: 62.582233\n",
      "Train epoch: 15 [55040/60000(92%)]\t Loss: 64.023163\n",
      "Train epoch: 15 [56320/60000(94%)]\t Loss: 66.232628\n",
      "Train epoch: 15 [57600/60000(96%)]\t Loss: 61.761639\n",
      "Train epoch: 15 [58880/60000(98%)]\t Loss: 61.072044\n",
      "====> Epoch: 15 Average loss: 64.2546\n",
      "Train epoch: 16 [0/60000(0%)]\t Loss: 61.845558\n",
      "Train epoch: 16 [1280/60000(2%)]\t Loss: 64.149345\n",
      "Train epoch: 16 [2560/60000(4%)]\t Loss: 64.691162\n",
      "Train epoch: 16 [3840/60000(6%)]\t Loss: 64.260620\n",
      "Train epoch: 16 [5120/60000(9%)]\t Loss: 64.283348\n",
      "Train epoch: 16 [6400/60000(11%)]\t Loss: 64.691986\n",
      "Train epoch: 16 [7680/60000(13%)]\t Loss: 64.521820\n",
      "Train epoch: 16 [8960/60000(15%)]\t Loss: 62.408127\n",
      "Train epoch: 16 [10240/60000(17%)]\t Loss: 67.114067\n",
      "Train epoch: 16 [11520/60000(19%)]\t Loss: 63.536469\n",
      "Train epoch: 16 [12800/60000(21%)]\t Loss: 65.743843\n",
      "Train epoch: 16 [14080/60000(23%)]\t Loss: 64.626022\n",
      "Train epoch: 16 [15360/60000(26%)]\t Loss: 64.078735\n",
      "Train epoch: 16 [16640/60000(28%)]\t Loss: 65.189476\n",
      "Train epoch: 16 [17920/60000(30%)]\t Loss: 63.468639\n",
      "Train epoch: 16 [19200/60000(32%)]\t Loss: 64.130447\n",
      "Train epoch: 16 [20480/60000(34%)]\t Loss: 61.034348\n",
      "Train epoch: 16 [21760/60000(36%)]\t Loss: 63.943287\n",
      "Train epoch: 16 [23040/60000(38%)]\t Loss: 65.178589\n",
      "Train epoch: 16 [24320/60000(41%)]\t Loss: 61.905910\n",
      "Train epoch: 16 [25600/60000(43%)]\t Loss: 62.723427\n",
      "Train epoch: 16 [26880/60000(45%)]\t Loss: 62.740940\n",
      "Train epoch: 16 [28160/60000(47%)]\t Loss: 63.628788\n",
      "Train epoch: 16 [29440/60000(49%)]\t Loss: 60.722504\n",
      "Train epoch: 16 [30720/60000(51%)]\t Loss: 60.749454\n",
      "Train epoch: 16 [32000/60000(53%)]\t Loss: 61.820766\n",
      "Train epoch: 16 [33280/60000(55%)]\t Loss: 63.347519\n",
      "Train epoch: 16 [34560/60000(58%)]\t Loss: 61.660858\n",
      "Train epoch: 16 [35840/60000(60%)]\t Loss: 62.877735\n",
      "Train epoch: 16 [37120/60000(62%)]\t Loss: 63.728130\n",
      "Train epoch: 16 [38400/60000(64%)]\t Loss: 64.324364\n",
      "Train epoch: 16 [39680/60000(66%)]\t Loss: 62.168034\n",
      "Train epoch: 16 [40960/60000(68%)]\t Loss: 64.487129\n",
      "Train epoch: 16 [42240/60000(70%)]\t Loss: 63.570835\n",
      "Train epoch: 16 [43520/60000(72%)]\t Loss: 64.514267\n",
      "Train epoch: 16 [44800/60000(75%)]\t Loss: 63.983334\n",
      "Train epoch: 16 [46080/60000(77%)]\t Loss: 62.616642\n",
      "Train epoch: 16 [47360/60000(79%)]\t Loss: 64.092720\n",
      "Train epoch: 16 [48640/60000(81%)]\t Loss: 63.734386\n",
      "Train epoch: 16 [49920/60000(83%)]\t Loss: 62.765461\n",
      "Train epoch: 16 [51200/60000(85%)]\t Loss: 63.505379\n",
      "Train epoch: 16 [52480/60000(87%)]\t Loss: 65.289703\n",
      "Train epoch: 16 [53760/60000(90%)]\t Loss: 63.406391\n",
      "Train epoch: 16 [55040/60000(92%)]\t Loss: 61.396622\n",
      "Train epoch: 16 [56320/60000(94%)]\t Loss: 61.748486\n",
      "Train epoch: 16 [57600/60000(96%)]\t Loss: 60.741100\n",
      "Train epoch: 16 [58880/60000(98%)]\t Loss: 60.853531\n",
      "====> Epoch: 16 Average loss: 63.2894\n",
      "Train epoch: 17 [0/60000(0%)]\t Loss: 64.044815\n",
      "Train epoch: 17 [1280/60000(2%)]\t Loss: 64.596291\n",
      "Train epoch: 17 [2560/60000(4%)]\t Loss: 62.848427\n",
      "Train epoch: 17 [3840/60000(6%)]\t Loss: 62.716320\n",
      "Train epoch: 17 [5120/60000(9%)]\t Loss: 63.591545\n",
      "Train epoch: 17 [6400/60000(11%)]\t Loss: 62.540478\n",
      "Train epoch: 17 [7680/60000(13%)]\t Loss: 65.240227\n",
      "Train epoch: 17 [8960/60000(15%)]\t Loss: 64.593964\n",
      "Train epoch: 17 [10240/60000(17%)]\t Loss: 61.874859\n",
      "Train epoch: 17 [11520/60000(19%)]\t Loss: 62.960381\n",
      "Train epoch: 17 [12800/60000(21%)]\t Loss: 61.732006\n",
      "Train epoch: 17 [14080/60000(23%)]\t Loss: 63.614990\n",
      "Train epoch: 17 [15360/60000(26%)]\t Loss: 60.755383\n",
      "Train epoch: 17 [16640/60000(28%)]\t Loss: 63.450668\n",
      "Train epoch: 17 [17920/60000(30%)]\t Loss: 61.625652\n",
      "Train epoch: 17 [19200/60000(32%)]\t Loss: 61.600014\n",
      "Train epoch: 17 [20480/60000(34%)]\t Loss: 64.483322\n",
      "Train epoch: 17 [21760/60000(36%)]\t Loss: 61.746223\n",
      "Train epoch: 17 [23040/60000(38%)]\t Loss: 61.219967\n",
      "Train epoch: 17 [24320/60000(41%)]\t Loss: 62.055771\n",
      "Train epoch: 17 [25600/60000(43%)]\t Loss: 63.471344\n",
      "Train epoch: 17 [26880/60000(45%)]\t Loss: 60.140034\n",
      "Train epoch: 17 [28160/60000(47%)]\t Loss: 65.915390\n",
      "Train epoch: 17 [29440/60000(49%)]\t Loss: 63.834309\n",
      "Train epoch: 17 [30720/60000(51%)]\t Loss: 63.583866\n",
      "Train epoch: 17 [32000/60000(53%)]\t Loss: 62.585609\n",
      "Train epoch: 17 [33280/60000(55%)]\t Loss: 63.393234\n",
      "Train epoch: 17 [34560/60000(58%)]\t Loss: 63.015659\n",
      "Train epoch: 17 [35840/60000(60%)]\t Loss: 64.579956\n",
      "Train epoch: 17 [37120/60000(62%)]\t Loss: 64.133423\n",
      "Train epoch: 17 [38400/60000(64%)]\t Loss: 61.057320\n",
      "Train epoch: 17 [39680/60000(66%)]\t Loss: 63.269402\n",
      "Train epoch: 17 [40960/60000(68%)]\t Loss: 63.857655\n",
      "Train epoch: 17 [42240/60000(70%)]\t Loss: 63.000229\n",
      "Train epoch: 17 [43520/60000(72%)]\t Loss: 60.688137\n",
      "Train epoch: 17 [44800/60000(75%)]\t Loss: 62.621532\n",
      "Train epoch: 17 [46080/60000(77%)]\t Loss: 63.922054\n",
      "Train epoch: 17 [47360/60000(79%)]\t Loss: 60.817493\n",
      "Train epoch: 17 [48640/60000(81%)]\t Loss: 60.027252\n",
      "Train epoch: 17 [49920/60000(83%)]\t Loss: 60.428383\n",
      "Train epoch: 17 [51200/60000(85%)]\t Loss: 63.755756\n",
      "Train epoch: 17 [52480/60000(87%)]\t Loss: 60.223431\n",
      "Train epoch: 17 [53760/60000(90%)]\t Loss: 62.288868\n",
      "Train epoch: 17 [55040/60000(92%)]\t Loss: 62.045349\n",
      "Train epoch: 17 [56320/60000(94%)]\t Loss: 61.614906\n",
      "Train epoch: 17 [57600/60000(96%)]\t Loss: 62.245163\n",
      "Train epoch: 17 [58880/60000(98%)]\t Loss: 60.438625\n",
      "====> Epoch: 17 Average loss: 62.4394\n",
      "Train epoch: 18 [0/60000(0%)]\t Loss: 61.196915\n",
      "Train epoch: 18 [1280/60000(2%)]\t Loss: 58.861473\n",
      "Train epoch: 18 [2560/60000(4%)]\t Loss: 60.151844\n",
      "Train epoch: 18 [3840/60000(6%)]\t Loss: 64.467842\n",
      "Train epoch: 18 [5120/60000(9%)]\t Loss: 59.362267\n",
      "Train epoch: 18 [6400/60000(11%)]\t Loss: 59.486240\n",
      "Train epoch: 18 [7680/60000(13%)]\t Loss: 61.052612\n",
      "Train epoch: 18 [8960/60000(15%)]\t Loss: 62.170464\n",
      "Train epoch: 18 [10240/60000(17%)]\t Loss: 64.694817\n",
      "Train epoch: 18 [11520/60000(19%)]\t Loss: 61.938480\n",
      "Train epoch: 18 [12800/60000(21%)]\t Loss: 61.552040\n",
      "Train epoch: 18 [14080/60000(23%)]\t Loss: 62.709629\n",
      "Train epoch: 18 [15360/60000(26%)]\t Loss: 63.438347\n",
      "Train epoch: 18 [16640/60000(28%)]\t Loss: 63.209736\n",
      "Train epoch: 18 [17920/60000(30%)]\t Loss: 62.106377\n",
      "Train epoch: 18 [19200/60000(32%)]\t Loss: 60.295567\n",
      "Train epoch: 18 [20480/60000(34%)]\t Loss: 61.167576\n",
      "Train epoch: 18 [21760/60000(36%)]\t Loss: 64.335861\n",
      "Train epoch: 18 [23040/60000(38%)]\t Loss: 61.926815\n",
      "Train epoch: 18 [24320/60000(41%)]\t Loss: 60.033710\n",
      "Train epoch: 18 [25600/60000(43%)]\t Loss: 60.387691\n",
      "Train epoch: 18 [26880/60000(45%)]\t Loss: 61.510033\n",
      "Train epoch: 18 [28160/60000(47%)]\t Loss: 59.583591\n",
      "Train epoch: 18 [29440/60000(49%)]\t Loss: 61.797199\n",
      "Train epoch: 18 [30720/60000(51%)]\t Loss: 60.915489\n",
      "Train epoch: 18 [32000/60000(53%)]\t Loss: 62.707237\n",
      "Train epoch: 18 [33280/60000(55%)]\t Loss: 60.726624\n",
      "Train epoch: 18 [34560/60000(58%)]\t Loss: 60.527771\n",
      "Train epoch: 18 [35840/60000(60%)]\t Loss: 61.147770\n",
      "Train epoch: 18 [37120/60000(62%)]\t Loss: 60.517761\n",
      "Train epoch: 18 [38400/60000(64%)]\t Loss: 60.886005\n",
      "Train epoch: 18 [39680/60000(66%)]\t Loss: 62.222538\n",
      "Train epoch: 18 [40960/60000(68%)]\t Loss: 61.858074\n",
      "Train epoch: 18 [42240/60000(70%)]\t Loss: 61.750904\n",
      "Train epoch: 18 [43520/60000(72%)]\t Loss: 61.192211\n",
      "Train epoch: 18 [44800/60000(75%)]\t Loss: 60.372242\n",
      "Train epoch: 18 [46080/60000(77%)]\t Loss: 59.864292\n",
      "Train epoch: 18 [47360/60000(79%)]\t Loss: 61.808361\n",
      "Train epoch: 18 [48640/60000(81%)]\t Loss: 60.951111\n",
      "Train epoch: 18 [49920/60000(83%)]\t Loss: 60.713490\n",
      "Train epoch: 18 [51200/60000(85%)]\t Loss: 59.901123\n",
      "Train epoch: 18 [52480/60000(87%)]\t Loss: 62.221859\n",
      "Train epoch: 18 [53760/60000(90%)]\t Loss: 61.803673\n",
      "Train epoch: 18 [55040/60000(92%)]\t Loss: 61.234646\n",
      "Train epoch: 18 [56320/60000(94%)]\t Loss: 62.866585\n",
      "Train epoch: 18 [57600/60000(96%)]\t Loss: 60.641712\n",
      "Train epoch: 18 [58880/60000(98%)]\t Loss: 58.439034\n",
      "====> Epoch: 18 Average loss: 61.6908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "# import pdb\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "batch_size=128 #'input batch size for training (default: 64)')\n",
    "epochs=19#'number of epochs to train (default: 2)')\n",
    "seed=1\n",
    "log_interval=10 #'how many batches to wait before logging training status')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "lam = 1e-4\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 256, bias = False) # Encoder\n",
    "        self.fc2 = nn.Linear(256, 784, bias = False) # Decoder\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h1 = self.relu(self.fc1(x.view(-1, 784)))\n",
    "        return h1\n",
    "\n",
    "    def decoder(self,z):\n",
    "        h2 = self.sigmoid(self.fc2(z))\n",
    "        return h2\n",
    "\n",
    "    def forward(self, x):\n",
    "            h1 = self.encoder(x)\n",
    "            h2 = self.decoder(h1)\n",
    "            return h1, h2\n",
    "\n",
    "        # Writing data in a grid to check the quality and progress\n",
    "    def samples_write(self, x, epoch):\n",
    "        _, samples = self.forward(x)\n",
    "        #pdb.set_trace()\n",
    "        samples = samples.data.cpu().numpy()[:16]\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "        plt.savefig('out/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        #self.c += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "mse_loss = nn.BCELoss(size_average = False)\n",
    "\n",
    "def loss_function(W, x, recons_x, h,  lam=0.0001):\n",
    "    \"\"\"Compute the Contractive AutoEncoder Loss\n",
    "    Evalutes the CAE loss, which is composed as the summation of a Mean\n",
    "    Squared Error and the weighted l2-norm of the Jacobian of the hidden\n",
    "    units with respect to the inputs.\n",
    "    See reference below for an in-depth discussion:\n",
    "      #1: http://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder\n",
    "    Args:\n",
    "        `W` (FloatTensor): (N_hidden x N), where N_hidden and N are the\n",
    "          dimensions of the hidden units and input respectively.\n",
    "        `x` (Variable): the input to the network, with dims (N_batch x N)\n",
    "        recons_x (Variable): the reconstruction of the input, with dims\n",
    "          N_batch x N.\n",
    "        `h` (Variable): the hidden units of the network, with dims\n",
    "          batch_size x N_hidden\n",
    "        `lam` (float): the weight given to the jacobian regulariser term\n",
    "    Returns:\n",
    "        Variable: the (scalar) CAE loss\n",
    "    \"\"\"\n",
    "    mse = mse_loss(recons_x, x)\n",
    "    # Since: W is shape of N_hidden x N. So, we do not need to transpose it as\n",
    "    # opposed to #1\n",
    "    dh = h * (1 - h) # Hadamard product produces size N_batch x N_hidden\n",
    "    # Sum through the input dimension to improve efficiency, as suggested in #1\n",
    "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
    "    # unsqueeze to avoid issues with torch.mv\n",
    "    w_sum = w_sum.unsqueeze(1) # shape N_hidden x 1\n",
    "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "    return mse + contractive_loss.mul_( lam)\n",
    "\n",
    "\n",
    "model = CAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_representation, recons_x = model(data)\n",
    "\n",
    "        # Get the weights\n",
    "        # model.state_dict().keys()\n",
    "        # change the key by seeing the keys manually.\n",
    "        # (In future I will try to make it automatic)\n",
    "        W = model.state_dict()['fc1.weight']\n",
    "        loss = loss_function(W, data.view(-1, 784), recons_x,\n",
    "                             hidden_representation,  0.0001)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            print('Train epoch: {} [{}/{}({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                  epoch, idx*len(data), len(train_loader.dataset),\n",
    "                  100*idx/len(train_loader),\n",
    "                  loss.data[0]/len(data)))\n",
    "\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "         epoch, train_loss / len(train_loader.dataset)))\n",
    "    model.samples_write(data,epoch)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d006a383-f4ee-4c5c-b934-1f83b05a07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([256, 784])\n",
      "fc2.weight \t torch.Size([784, 256])\n",
      "Optimizer's state_dict:\n",
      "state \t {0: {'step': 8911, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}, 1: {'step': 8911, 'exp_avg': tensor([[0.0016, 0.0011, 0.0009,  ..., 0.0017, 0.0012, 0.0016],\n",
      "        [0.0011, 0.0008, 0.0006,  ..., 0.0011, 0.0008, 0.0011],\n",
      "        [0.0011, 0.0010, 0.0007,  ..., 0.0012, 0.0010, 0.0013],\n",
      "        ...,\n",
      "        [0.0016, 0.0014, 0.0010,  ..., 0.0018, 0.0015, 0.0021],\n",
      "        [0.0010, 0.0008, 0.0006,  ..., 0.0011, 0.0008, 0.0010],\n",
      "        [0.0013, 0.0011, 0.0007,  ..., 0.0015, 0.0012, 0.0015]]), 'exp_avg_sq': tensor([[0.0026, 0.0041, 0.0088,  ..., 0.0067, 0.0010, 0.0127],\n",
      "        [0.0009, 0.0015, 0.0039,  ..., 0.0027, 0.0003, 0.0061],\n",
      "        [0.0013, 0.0022, 0.0054,  ..., 0.0039, 0.0004, 0.0084],\n",
      "        ...,\n",
      "        [0.0052, 0.0079, 0.0149,  ..., 0.0116, 0.0020, 0.0209],\n",
      "        [0.0008, 0.0014, 0.0038,  ..., 0.0026, 0.0003, 0.0061],\n",
      "        [0.0019, 0.0030, 0.0067,  ..., 0.0051, 0.0007, 0.0099]])}}\n",
      "param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032b67c8-6ebb-4f96-8faa-942b9ecd176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/crns/Desktop/PFE/PFE/Code/Self-Net/ICLR 2019/models/CAE.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b816197-0296-41f2-a3db-7d6f8454cbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
