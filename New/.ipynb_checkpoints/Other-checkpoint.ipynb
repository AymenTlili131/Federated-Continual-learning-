{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00197fa-36ea-4189-8b79-60aa13f622f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for all combinations\n",
    "for combination in ListExperiences_:\n",
    "    G.add_node(tuple(combination))\n",
    "\n",
    "# Add edges for hierarchical dependency\n",
    "for i, combination in enumerate(ListExperiences_):\n",
    "    for j in range(i + 1, len(ListExperiences_)):\n",
    "        if set(ListExperiences_[i]).issubset(ListExperiences_[j]):\n",
    "            G.add_edge(tuple(ListExperiences_[i]), tuple(ListExperiences_[j]))\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Position nodes using a spring layout algorithm\n",
    "nx.draw(G, pos, with_labels=True, node_size=50, node_color='lightblue', font_size=8, font_weight='bold')\n",
    "plt.title('Hierarchical Dependency between Combinations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86910a0-deb5-404b-af24-0c157e7c9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"zoo.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430f97c-fd24-4d5d-944f-708e9aa7e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolutionLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, adjacency_matrix, node_features):\n",
    "        support = torch.matmul(node_features, self.weight)\n",
    "        output = torch.matmul(adjacency_matrix, support) + self.bias\n",
    "        return output\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer1 = GraphConvolutionLayer(input_features, hidden_features)\n",
    "        self.layer2 = GraphConvolutionLayer(hidden_features, output_classes)\n",
    "\n",
    "    def forward(self, adjacency_matrix, node_features):\n",
    "        h1 = F.relu(self.layer1(adjacency_matrix, node_features))\n",
    "        output = self.layer2(adjacency_matrix, h1)\n",
    "        return F.log_softmax(output, dim=1)  # Using log_softmax for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3dbbb-fa50-4da3-a25d-32f403382862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881f119-0bae-46ba-9906-1e3702221bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4045cec-3044-41c0-bbc2-0e9e032ef7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e05b35-bb1c-43c8-8aac-5e070ca5b7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdbc4c-987c-495e-99d5-78b5750ef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_=[k for k in ListExperiences_ if k not in ListExperiences] \n",
    "print(\"Missing elements:\", L2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cbe015-a8d4-44f3-abce-fdeeb44e2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "exper=os.listdir(\"checkpoints/\")\n",
    "missing=0\n",
    "L_missing=[]\n",
    "L_missingL=[]\n",
    "for folder in exper:\n",
    "    if len(os.listdir(\"checkpoints/{}\".format(folder)))<3 and (\"gelu\" not in os.listdir(\"checkpoints/{}\".format(folder))) :\n",
    "        missing=missing+1\n",
    "        L_missing.append(folder)\n",
    "for folder in L_missing :       \n",
    "    L_class=[]\n",
    "    for classe in folder[1:-1].split(\",\"):\n",
    "        L_class.append(int(classe))\n",
    "    L_missingL.append(L_class)           \n",
    "len(L_missingL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a81cd-2123-4752-872b-ad8926fc99a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf60d1-1d5a-486a-95b9-335c30901114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "L_Datasets=[]\n",
    "L_activations=[\"gelu\",\"relu\"]#,\"silu\",\"leakyrelu\",\"sigmoid\",\"tanh\"]\n",
    "for activ in L_activations: \n",
    "    dsa=torch.load(f'train_dataset_{activ}.pt')\n",
    "    L_Datasets.append(dsa)\n",
    "train_dsab_cat = ConcatDataset(L_Datasets)\n",
    "train_loader = DataLoader(train_dsab_cat,batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fab368-23ba-4ab1-ba90-456dc3632069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fcf1d-2213-444b-8667-db202b28e64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88938e00-4c83-4081-9230-1795cead1c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1332-d578-4857-8453-32d0c36db405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db714871-10ca-4545-ba23-12bfb57e3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import pandas as pd\n",
    "\n",
    "# Define the number of components for MDS\n",
    "n_components = 48\n",
    "\n",
    "# Apply MDS for dimensionality reduction\n",
    "mds_tr = MDS(n_components=n_components)\n",
    "train_reduced_data = mds_tr.fit_transform(train_labels)\n",
    "test_reduced_data = mds_ts.fit_transform(test_labels)\n",
    "\n",
    "train_reduced_data.shape,test_reduced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a866a-563c-4f0b-8362-e3151fcc803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = train_features  # Example 1D vectors of length 2464\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Compute persistent homology using scikit-tda\n",
    "diagrams = ripser(data, maxdim=48)['dgms']\n",
    "\n",
    "diagrams[0][-1]=diagrams[0][-2]\n",
    "#print(diagrams)\n",
    "# Visualize the persistent homology diagrams\n",
    "plot_diagrams(diagrams, show=True)\n",
    "\n",
    "# Extract features from the persistent homology diagram\n",
    "features = np.array([np.mean(diagram[:, 1] - diagram[:, 0]) for diagram in diagrams])\n",
    "\n",
    "# Reshape the features array to make it 2D\n",
    "features = features.reshape(-1, 1)\n",
    "\n",
    "# Normalize the features (optional)\n",
    "features = (features - np.mean(features)) / np.std(features)\n",
    "\n",
    "# Convert features to PyTorch tensor\n",
    "features_tensor = torch.FloatTensor(features)\n",
    "\n",
    "# Instantiate the autoencoder\n",
    "input_size = features.shape[1]\n",
    "encoding_size = 48  # Choose an appropriate encoding size\n",
    "autoencoder = Autoencoder(input_size, encoding_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    reconstructed_data = autoencoder(features_tensor)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(reconstructed_data, features_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print or log the loss for monitoring training progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Visualize the results\n",
    "with torch.no_grad():\n",
    "    encoded_data = autoencoder.encoder(features_tensor).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333e80a-f4fc-4cc6-b32e-50462e65c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "  # Assuming the first 10 columns are labels\n",
    " # Assuming the rest are features\n",
    "\n",
    "# Create an adjacency matrix based on the labels\n",
    "train_adjacency_matrix = np.dot(train_features, train_features.T)\n",
    "\n",
    "train_graph = nx.Graph(train_adjacency_matrix)\n",
    "\n",
    "# Add node features to the graph\n",
    "for node_index, feature_vector in enumerate(train_features):\n",
    "    train_graph.nodes[node_index]['features'] = train_reduced_data\n",
    "    \n",
    "    \n",
    "test_adjacency_matrix = np.dot(test_features, test_features.T)\n",
    "\n",
    "test_graph = nx.Graph(test_adjacency_matrix)\n",
    "\n",
    "# Add node features to the graph\n",
    "for node_index, feature_vector in enumerate(test_features):\n",
    "    test_graph.nodes[node_index]['features'] = test_reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe70039-6bcb-4237-9d06-2099b1311e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_size = 20 # Set the size of nodes\n",
    "label_font = {'fontweight': 'bold', 'fontsize': 8}  # Set label font properties\n",
    "\n",
    "# Visualize the graph\n",
    "pos = nx.spring_layout(graph)\n",
    "fig, ax = plt.subplots(figsize=(80, 60))  # Set the figure size\n",
    "nx.draw(graph, pos, with_labels=False, node_size=node_size, ax=ax)\n",
    "nx.draw_networkx_labels(graph, pos, font_size=8, font_family=\"sans-serif\", font_color=\"red\",alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f119b-8705-492a-b159-dc2d75a02367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch_geometric\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Convert NetworkX graph to PyTorch Geometric Data\n",
    "edges = np.array(train_graph.edges()).T\n",
    "train_edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "x_train = torch.tensor(train_features, dtype=torch.float)\n",
    "y_train = torch.tensor(train_reduced_data, dtype=torch.float)\n",
    "\n",
    "edges = np.array(test_graph.edges()).T\n",
    "test_edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "x_test = torch.tensor(test_features, dtype=torch.float)\n",
    "y_test = torch.tensor(test_reduced_data, dtype=torch.float)\n",
    "train_data = Data(x=x_train, edge_index=train_edge_index, y=y_train)\n",
    "test_data = Data(x=x_test, edge_index=test_edge_index, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34bced-cd66-4cdf-9872-d4fc04f1aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_features, hidden_features)\n",
    "        self.conv2 = GCNConv(hidden_features, output_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = GCN(input_features=train_features.shape[1], hidden_features=64, output_classes=train_reduced_data.shape[1])\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-3, max_lr=0.1, step_size_up=200, mode=\"triangular2\", cycle_momentum=False)\n",
    "\n",
    "data_list = [train_data]\n",
    "loader = DataLoader(data_list, batch_size=8)\n",
    "L=[]\n",
    "L_tr_mape=[]\n",
    "L_ts_mape=[]\n",
    "# Train the model\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            L.append(loss.item())\n",
    "    scheduler.step()\n",
    "    if epoch%30==0:\n",
    "        mape_train=calculate_mape(train_data.y.detach().numpy(), output.detach().numpy())\n",
    "        L_tr_mape.append(mape_train)\n",
    "        preds=model(test_data)\n",
    "        mape_test_result = calculate_mape(test_data.y.detach().numpy(), preds.detach().numpy())\n",
    "        L_ts_mape.append(mape_test_result)\n",
    "        print( f\"epoch {epoch} || 30 epoch mean MAE :{ (sum(L[-30:]) / len(L[-30:])):.2f} || last MAE {L[-1]} || train MAPE {mape_train}% || test MAPE {mape_test_result}%\"  )\n",
    "\n",
    "# You can now use the trained model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcaf636-2266-4a06-b957-1afe16af5a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe2392-e5d2-4206-bc45-08d84abad4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f0b30-2fa4-4ad3-b5d6-9569bc32b0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a83ad-2fdc-419f-bebc-52019a41c2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe279a-87e0-4182-ac1f-dd2c94d19698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
