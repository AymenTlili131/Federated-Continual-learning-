{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67987ec9-75b3-44c6-b850-c5d1d4451b3c",
   "metadata": {
    "id": "67987ec9-75b3-44c6-b850-c5d1d4451b3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%load_ext cudf.pandas\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "import cudf as pd\n",
    "import shap\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, ConcatDataset\n",
    "\n",
    "import random\n",
    "import ast\n",
    "        \n",
    "import sys\n",
    "\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "from torch.optim import Adam ,SGD ,Adadelta\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "import optuna\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation ,FFMpegWriter ,PillowWriter\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os \n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "import wandb\n",
    "\n",
    "\n",
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA magic\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "# ML tools\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from gtda.mapper.filter import Projection,Entropy,Eccentricity\n",
    "from gtda.mapper.cover import CubicalCover\n",
    "# scikit-learn method\n",
    "# giotto-tda method\n",
    "from gtda.mapper.cluster import FirstSimpleGap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='threadpoolctl')\n",
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "# Redirect stderr to null to suppress the exception messages\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "import ctypes\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress low-level warnings from C code\n",
    "libc = ctypes.CDLL(None)\n",
    "libc.prctl(15, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c973ef-a1c2-4c67-88f5-5c2cdc634f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb\n",
    "\n",
    "#print(cudf.__file__)\n",
    "#print(cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceed1bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"ab631efc36e2c87f5f54d82b5cdbd6c501d5221f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ecd06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c395ca-191a-49f4-b45c-e87ecd231e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!accelerate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8323c49d-240a-4e46-98ec-c00e2e783a63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8323c49d-240a-4e46-98ec-c00e2e783a63",
    "outputId": "9229fdc7-630d-4e87-d54b-3771391ef44c"
   },
   "outputs": [],
   "source": [
    "seed=74\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a1e1f6-dcf6-4c01-b2c3-ab25ee771a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Double_input_transformer import CustomDataset,TransformerAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655000ba-32ed-41d3-93a3-522336521239",
   "metadata": {
    "id": "655000ba-32ed-41d3-93a3-522336521239"
   },
   "source": [
    "# Trainloader code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fcd50cb-ea33-4ad6-b2b0-15d5dba08000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7] 31 leakyrelu/train pair.npy 10878\n"
     ]
    }
   ],
   "source": [
    "nb_scen=len(os.listdir(\"./data/Scenario/\"))\n",
    "for t in range(nb_scen):\n",
    "    if(len(os.listdir(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t])))==3:\n",
    "        train_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/train pair.npy\", allow_pickle=True)\n",
    "        print(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/\"+\"train pair.npy\" ,len(train_pair2))\n",
    "        val_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/val pair.npy\", allow_pickle=True)\n",
    "        test_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/test pair.npy\", allow_pickle=True)\n",
    "\n",
    "        train_pair2 = [ list(x) for x in train_pair2]\n",
    "        test_pair2 = [ list(x) for x in test_pair2]\n",
    "        val_pair2 = [ list(x) for x in val_pair2]\n",
    "        random.shuffle(train_pair2)\n",
    "        random.shuffle(test_pair2)\n",
    "        random.shuffle(val_pair2)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2e5630-1312-47db-b6b1-1646cb9eb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the list\n",
    "# train_pair2 = np.load('train_pair++.npy', allow_pickle=True)\n",
    "# test_pair2 = np.load('test_pair++.npy', allow_pickle=True)\n",
    "# val_pair2 = np.load('val_pair++.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "train_pair2 = [ list(x) for x in train_pair2]\n",
    "test_pair2 = [ list(x) for x in test_pair2]\n",
    "val_pair2 = [ list(x) for x in val_pair2]\n",
    "random.shuffle(train_pair2)\n",
    "random.shuffle(test_pair2)\n",
    "random.shuffle(val_pair2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d41724-e6aa-4780-b97c-ea7347c33cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 4, 5, 7], [2, 5], 4, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pair2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96f0b1d-6532-48da-9cfb-44a1fe16465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10]]\n"
     ]
    }
   ],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    return [lst[i:i+batch_size] for i in range(0, len(lst), batch_size)]\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "# Example usage:\n",
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "batch_size = 4\n",
    "batches = batchify(my_list, batch_size)\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12e37e4a-a947-407d-9700-352fec773e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10878, 44968, 3220)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pair2),len(test_pair2),len(val_pair2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f58a0b2-cde4-4e58-9f7a-61c3576e245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59066"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All = list(train_pair2)+list(test_pair2)+list(val_pair2)\n",
    "All = [ list(x) for x in All]\n",
    "All.sort(reverse=True)\n",
    "len(All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83dd4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_pair2=[]\n",
    "# test_pair2=[]\n",
    "# val_pair2=[]\n",
    "# for i,L1 in tqdm(enumerate(All)) :\n",
    "#     matches=[]\n",
    "#     for L2 in All :\n",
    "#         if L2[2]==4 and L2[3]==0 and L1[2]==4 and L1[3]==0 and L1[0]==L2[0] and L2 not in matches and L2 not in train_pair2 and L2 not in test_pair2  and L2 not in val_pair2 :\n",
    "#             matches.append(L2)\n",
    "#     train_pair2.extend(matches[:int(len(matches)*0.7)])\n",
    "#     test_pair2.extend(matches[int(len(matches)*0.7):int(len(matches)*0.85)])\n",
    "#     val_pair2.extend(matches[int(len(matches)*0.85):])\n",
    "#     if L1[0]==[0,1,2,3,4,5,6,7] :\n",
    "#         print(len(matches))\n",
    "# print(len(train_pair2))\n",
    "# print(len(test_pair2))\n",
    "# print(len(val_pair2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9d535da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cs_tr=CustomDataset(train_pair2,batch_size=200)\n",
    "# end_time = time.time() \n",
    "# execution_time = end_time - start_time\n",
    "\n",
    "# Dataset,EXP,ACC,U = cs_tr[0]\n",
    "# x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c882cca-06d4-42a3-ab31-3cfed2e7d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(cs_tr.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21c1f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1.shape,x2.shape,tg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a4e714f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2, 5, 7], [4, 5], 4, 3], [[1, 7], [1, 2, 4, 6], 4, 3]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "random.shuffle(train_pair2)\n",
    "train_pair2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0930913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All = list(train_pair)+list(test_pair)+list(val_pair)\n",
    "# All = [ list(x) for x in All]\n",
    "# train_pair=[]\n",
    "# test_pair=[]\n",
    "# val_pair=[]\n",
    "# for L1 in tqdm(All) :\n",
    "#     matches=[]\n",
    "#     for L2 in All :\n",
    "#         if L1[0]==L2[0] and L2 not in matches and L2[2]==4 and L2[3]==0 :\n",
    "#             matches.append(L2)\n",
    "#     train_pair.extend(matches[:int(len(matches)*0.7)])\n",
    "#     test_pair.extend(matches[int(len(matches)*0.7):int(len(matches)*0.85)])\n",
    "#     val_pair.extend(matches[int(len(matches)*0.85):])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b8df0e3-2ced-4468-a5aa-75d9102b8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pair=test_pair[int(len(test_pair)/2):]\n",
    "# test_pair=test_pair[:int(len(test_pair)/2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86052495-ee40-454c-a374-25172cfa6083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30a0a5b4-7b2e-4abb-b007-6b4cf79c7a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[6, 7], [2, 5], 4, 3], 10878)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pair2[0] ,len(train_pair2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7adeaf93-b728-4094-9c5c-1ace4bd7f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"train_pair++\", train_pair2)\n",
    "# np.save(\"test_pair++\", test_pair2)\n",
    "# np.save(\"val_pair++\", val_pair2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0baa11-e37f-4903-8215-2b7c0e0b8ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f045b5b9-750a-4236-b6d4-3cea89fdf1a2",
   "metadata": {
    "id": "f045b5b9-750a-4236-b6d4-3cea89fdf1a2"
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "387b2953-3794-42d9-a327-5e7cca27939d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "387b2953-3794-42d9-a327-5e7cca27939d",
    "outputId": "29fef800-a2d7-43d4-dafb-9fa69bda52ca"
   },
   "outputs": [],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1b3c7-eae9-472b-8d5e-2fcade409e9b",
   "metadata": {
    "id": "f4e1b3c7-eae9-472b-8d5e-2fcade409e9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cb2ca-7fee-4e0e-ad6a-bfbd880f1d30",
   "metadata": {
    "id": "8f7cb2ca-7fee-4e0e-ad6a-bfbd880f1d30"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cd349-fa4c-42c1-8330-6c8b77c63f72",
   "metadata": {
    "id": "6e4cd349-fa4c-42c1-8330-6c8b77c63f72"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3bbd0b1-6ed0-4c53-9762-6e323034719a",
   "metadata": {
    "id": "f3bbd0b1-6ed0-4c53-9762-6e323034719a"
   },
   "outputs": [],
   "source": [
    "class EmbedderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, seed=22):\n",
    "        super().__init__()\n",
    "        #print(\"EmbedderNeuroneGroup\")\n",
    "        self.neuron_l1 = nn.Linear(200, d_model) #8\n",
    "        self.neuron_l2 = nn.Linear(72, d_model) #12\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        #print(\"multi-linear method\",v.shape)\n",
    "\n",
    "        l = []\n",
    "\n",
    "        for ndx in range(8):\n",
    "            idx_start = ndx * 200\n",
    "            idx_end = idx_start + 200\n",
    "            l.append(self.neuron_l1(v[:,idx_start:idx_end]))\n",
    "\n",
    "        # l2\n",
    "        for ndx in range(12):\n",
    "            idx_start = 200*8 + ndx * 72\n",
    "            idx_end = idx_start + 72\n",
    "            l.append(self.neuron_l2(v[:,idx_start:idx_end]))\n",
    "        #print(len(l))\n",
    "        #print(len(l[0]))\n",
    "        final = torch.stack(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc54b7-041c-4dbc-a3cc-c22554a8c8f2",
   "metadata": {
    "id": "fdbc54b7-041c-4dbc-a3cc-c22554a8c8f2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56c1a547-55ed-45b9-9b9b-6a3c9a5d2f6f",
   "metadata": {
    "id": "56c1a547-55ed-45b9-9b9b-6a3c9a5d2f6f"
   },
   "outputs": [],
   "source": [
    "# max_seq_len=176,\n",
    "# N=4\n",
    "# heads=3\n",
    "# d_model=900\n",
    "# d_ff=900\n",
    "# neck=700\n",
    "# dropout=0.1\n",
    "# # Enc=EncoderNeuronGroup(d_model=d_model, N=N, heads=heads, max_seq_len=max_seq_len, dropout=dropout,d_ff=d_ff)\n",
    "# # vec1 = torch.rand(1,2464)\n",
    "# # res,scores=Enc(vec1)\n",
    "# # res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1616a-7a25-4af5-bc73-e64a0612d0ad",
   "metadata": {
    "id": "05d1616a-7a25-4af5-bc73-e64a0612d0ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a844bdf8-de0a-4fb1-8d6e-bbbcd841b278",
   "metadata": {
    "id": "a844bdf8-de0a-4fb1-8d6e-bbbcd841b278"
   },
   "outputs": [],
   "source": [
    "# vec2neck = nn.Linear(d_ff*2, neck)\n",
    "# print(res.shape)\n",
    "# out3=torch.cat([res,res], dim=2)\n",
    "# print(\"neck input:\",out3.shape)\n",
    "# sum_r=torch.sum(out3, dim=1, keepdim=False)\n",
    "# vec2=vec2neck(sum_r)\n",
    "# print(len(vec2))\n",
    "# tanh = nn.Tanh()\n",
    "# neck_t=tanh(vec2)\n",
    "# print(\"neck shape:\",neck_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4a2a5-8711-4948-8b8a-7e298d4502ac",
   "metadata": {
    "id": "5db4a2a5-8711-4948-8b8a-7e298d4502ac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150e43a-f20a-4606-b1a8-5fb75ea2e1f4",
   "metadata": {
    "id": "4150e43a-f20a-4606-b1a8-5fb75ea2e1f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efa74a80-dee8-4710-a221-a448b37758d1",
   "metadata": {
    "id": "efa74a80-dee8-4710-a221-a448b37758d1"
   },
   "outputs": [],
   "source": [
    "# Dec=DecoderNeuronGroup(d_model=d_model, N=N, heads=heads, max_seq_len=max_seq_len, dropout=dropout,d_ff=d_ff,neck=neck)\n",
    "# res,scores=Dec(neck_t)\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48474d-d348-4b5f-bdfb-3edabe5d46cf",
   "metadata": {
    "id": "da48474d-d348-4b5f-bdfb-3edabe5d46cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5c635-1abd-40e5-a4c3-70ac38d69bf5",
   "metadata": {
    "id": "ddb5c635-1abd-40e5-a4c3-70ac38d69bf5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198b3a0-4243-4028-a1d1-c2992c56ab3d",
   "metadata": {
    "id": "2198b3a0-4243-4028-a1d1-c2992c56ab3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fad956-1f6d-487c-a3a9-1003e7e55b65",
   "metadata": {
    "id": "51fad956-1f6d-487c-a3a9-1003e7e55b65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8ef6a42-2b3e-4fc8-babe-2d6f6ff37500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8ef6a42-2b3e-4fc8-babe-2d6f6ff37500",
    "outputId": "7301efd4-e73d-46ce-baa8-b3b12a63c900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44968, 10878, 3220)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pair2),len(train_pair2),len(val_pair2)#,len(test_tgt),len(train_tgt),len(val_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b6e843a-7f62-4bdb-bfe2-c55960b6b649",
   "metadata": {
    "id": "5b6e843a-7f62-4bdb-bfe2-c55960b6b649"
   },
   "outputs": [],
   "source": [
    "#test_pair=[ x for x in test_pair  if (0 in x[0]) and (1 in x[0]) and (2 in x[0] or 2 in x[1] ) and (3 in[0] or 2 in x[1]) and (4 in[0] or 2 in x[1] ) ]\n",
    "#len(test_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "619486cb-4a43-408f-976c-74c7d6a840d9",
   "metadata": {
    "id": "619486cb-4a43-408f-976c-74c7d6a840d9"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "class ClassSpecificImageFolder(datasets.DatasetFolder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root,\n",
    "            dropped_classes=[],\n",
    "            transform = None,\n",
    "            target_transform = None,\n",
    "            loader = datasets.folder.default_loader,\n",
    "            is_valid_file = None,\n",
    "    ):\n",
    "        self.dropped_classes = dropped_classes\n",
    "        super(ClassSpecificImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                                       transform=transform,\n",
    "                                                       target_transform=target_transform,\n",
    "                                                       is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "        classes = [c for c in classes if c not in self.dropped_classes]\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71f35825-6623-41d4-9747-5e6c1d657c65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71f35825-6623-41d4-9747-5e6c1d657c65",
    "outputId": "9429472d-98a4-47db-c47e-af78aadfba49"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(mod.numParams())\n",
    "# x1 = torch.rand(1,2464)\n",
    "# x2 = torch.rand(1,2464)\n",
    "# mod=mod.to(device).to(torch.float32)\n",
    "\n",
    "# #x1=x1.to(torch.float32)\n",
    "# #x2=x2.to(torch.float32)\n",
    "# x1=x1.to(device)\n",
    "# x2=x2.to(device)\n",
    "# mod=mod.to(device)\n",
    "# out = mod(x1,x2)\n",
    "# print(\"Output Shape: \", out[0].shape)\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "# summary(mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1e0eefd-9bb1-4f3e-b722-78f4108efe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in,\n",
    "        nlin=\"leakyrelu\",\n",
    "        dropout=0.0,\n",
    "        init_type=\"uniform\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init module list\n",
    "        self.module_list = nn.ModuleList()\n",
    "        ### ASSUMES 28x28 image size\n",
    "        ## compose layer 1\n",
    "        self.module_list.append(nn.Conv2d(channels_in, 8, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        # apply dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 2\n",
    "        self.module_list.append(nn.Conv2d(8, 6, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 3\n",
    "        self.module_list.append(nn.Conv2d(6, 4, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add flatten layer\n",
    "        self.module_list.append(nn.Flatten())\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(3 * 3 * 4, 20))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(20, 10))\n",
    "\n",
    "        ### initialize weights with se methods\n",
    "        self.initialize_weights(init_type)\n",
    "\n",
    "    def initialize_weights(self, init_type):\n",
    "        # print(\"initialze model\")\n",
    "        for m in self.module_list:\n",
    "            if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "                if init_type == \"xavier_uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if init_type == \"xavier_normal\":\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                if init_type == \"uniform\":\n",
    "                    torch.nn.init.uniform_(m.weight)\n",
    "                if init_type == \"normal\":\n",
    "                    torch.nn.init.normal_(m.weight)\n",
    "                if init_type == \"kaiming_normal\":\n",
    "                    torch.nn.init.kaiming_normal_(m.weight)\n",
    "                if init_type == \"kaiming_uniform\":\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight)\n",
    "                # set bias to some small non-zero value\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    def get_nonlin(self, nlin):\n",
    "        # apply nonlinearity\n",
    "        if nlin == \"leakyrelu\":\n",
    "            return nn.LeakyReLU()\n",
    "        if nlin == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        if nlin == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        if nlin == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        if nlin == \"silu\":\n",
    "            return nn.SiLU()\n",
    "        if nlin == \"gelu\":\n",
    "            return nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward prop through module_list\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward_activations(self, x):\n",
    "        # forward prop through module_list\n",
    "        activations = []\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "            if (\n",
    "                isinstance(layer, nn.Tanh)\n",
    "                or isinstance(layer, nn.Sigmoid)\n",
    "                or isinstance(layer, nn.ReLU)\n",
    "                or isinstance(layer, nn.LeakyReLU)\n",
    "                or isinstance(layer, nn.SiLU)\n",
    "                or isinstance(layer, nn.GELU)\n",
    "                or isinstance(layer, ORU)\n",
    "                or isinstance(layer, ERU)\n",
    "            ):\n",
    "                activations.append(x)\n",
    "        return x, activations\n",
    "def train(model, trainloader, optimizer, criterion,nb_classes,First=False,df=None,verbose=False,log_freq_steps=25):\n",
    "    List_mx=[]\n",
    "    model.train()\n",
    "    #print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image\n",
    "        labels = labels\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        #mx=multiclass_confusion_matrix(preds ,labels,nb_classes,normalize=\"pred\")\n",
    "        #List_mx.append(mx)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "        if First==True and i%log_freq_steps==0 :\n",
    "            epoch_loss = train_running_loss / counter\n",
    "            epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "            if verbose==True:\n",
    "                print(epoch_acc,\"%\")\n",
    "            #print(f\"step {i}:\",epoch_loss, epoch_acc)\n",
    "            if type(df)!='NoneType':\n",
    "                df.at[track,f\"Step {i}\"]=epoch_acc\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc,List_mx\n",
    "\n",
    "\n",
    "\n",
    "def compute_shap_values(model, images, baseline):\n",
    "    explainer = shap.GradientExplainer(model, baseline)\n",
    "    shap_values = explainer.shap_values(images)\n",
    "    return shap_values\n",
    "\n",
    "def validate(model, testloader, criterion, nb_classes):\n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    correct_counts = {str(i): 0 for i in range(nb_classes)}\n",
    "    total_counts = {str(i): 0 for i in range(nb_classes)}\n",
    "    shapley_values_per_class = {str(i): [] for i in range(nb_classes)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            if data is None:\n",
    "                continue\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "            \n",
    "            # Update correct/total counts\n",
    "            for j in range(len(labels)):\n",
    "                class_label = str(labels[j].item())\n",
    "                total_counts[class_label] += 1\n",
    "                if preds[j] == labels[j]:\n",
    "                    correct_counts[class_label] += 1\n",
    "            \n",
    "            # Compute Shapley values for a batch\n",
    "            baseline = torch.zeros_like(image)  # Use black image as baseline\n",
    "            shap_values = compute_shap_values(model, image, baseline)\n",
    "            \n",
    "            for j in range(len(labels)):\n",
    "                class_label = str(labels[j].item())\n",
    "                shapley_values_per_class[class_label].append(np.mean(shap_values[j].numpy()))\n",
    "    \n",
    "    # Compute final metrics\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    \n",
    "    # Prepare data for wandb histograms\n",
    "    correct_data = [[correct_counts[str(i)]] for i in range(nb_classes)]\n",
    "    total_data = [[total_counts[str(i)]] for i in range(nb_classes)]\n",
    "    \n",
    "    correct_table = wandb.Table(data=correct_data, columns=[\"Correct Predictions\"])\n",
    "    total_table = wandb.Table(data=total_data, columns=[\"Total Instances\"])\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Prediction Histogram\": wandb.plot.histogram(correct_table, \"Correct Predictions\", title=\"Correct Predictions Per Class\"),\n",
    "        \"Total Histogram\": wandb.plot.histogram(total_table, \"Total Instances\", title=\"Total Instances Per Class\")\n",
    "    })\n",
    "    \n",
    "    # Log Shapley value histograms\n",
    "    for class_label, shap_vals in shapley_values_per_class.items():\n",
    "        if shap_vals:\n",
    "            shap_table = wandb.Table(data=[[val] for val in shap_vals], columns=[\"Shapley Values\"])\n",
    "            wandb.log({f\"Shapley Values - Class {class_label}\": wandb.plot.histogram(shap_table, \"Shapley Values\", title=f\"Shapley Values for Class {class_label}\")})\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "def create_frame(step,ax,data):\n",
    "    ax=ax.cla()\n",
    "    sns.heatmap(data[step][-1].cpu(),annot=True,cmap=\"cubehelix\",ax=ax,cbar=False)\n",
    "    plt.title('Epoch {} training {}'.format(step,exp)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5cf2e31-6295-42f1-9d0e-284ad545f94c",
   "metadata": {
    "id": "b5cf2e31-6295-42f1-9d0e-284ad545f94c"
   },
   "outputs": [],
   "source": [
    "#L_activations=[\"gelu\",\"relu\",\"silu\",\"leakyrelu\",\"sigmoid\",\"tanh\"]\n",
    "#csv_files,L_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c657276f-7578-45a1-a58b-db8a21914d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) in https://arxiv.org/pdf/2209.14733.pdf\n",
    "vec1 = torch.rand(1,2464)\n",
    "vec2 = torch.rand(1,2464)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "48684b12-575d-4354-84ed-04ce490df41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label task 1</th>\n",
       "      <th>index</th>\n",
       "      <th>Accuracy task1</th>\n",
       "      <th>label task 2</th>\n",
       "      <th>Accuracy task2</th>\n",
       "      <th>weight 0</th>\n",
       "      <th>weight 1</th>\n",
       "      <th>weight 2</th>\n",
       "      <th>weight 3</th>\n",
       "      <th>weight 4</th>\n",
       "      <th>...</th>\n",
       "      <th>bias 2462</th>\n",
       "      <th>bias 2463</th>\n",
       "      <th>Loader Set</th>\n",
       "      <th>Reconstructed Accuracy ID</th>\n",
       "      <th>Actual Accuracy</th>\n",
       "      <th>Reconstructed Accuracy OOD</th>\n",
       "      <th>Transformer Loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochCNN</th>\n",
       "      <th>ActivationCNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 2477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label task 1, index, Accuracy task1, label task 2, Accuracy task2, weight 0, weight 1, weight 2, weight 3, weight 4, weight 5, weight 6, weight 7, weight 8, weight 9, weight 10, weight 11, weight 12, weight 13, weight 14, weight 15, weight 16, weight 17, weight 18, weight 19, weight 20, weight 21, weight 22, weight 23, weight 24, weight 25, weight 26, weight 27, weight 28, weight 29, weight 30, weight 31, weight 32, weight 33, weight 34, weight 35, weight 36, weight 37, weight 38, weight 39, weight 40, weight 41, weight 42, weight 43, weight 44, weight 45, weight 46, weight 47, weight 48, weight 49, weight 50, weight 51, weight 52, weight 53, weight 54, weight 55, weight 56, weight 57, weight 58, weight 59, weight 60, weight 61, weight 62, weight 63, weight 64, weight 65, weight 66, weight 67, weight 68, weight 69, weight 70, weight 71, weight 72, weight 73, weight 74, weight 75, weight 76, weight 77, weight 78, weight 79, weight 80, weight 81, weight 82, weight 83, weight 84, weight 85, weight 86, weight 87, weight 88, weight 89, weight 90, weight 91, weight 92, weight 93, weight 94, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 2477 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cols=[\"label task 1\",\"index\",\"Accuracy task1\",\\\n",
    "      \"label task 2\",\"Accuracy task2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Loader Set\",\"Reconstructed Accuracy ID\",\"Actual Accuracy\",\"Reconstructed Accuracy OOD\",\"Transformer Loss\",\"lr\",'epochCNN','ActivationCNN'] \n",
    "\n",
    "print(len(Cols))\n",
    "predicted_Weights= pd.DataFrame(columns=Cols)\n",
    "\n",
    "# row=[\"\".format(task1),int(ind[0]),ACC[0],\"\".format(task2),ACC[1]]+vector_aux.to_list()+[\"train\",valid_epoch_acc0,ACC[2],valid_epoch_acc1,L_train[-1]]\n",
    "# predicted_Weights.append(row, ignore_index=True)\n",
    "predicted_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cec127-29d7-4d8a-b935-5d0f1dbcb2e5",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dcd60539-fb7e-4145-a4cb-8183f9f31f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)\n",
    "def scheduler_to(sched, device):\n",
    "    for param in sched.__dict__.values():\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e960c79-58b9-4f1d-93df-cbccebb9a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(type(str(datetime.datetime.now())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1cb7763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def loss_Contractive(W, x, recons_x, h, lam):\n",
    "    dh = h * (1 - h) \n",
    "\n",
    "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
    "\n",
    "    w_sum = w_sum.unsqueeze(1) # shape N_hidden x 1\n",
    " \n",
    "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "\n",
    "    return contractive_loss.mul_(lam)\n",
    "\n",
    "vec1 = torch.rand(1,2464)\n",
    "vec2 = torch.rand(1,2464)\n",
    "#print(out[1].shape,W.shape)\n",
    "# for name, param in mod.named_parameters():\n",
    "#     if name == 'vec2neck.weight':\n",
    "#         W = param\n",
    "#         break\n",
    "# CL=loss_Contractive(W,vec1,vec2, out[1], 0.005)\n",
    "\n",
    "# CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "538a483f-172d-42bc-b98d-d1abb8811164",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = torch.rand(5,2464)\n",
    "vec2 = torch.rand(5,2464)\n",
    "from scipy.stats import wasserstein_distance\n",
    "# Convert to numpy arrays\n",
    "vec1_np = vec1.numpy()\n",
    "vec2_np = vec2.numpy()\n",
    "\n",
    "# Compute Wasserstein distance for each pair of vectors\n",
    "wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "\n",
    "# If you need an aggregate measure, you can compute the average distance\n",
    "average_wsd = sum(wsd_list) / len(wsd_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9368a3f-99b5-422c-939d-ae62b83cd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "11d704c7-aa70-4b62-ba08-b5ba4459ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31a86db2-b453-4c47-86b9-77ea21619679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_quantile_loss(pred, target, q):\n",
    "    error = target - pred\n",
    "    return torch.mean(torch.max(q * error, (q - 1) * error))\n",
    "\n",
    "def frobenius_norm_jacobian(model, x1,x2):\n",
    "    x1 = x1.clone().detach().requires_grad_(True)\n",
    "    x2 = x2.clone().detach().requires_grad_(True)\n",
    "    y = model(x1,x2)\n",
    "    jacobian = []\n",
    "    for i in range(y[0].shape[0]):\n",
    "        grad_outputs = torch.zeros_like(y[0])\n",
    "        grad_outputs[:, i] = 1\n",
    "        grad = torch.autograd.grad(outputs=y[0], inputs=(x1,x2), grad_outputs=grad_outputs,\n",
    "                                   retain_graph=True, create_graph=True)[0]\n",
    "        jacobian.append(grad)\n",
    "    jacobian = torch.stack(jacobian, dim=1)\n",
    "    frobenius_norm = torch.norm(jacobian, p='fro')\n",
    "    return frobenius_norm.detach().item()\n",
    "\n",
    "# mod= TransformerAE(max_seq_len=50,\n",
    "#                         N=4,\n",
    "#                         heads=4,\n",
    "#                         d_model=40,\n",
    "#                         d_ff=40,\n",
    "#                         neck=25,\n",
    "#                         dropout=0.12\n",
    "#                        )\n",
    "# mod=mod.cuda()\n",
    "# vec1 = torch.rand(5,2464)\n",
    "# vec2 = torch.rand(5,2464)\n",
    "# vec1=vec1.cuda()\n",
    "# vec2=vec2.cuda()\n",
    "# output=mod(vec1,vec2)\n",
    "# print(output[0].shape)\n",
    "# print(frobenius_norm_jacobian(mod, vec1,vec2))\n",
    "\n",
    "# 0.15*frobenius_norm_jacobian(mod, vec1,vec2)+q_quantile_loss(output[0],vec1,0.9)"
   ]
  },
  {
   "attachments": {
    "4ccda37e-c305-42a8-8a5e-80b1cabf5c6f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d9d74fc2-5cac-4be5-8071-9815a3502efb",
   "metadata": {},
   "source": [
    "![image.png](attachment:4ccda37e-c305-42a8-8a5e-80b1cabf5c6f.png)"
   ]
  },
  {
   "attachments": {
    "2799996e-6039-4342-adea-c376fd76a886.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c8ad4b87-456d-4c95-843a-748d934ef0be",
   "metadata": {},
   "source": [
    "![image.png](attachment:2799996e-6039-4342-adea-c376fd76a886.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6599bc9-e8cc-45b7-b881-180256365842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 Î¼s, sys: 0 ns, total: 26 Î¼s\n",
      "Wall time: 28.8 Î¼s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Example vectors\\n    # vec1 = np.random.randn(1000)\\n    # vec2 = np.random.randn(1000)\\n    # fisher_info_diff = fisher_information_difference(vec1, vec2)\\nvec1 = torch.rand(80,2464)\\nvec2 = torch.rand(80,2464)\\n\\nvec1_np = vec1.detach().cpu().numpy()\\nvec2_np = vec2.detach().cpu().numpy()\\nwsd_list = [fisher_information_difference(vec1_np[i,:], vec2_np[i,:])\\n            +np.log(abs(np.sum(np.abs(vec1_np[i,:])-np.sum(np.abs(vec2_np[i,:])))))\\n            for i in range(vec1_np.shape[0])]\\naverage_FIM_N1_Loss= sum(wsd_list) / len(wsd_list)\\naverage_FIM_N1_Loss\\n###loss_tr = criterion(output[0],tg)+average_wsd'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def kernel_density_estimate(vector):\n",
    "    kde = gaussian_kde(vector)\n",
    "    return kde\n",
    "\n",
    "def finite_difference_gradient(x, kde, h=1e-5):\n",
    "    pdf_value_plus_h = kde.evaluate(x + h)\n",
    "    pdf_value_minus_h = kde.evaluate(x - h)\n",
    "    gradient = (np.log(pdf_value_plus_h) - np.log(pdf_value_minus_h)) / (2 * h)\n",
    "    return gradient\n",
    "\n",
    "def score_function(x, kde):\n",
    "    return finite_difference_gradient(x, kde)\n",
    "\n",
    "def fisher_information_integral(kde, xmin, xmax):\n",
    "    def integrand(x):\n",
    "        score = score_function(x, kde)\n",
    "        return score**2\n",
    "    \n",
    "    fisher_info, _ = quad(integrand, xmin, xmax)\n",
    "    return fisher_info\n",
    "\n",
    "def fisher_information(vector):\n",
    "    kde = kernel_density_estimate(vector)\n",
    "    xmin, xmax = np.min(vector), np.max(vector)\n",
    "    fisher_info = fisher_information_integral(kde, xmin, xmax)\n",
    "    return fisher_info\n",
    "\n",
    "def fisher_information_difference(vec1, vec2):\n",
    "    fisher_info_1 = fisher_information(vec1)\n",
    "    fisher_info_2 = fisher_information(vec2)\n",
    "    \n",
    "    fisher_info_diff = np.abs(fisher_info_1 - fisher_info_2)\n",
    "    return fisher_info_diff\n",
    "\n",
    "\"\"\"# Example vectors\n",
    "    # vec1 = np.random.randn(1000)\n",
    "    # vec2 = np.random.randn(1000)\n",
    "    # fisher_info_diff = fisher_information_difference(vec1, vec2)\n",
    "vec1 = torch.rand(80,2464)\n",
    "vec2 = torch.rand(80,2464)\n",
    "\n",
    "vec1_np = vec1.detach().cpu().numpy()\n",
    "vec2_np = vec2.detach().cpu().numpy()\n",
    "wsd_list = [fisher_information_difference(vec1_np[i,:], vec2_np[i,:])\n",
    "            +np.log(abs(np.sum(np.abs(vec1_np[i,:])-np.sum(np.abs(vec2_np[i,:])))))\n",
    "            for i in range(vec1_np.shape[0])]\n",
    "average_FIM_N1_Loss= sum(wsd_list) / len(wsd_list)\n",
    "average_FIM_N1_Loss\n",
    "###loss_tr = criterion(output[0],tg)+average_wsd\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2277b795-0690-4b10-844c-dacd90f934e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pykeops in /home/user/anaconda3/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/user/anaconda3/lib/python3.10/site-packages (from pykeops) (1.26.4)\n",
      "Requirement already satisfied: pybind11 in /home/user/anaconda3/lib/python3.10/site-packages (from pykeops) (2.13.6)\n",
      "Requirement already satisfied: keopscore==2.2.3 in /home/user/anaconda3/lib/python3.10/site-packages (from pykeops) (2.2.3)\n",
      "Requirement already satisfied: geomloss in /home/user/anaconda3/lib/python3.10/site-packages (0.2.6)\n",
      "Requirement already satisfied: numpy in /home/user/anaconda3/lib/python3.10/site-packages (from geomloss) (1.26.4)\n",
      "Requirement already satisfied: torch in /home/user/anaconda3/lib/python3.10/site-packages (from geomloss) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/user/anaconda3/lib/python3.10/site-packages (from torch->geomloss) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/user/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->geomloss) (75.8.0)\n",
      "Requirement already satisfied: wheel in /home/user/anaconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->geomloss) (0.45.1)\n",
      "Requirement already satisfied: cmake in /home/user/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch->geomloss) (3.31.6)\n",
      "Requirement already satisfied: lit in /home/user/anaconda3/lib/python3.10/site-packages (from triton==2.0.0->torch->geomloss) (18.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/anaconda3/lib/python3.10/site-packages (from jinja2->torch->geomloss) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/anaconda3/lib/python3.10/site-packages (from sympy->torch->geomloss) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pykeops\n",
    "!pip install geomloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bb885f80-b009-40b2-82dc-e2160b880167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3014, 0.7426, 0.7543,  ..., 0.0899, 0.3142, 0.4017])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "013f63ab-947a-465e-8854-72f8a7a8b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(210.8783) 0.01862557338816779\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from geomloss import SamplesLoss\n",
    "vec1 = torch.rand(1,2464)\n",
    "vec2 = torch.rand(1,2464)\n",
    "LossWS=SamplesLoss('sinkhorn')\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "# Convert to numpy arrays\n",
    "vec1_np = vec1[0].numpy()\n",
    "vec2_np = vec2[0].numpy()\n",
    "print(LossWS(vec1,vec2), wasserstein_distance(vec1_np,vec2_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3901085-aa0a-415d-b9a0-a8f4f2d82488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off optuna log notes.\n",
    "optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "\n",
    "\n",
    "def logging_callback(study, frozen_trial):\n",
    "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "    if previous_best_value != study.best_value:\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "        print(\n",
    "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
    "            frozen_trial.number,\n",
    "            frozen_trial.value,\n",
    "            frozen_trial.params,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7f355ed7-d02d-45d1-abb7-4cf1b2273369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "from collections import deque\n",
    "from typing import Dict, Optional, Literal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "### Grokfast\n",
    "def gradfilter_ema(\n",
    "    m: nn.Module,\n",
    "    grads: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    alpha: float = 0.99,\n",
    "    lamb: float = 5.0,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    if grads is None:\n",
    "        grads = {n: p.grad.data.detach() for n, p in m.named_parameters() if p.requires_grad}\n",
    "\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            grads[n] = grads[n] * alpha + p.grad.data.detach() * (1 - alpha)\n",
    "            p.grad.data = p.grad.data + grads[n] * lamb\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "### Grokfast-MA\n",
    "def gradfilter_ma(\n",
    "    m: nn.Module,\n",
    "    grads: Optional[Dict[str, deque]] = None,\n",
    "    window_size: int = 128,\n",
    "    lamb: float = 5.0,\n",
    "    filter_type: Literal['mean', 'sum'] = 'mean',\n",
    "    warmup: bool = True,\n",
    "    trigger: bool = False,\n",
    ") -> Dict[str, deque]:\n",
    "    if grads is None:\n",
    "        grads = {n: deque(maxlen=window_size) for n, p in m.named_parameters() if p.requires_grad}\n",
    "\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            grads[n].append(p.grad.data.detach())\n",
    "\n",
    "            if not warmup or len(grads[n]) == window_size and not trigger:\n",
    "                if filter_type == \"mean\":\n",
    "                    avg = sum(grads[n]) / len(grads[n])\n",
    "                elif filter_type == \"sum\":\n",
    "                    avg = sum(grads[n])\n",
    "                else:\n",
    "                    raise ValueError(f\"Unrecognized filter_type {filter_type}\")\n",
    "                p.grad.data = p.grad.data + avg * lamb\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5fb637-3239-431d-bffa-3d739edfa27f",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a37c5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = TransformerAE(max_seq_len=50,\n",
    "#                         N=4,\n",
    "#                         heads=4,\n",
    "#                         d_model=100,\n",
    "#                         d_ff=100,\n",
    "#                         neck=50,\n",
    "#                         dropout=0.8\n",
    "#                        ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "814d8863-4ecb-486e-8347-dc97d3294db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec1 = torch.rand(200, 2464).cuda()\n",
    "# vec2 = torch.rand(200, 2464).cuda()\n",
    "# target= torch.rand(200, 2464).cuda()\n",
    "\n",
    "# output=mod(vec1,vec2)\n",
    "# output[0].shape,target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "916aea9f-3bc5-4f5f-8d73-d7d0bc847d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z1_pred[0].shape,z1_vec1["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c14b55-1b87-4dbd-91ce-0b3448500db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3845b-e657-4617-9c68-e89c64eb7b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4facea-8bf3-4196-b34e-780ed1d5daa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1a5e9-424c-42cb-ae9b-0bad16223e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fbeb943a-3cb1-4997-8f88-a1c7e21a86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ffdcfc7-592d-45df-8130-90a515915f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PD=DF[Cols[2466:4930]]\n",
    "# dataPD=PD.to_numpy()\n",
    "# GT=DF[Cols[2:2466]]\n",
    "# dataGT=GT.to_numpy()\n",
    "# FN=DF[Cols[4930:7394]]\n",
    "# dataF=FN.to_numpy()\n",
    "# # PD.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "de60dce0-57eb-409f-ab2b-8719fb78fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT=DF[Cols[2:2466]]\n",
    "# dataGT=GT.to_numpy()\n",
    "# GT.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f9d2b-55b7-4db7-b620-3b2d289acd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d86c8-ce92-4847-aec8-2c6391684a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdba27-bd50-4ead-8724-935cdc558dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fa4cbafe-b20f-4b57-88f7-b584459d532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import vjp\n",
    "import networkx as nx\n",
    "# Now you can use fisher_distance as a differentiable loss component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b43337-cdc8-482f-8e6b-34eae7f4bce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "60b736af-4a19-40d0-a864-2cbd459a6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(torch.nn.Module):\n",
    "    def __init__(self, q):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute the quantile loss for each prediction\n",
    "        error = y_true - y_pred\n",
    "        loss = torch.maximum(self.q * error, (self.q - 1) * error)\n",
    "        return loss.mean()\n",
    "quantile = 0.5\n",
    "q_quantile_loss = QuantileLoss(q=quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d4622d5-d743-4296-85b7-d37f824246a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NormDifferenceDistance(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormDifferenceDistance, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate L2 norms for each tensor\n",
    "        norm_pred = torch.norm(y_pred, p=2)\n",
    "        norm_true = torch.norm(y_true, p=2)\n",
    "        \n",
    "        # Compute the absolute difference of norms as the distance\n",
    "        distance = torch.abs(norm_pred - norm_true)\n",
    "        return distance\n",
    "\n",
    "class StandardizedNormDifferenceDistance(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardizedNormDifferenceDistance, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Normalize each tensor by its L2 norm\n",
    "        norm_pred = torch.norm(y_pred, p=2)\n",
    "        norm_true = torch.norm(y_true, p=2)\n",
    "        y_pred_normalized = y_pred / (norm_pred + 1e-8)\n",
    "        y_true_normalized = y_true / (norm_true + 1e-8)\n",
    "        \n",
    "        # Calculate L2 distance between normalized tensors\n",
    "        distance = torch.norm(y_pred_normalized - y_true_normalized, p=2)\n",
    "        return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d094d51c-b9e1-4d11-8665-f3e608eb1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.017904758453369\n"
     ]
    }
   ],
   "source": [
    "class AutoregressiveMSELoss(nn.Module):\n",
    "    def __init__(self, lambda_weights=None, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Autoregressive MSE Loss with weighted contributions.\n",
    "\n",
    "        Args:\n",
    "            lambda_weights (list or None): Weights for the autoregressive components.\n",
    "                                           If None, equal weights are used.\n",
    "            epsilon (float): A small constant added to denominators for numerical stability.\n",
    "        \"\"\"\n",
    "        super(AutoregressiveMSELoss, self).__init__()\n",
    "        self.lambda_weights = lambda_weights\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predictions, targets, delimiters):\n",
    "        \"\"\"\n",
    "        Compute the autoregressive MSE loss.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted tensor of shape [batch_size, total_length].\n",
    "            targets (torch.Tensor): Target tensor of shape [batch_size, total_length].\n",
    "            delimiters (list of ints): End indices for chunks, defining tensor splits.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        n_chunks = len(delimiters)\n",
    "        mse_losses = []\n",
    "        start = 0\n",
    "\n",
    "        # Compute MSE loss for each chunk\n",
    "        for end in delimiters:\n",
    "            pred_chunk = predictions[:, start:end]\n",
    "            target_chunk = targets[:, start:end]\n",
    "            mse_loss = nn.functional.mse_loss(pred_chunk, target_chunk, reduction='mean')\n",
    "            mse_losses.append(mse_loss)\n",
    "            start = end\n",
    "\n",
    "        # Set default weights if not provided\n",
    "        if self.lambda_weights is None:\n",
    "            self.lambda_weights = [1.0] * n_chunks\n",
    "\n",
    "        if len(self.lambda_weights) != n_chunks:\n",
    "            raise ValueError(\"lambda_weights must have the same length as the number of chunks.\")\n",
    "\n",
    "        # Compute the weighted autoregressive loss\n",
    "        autoregressive_loss = mse_losses[0]  # Start with the first chunk\n",
    "        for i in range(1, n_chunks):\n",
    "            autoregressive_loss += (\n",
    "                self.lambda_weights[i - 1] * (mse_losses[i] / (mse_losses[i - 1] + self.epsilon))\n",
    "            )\n",
    "        \n",
    "        return autoregressive_loss\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# Suppose the sequence is split into chunks based on delimiters\n",
    "predictions = torch.randn(10, 2464)  # [batch_size, total_length]\n",
    "targets = torch.randn(10, 2464)     # [batch_size, total_length]\n",
    "delimiters = [208, 1414, 1514, 2254, 2464]\n",
    "\n",
    "# Initialize the loss function with custom weights\n",
    "loss_fn = AutoregressiveMSELoss(lambda_weights=[1,0.5, 0.7, 0.9, 1.1])\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(predictions, targets, delimiters)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "911982a5-bec6-41de-b637-8dc25d305658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MAE', 'MAPE', 'MSE', 'Latent', 'sinkhorn', 'KL divergence', 'Q-quantile_loss', 'LWWS_scipy', 'AUTO', ['MAE', 'MAPE'], ['MAE', 'MSE'], ['MAE', 'Latent'], ['MAE', 'sinkhorn'], ['MAE', 'KL divergence'], ['MAE', 'CAE'], ['MAE', 'Q-quantile_loss'], ['MAE', 'LWLN'], ['MAE', 'LWWS'], ['MAE', 'log-norm'], ['MAE', 'Forb_norm'], ['MAE', 'LWWS_scipy'], ['MAE', 'AUTO'], ['MAPE', 'MSE'], ['MAPE', 'Latent'], ['MAPE', 'sinkhorn'], ['MAPE', 'KL divergence'], ['MAPE', 'CAE'], ['MAPE', 'Q-quantile_loss'], ['MAPE', 'LWLN'], ['MAPE', 'LWWS'], ['MAPE', 'log-norm'], ['MAPE', 'Forb_norm'], ['MAPE', 'LWWS_scipy'], ['MAPE', 'AUTO'], ['MSE', 'Latent'], ['MSE', 'sinkhorn'], ['MSE', 'KL divergence'], ['MSE', 'CAE'], ['MSE', 'Q-quantile_loss'], ['MSE', 'LWLN'], ['MSE', 'LWWS'], ['MSE', 'log-norm'], ['MSE', 'Forb_norm'], ['MSE', 'LWWS_scipy'], ['MSE', 'AUTO'], ['Latent', 'sinkhorn'], ['Latent', 'KL divergence'], ['Latent', 'CAE'], ['Latent', 'Q-quantile_loss'], ['Latent', 'LWLN'], ['Latent', 'LWWS'], ['Latent', 'log-norm'], ['Latent', 'Forb_norm'], ['Latent', 'LWWS_scipy'], ['Latent', 'AUTO'], ['sinkhorn', 'KL divergence'], ['sinkhorn', 'CAE'], ['sinkhorn', 'Q-quantile_loss'], ['sinkhorn', 'LWLN'], ['sinkhorn', 'LWWS'], ['sinkhorn', 'log-norm'], ['sinkhorn', 'Forb_norm'], ['sinkhorn', 'LWWS_scipy'], ['sinkhorn', 'AUTO'], ['KL divergence', 'CAE'], ['KL divergence', 'Q-quantile_loss'], ['KL divergence', 'LWLN'], ['KL divergence', 'LWWS'], ['KL divergence', 'log-norm'], ['KL divergence', 'Forb_norm'], ['KL divergence', 'LWWS_scipy'], ['KL divergence', 'AUTO'], ['CAE', 'Q-quantile_loss'], ['CAE', 'LWWS_scipy'], ['CAE', 'AUTO'], ['Q-quantile_loss', 'LWLN'], ['Q-quantile_loss', 'LWWS'], ['Q-quantile_loss', 'log-norm'], ['Q-quantile_loss', 'Forb_norm'], ['Q-quantile_loss', 'LWWS_scipy'], ['Q-quantile_loss', 'AUTO'], ['LWLN', 'LWWS_scipy'], ['LWLN', 'AUTO'], ['LWWS', 'LWWS_scipy'], ['LWWS', 'AUTO'], ['log-norm', 'LWWS_scipy'], ['log-norm', 'AUTO'], ['Forb_norm', 'LWWS_scipy'], ['Forb_norm', 'AUTO'], ['LWWS_scipy', 'AUTO'], 'CAE', 'LWLN', 'LWWS', 'log-norm', 'Forb_norm', ['CAE', 'LWLN'], ['CAE', 'LWWS'], ['CAE', 'log-norm'], ['CAE', 'Forb_norm'], ['LWLN', 'LWWS'], ['LWLN', 'log-norm'], ['LWLN', 'Forb_norm'], ['LWWS', 'log-norm'], ['LWWS', 'Forb_norm'], ['log-norm', 'Forb_norm']]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Initial list of losses\n",
    "Losses=[\"MAE\",\"MAPE\",\"MSE\",\"Latent\",\"sinkhorn\",\"KL divergence\",\"CAE\",\"Q-quantile_loss\",\"LWLN\",\"LWWS\",\"log-norm\",\"Forb_norm\",\"LWWS_scipy\",\"AUTO\"]\n",
    "\n",
    "# Define prioritized losses\n",
    "priority_losses = {\"MAE\",\"MAPE\",\"MSE\",\"Latent\",\"LWWS_scipy\",\"AUTO\",\"sinkhorn\",\"KL divergence\",\"FIM\",\"Q-quantile_loss\", }\n",
    "\n",
    "# Separate prioritized singletons and other singletons\n",
    "prioritized_singletons = [loss for loss in Losses if loss in priority_losses]\n",
    "other_singletons = [loss for loss in Losses if loss not in priority_losses]\n",
    "\n",
    "# Generate pairs, separating prioritized pairs from non-prioritized ones\n",
    "priority_pairs = [[a, b] for a, b in itertools.combinations(Losses, 2) if a in priority_losses or b in priority_losses]\n",
    "non_priority_pairs = [[a, b] for a, b in itertools.combinations(Losses, 2) if a not in priority_losses and b not in priority_losses]\n",
    "\n",
    "# Combine prioritized singletons, prioritized pairs, other singletons, and other pairs\n",
    "all_losses = prioritized_singletons + priority_pairs + other_singletons + non_priority_pairs\n",
    "\n",
    "# Print the result\n",
    "#all_losses.insert(0,['MSE', 'ws_scipy','LWWS_scipy'])\n",
    "print(all_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a6bd6-6efb-4ebf-8da4-290ef542e3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae2cd24f-5731-4369-ab9c-49b3fe3f1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrobeniusNormJacobian(nn.Module):\n",
    "    def __init__(self, num_samples=10):\n",
    "        \"\"\"\n",
    "        Custom loss module that calculates the Frobenius norm approximation\n",
    "        of the Jacobian with respect to two inputs using Hutchinson's method.\n",
    "        \n",
    "        Parameters:\n",
    "            num_samples (int): Number of random vectors to sample for the approximation.\n",
    "        \"\"\"\n",
    "        super(FrobeniusNormJacobian, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def forward(self, model, inputs1, inputs2):\n",
    "        \"\"\"\n",
    "        Forward pass to calculate the approximate Frobenius norm of the Jacobian.\n",
    "        \n",
    "        Parameters:\n",
    "            model (torch.nn.Module): The model whose Jacobian's Frobenius norm we want to approximate.\n",
    "            inputs1 (torch.Tensor): The first input tensor for which the Jacobian is computed.\n",
    "            inputs2 (torch.Tensor): The second input tensor for which the Jacobian is computed.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The approximate Frobenius norm of the Jacobian as a loss.\n",
    "        \"\"\"\n",
    "        frobenius_norm_estimate = 0.0\n",
    "        inputs1.requires_grad_(True)\n",
    "        inputs2.requires_grad_(True)\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # Generate a random vector with the same shape as model output\n",
    "            random_vec = torch.randn_like(model(inputs1, inputs2)[0])\n",
    "            \n",
    "            # Compute the vector-Jacobian product for each input\n",
    "            _, vjps1 = vjp(lambda x: model(x, inputs2)[0], inputs1, v=random_vec)\n",
    "            _, vjps2 = vjp(lambda y: model(inputs1, y)[0], inputs2, v=random_vec)\n",
    "            \n",
    "            # Sum up the squared vjp norm for both inputs\n",
    "            frobenius_norm_estimate += (vjps1.norm() ** 2 + vjps2.norm() ** 2)\n",
    "        \n",
    "        # Average over the number of samples and take square root\n",
    "        frobenius_norm_estimate = (frobenius_norm_estimate / self.num_samples) ** 0.5\n",
    "        return frobenius_norm_estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2fb0e1ef-6479-430e-adfd-6ed0b388b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWLN_loss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(LWLN_loss, self).__init__()\n",
    "        def forward(self, vec1,vec2):\n",
    "            loss = (torch.mean((vec1[:,0:208]-vec2[:,0:208])**2)/vec2[:,0:208].std() + \n",
    "                     torch.mean((vec1[:,208:1414]-vec2[:,208:1414])**2)/vec2[:,208:1414].std()+ \n",
    "                     torch.mean((vec1[:,1414:1514]-vec2[:,1414:1514])**2)/vec2[:,1414:1514].std()+\n",
    "                     torch.mean((vec1[:,1514:2254]-vec2[:,1514:2254])**2)/vec2[:,1514:2254].std()+\n",
    "                     torch.mean((vec1[:,2254:2464]-vec2[:,2254:2464])**2)/vec2[:,2254:2464].std())/(6)\n",
    "            return loss\n",
    "\n",
    "class MAPE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MAPE, self).__init__()\n",
    "        def forward(self, vec1,vec2):\n",
    "            loss = torch.mean(torch.abs((vec1 - vec2) / vec2))\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "07cbe5a7-1519-4a69-8938-86e751e84f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Losses=[\"MAPE\",\"MAE\",\"MSE\",\"Latent\",\"sinkhorn\",\"gw_loss\",\"ws_scipy\",\"CAE\",\"Q-quantile_loss\",\"LWLN\",\"LWWS\",\"FIM\",\"log-norm\",\"AUTO\",\"KL divergence\",\"Forb_norm\",\"LWWS_scipy\",\"ws_scipy 0.9\",\"ws_scipy\"]#py_riemanian , geodesic\n",
    "from torch.nn.functional import pairwise_distance\n",
    "#from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "vec1 = torch.rand(1,2464).cuda()\n",
    "vec2 = torch.rand(1,2464).cuda()\n",
    "\n",
    "def update_all_metrics(d_loos,mod,vec1,vec2,tg,DF):\n",
    "    Losses=[\"MAPE\",\"MAE\",\"MSE\",\"Latent\",\"sinkhorn\",\"gw_loss\",\"ws_scipy\",\"CAE\",\"Q-quantile_loss\",\"LWLN\",\"LWWS\",\"FIM\",\"log-norm\",\"AUTO\",\"KL divergence\",\"Forb_norm\",\"LWWS_scipy\",\"ws_scipy 0.9\",\"ws_scipy\"]\n",
    "    Lambda=0.05\n",
    "    output=mod(vec1,vec2)\n",
    "    for Loss in Losses:\n",
    "        if Loss==\"Latent\":\n",
    "            mse_loss = torch.nn.MSELoss()\n",
    "            z1_vec1=mod.enc1(vec1)[0]#[0,:,:]\n",
    "            z1_vec2=mod.enc1(vec2)[0]#[0,:,:]\n",
    "            z1_pred=mod.enc1(output[0])[0]#[0,:,:]\n",
    "            z1_tg=mod.enc1(tg)[0]\n",
    "            \n",
    "            out3=torch.cat([z1_tg,z1_tg], dim=2)\n",
    "            Z1_tg = mod.tanh(mod.vec2neck(torch.sum(out3, dim=1, keepdim=False))).to(\"cpu\")\n",
    "            \n",
    "            l2_z11 = mse_loss(z1_vec1, z1_tg)\n",
    "            l2_z12 = mse_loss(z1_vec2,z1_tg)\n",
    "            l2_z1p = mse_loss(z1_pred,z1_tg)\n",
    "            del(z1_vec1)\n",
    "            del(z1_vec2)\n",
    "            del(z1_pred)\n",
    "            del(z1_tg)\n",
    "            \n",
    "            z2_vec1=mod.enc2(vec1)[0]#[0,:,:]\n",
    "            z2_vec2=mod.enc2(vec2)[0]#[0,:,:]\n",
    "            z2_pred=mod.enc2(output[0])[0]#[0,:,:]\n",
    "            z2_tg=mod.enc2(tg)[0]#[0,:,:]\n",
    "            \n",
    "            \n",
    "            out3=torch.cat([z2_tg,z2_tg], dim=2)\n",
    "            Z2_tg = mod.tanh(mod.vec2neck(torch.sum(out3, dim=1, keepdim=False))).to(\"cpu\")\n",
    "            \n",
    "            l2_z21 = mse_loss(z2_vec1, z2_tg)\n",
    "            l2_z22 = mse_loss(z2_vec2,z2_tg)\n",
    "            l2_z2p = mse_loss(z2_pred,z2_tg)\n",
    "            del(z2_vec1)\n",
    "            del(z2_vec2)\n",
    "            del(z2_pred)\n",
    "            del(z2_tg)\n",
    "            \n",
    "            LZ1= mse_loss(Z1_tg.cuda() ,output[1].cuda())\n",
    "            LZ2=mse_loss(Z2_tg.cuda(),output[1].cuda())\n",
    "            \n",
    "            LZ = LZ1 +LZ2\n",
    "            del(Z1_tg)\n",
    "            del(Z2_tg)\n",
    "            loss_tr =LZ +(2.0*l2_z1p/(l2_z11 + l2_z12))+(2.0*l2_z2p/(l2_z21 + l2_z22))\n",
    "\n",
    "            wandb.log({\"Branch 1  x1-p MSE Z\":l2_z11})\n",
    "            wandb.log({\"Branch 1  x2-p MSE Z\":l2_z12})\n",
    "            wandb.log({\"Branch 1  p-t MSE Z\":l2_z1p})\n",
    "            \n",
    "            wandb.log({\"Branch 2  x1-p MSE Z\":l2_z21})\n",
    "            wandb.log({\"Branch 2  x2-p MSE Z\":l2_z22})\n",
    "            wandb.log({\"Branch 2  p-t MSE Z\":l2_z2p})\n",
    "\n",
    "            wandb.log({\"Branch 1  before after merge\":LZ1})\n",
    "            wandb.log({\"Branch 2  before after merge\":LZ2})\n",
    "\n",
    "\n",
    "            \n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "            mod=mod.cuda()\n",
    "        if Loss==\"MAPE\":\n",
    "            mape=MAPE()\n",
    "            loss_tr = mape(output[0],tg)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"MAE\":\n",
    "            mae = nn.L1Loss()\n",
    "            loss_tr = mae(output[0],tg)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"MSE\":\n",
    "            loss_tr = criterion(output[0],tg)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"sinkhorn\":\n",
    "            loss_tr=LossWS(output[0],tg)\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"AUTO\":\n",
    "            delimiters = [208, 1414, 1514, 2254, 2464]\n",
    "            predictions=output[0]\n",
    "            targets=tg\n",
    "            # Initialize the loss function with custom weights\n",
    "            loss_fn = AutoregressiveMSELoss(lambda_weights=[10,5,2, 1, 0.5])\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_tr = loss_fn(predictions, targets, delimiters)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"ws_scipy 0.9\":\n",
    "            vec1_np = output[0].detach().cpu().numpy()\n",
    "            vec2_np = tg.detach().cpu().numpy()\n",
    "            wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "            average_wsd = sum(wsd_list) / len(wsd_list)\n",
    "            loss_tr = average_wsd *0.9\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"ws_scipy\":\n",
    "            vec1_np = output[0].detach().cpu().numpy()\n",
    "            vec2_np = tg.detach().cpu().numpy()\n",
    "            wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "            average_wsd = sum(wsd_list) / len(wsd_list)\n",
    "            loss_tr = average_wsd *0.9\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"gw_loss\":\n",
    "            if len(DF)>0:\n",
    "                PD=DF[Cols[2466:4930]]\n",
    "                dataPD=PD.to_numpy()\n",
    "                GT=DF[Cols[2:2466]]\n",
    "                dataGT=GT.to_numpy()\n",
    "                FN=DF[Cols[4930:7394]]\n",
    "                dataF=FN.to_numpy()\n",
    "                # Define filter function â€“ can be any scikit-learn transformer\n",
    "                filter_func = Projection(columns=[0,1])\n",
    "                # Define cover\n",
    "                cover = CubicalCover(n_intervals=20, overlap_frac=0.5)\n",
    "                # Choose clustering algorithm â€“ default is DBSCAN\n",
    "                clusterer = DBSCAN()\n",
    "\n",
    "                # Configure parallelism of clustering step\n",
    "                n_jobs = 8\n",
    "\n",
    "                # Initialise pipeline\n",
    "                pipe = make_mapper_pipeline(\n",
    "                    filter_func=filter_func,\n",
    "                    cover=cover,\n",
    "                    clusterer=clusterer,\n",
    "                    verbose=True,\n",
    "                   n_jobs=n_jobs)\n",
    "                graphT=pipe.fit_transform(dataGT)\n",
    "                graphP = pipe.fit_transform(dataPD)\n",
    "                # Transform the second point cloud using the fitted pipeline\n",
    "\n",
    "                T=graphT.to_networkx()\n",
    "                P=graphP.to_networkx()\n",
    "\n",
    "                # Convert graphs to adjacency matrices\n",
    "                def graph_to_adjacency_matrix(graph):\n",
    "                    return torch.tensor(nx.to_numpy_array(graph), dtype=torch.float32)\n",
    "\n",
    "                # Assume G1 and G2 are your two NetworkX graph objects\n",
    "                adj1 = graph_to_adjacency_matrix(T)\n",
    "                adj2 = graph_to_adjacency_matrix(P)\n",
    "\n",
    "                def gw_loss(C1, C2, P):\n",
    "                    C1P = torch.matmul(C1, P)  # n x m matrix\n",
    "                    PC2 = torch.matmul(P, C2)  # n x m matrix\n",
    "                    return torch.sum((C1P - PC2) ** 2)\n",
    "                # Compute pairwise distance matrices for each graph\n",
    "                C1 = torch.cdist(adj1, adj1, p=2)  # Pairwise distance matrix for G1\n",
    "                C2 = torch.cdist(adj2, adj2, p=2)  # Pairwise distance matrix for G2\n",
    "                # Initialize coupling matrix with uniform distribution\n",
    "                n, m = C1.shape[0], C2.shape[0]\n",
    "                P = torch.ones((n, m), dtype=torch.float32, requires_grad=True) / (n * m)\n",
    "                P=P.detach()\n",
    "                P.requires_grad=True\n",
    "\n",
    "                P=P.detach()\n",
    "                P.requires_grad=True\n",
    "                #print(P.requires_grad)\n",
    "                ##optimizer = torch.optim.Adam([P], lr=0.01)\n",
    "                ##optimizer.step()\n",
    "                loss_tr=gw_loss(C1,C2,P)*0.0005\n",
    "            else:\n",
    "                loss_tr=0\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"CAE\":\n",
    "            for name, param in mod.named_parameters():\n",
    "                if name == 'vec2neck.weight':\n",
    "                    W = param\n",
    "            CL=loss_Contractive(W,output[0],tg, output[1], Lambda)\n",
    "            loss_tr = CL\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"Q-quantile_loss\":    \n",
    "            loss_tr=q_quantile_loss(output[0],tg).cuda()\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"LWLN\":\n",
    "            LW=LWLN_loss()\n",
    "            loss_tr=LW(output[0],tg)\n",
    "            #print(Loss,\": \\t\",loss_tr\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"LWWS\":\n",
    "            loss_tr=LossWS(output[0][:,0:208],tg[:,0:208]) +LossWS(output[0][:,208:1414],tg[:,208:1414])+LossWS(output[0][:,1414:1514],tg[:,1414:1514])+LossWS(output[0][:,1514:2254],tg[:,1514:2254])+LossWS(output[0][:,2254:2464],tg[:,2254:2464])\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "            \n",
    "            \n",
    "        if Loss==\"LWWS_scipy\":\n",
    "            vec1_np = output[0].detach().cpu().numpy()\n",
    "            vec2_np = tg.detach().cpu().numpy()\n",
    "            \n",
    "            delimiters = [208, 1414, 1514, 2254, 2464]\n",
    "            # Compute Wasserstein distance for each chunk and sum the averages\n",
    "            total_average_wsd = 0\n",
    "            start = 0\n",
    "            for end in delimiters:\n",
    "                chunk1 = vec1_np[:, start:end]\n",
    "                chunk2 = vec2_np[:, start:end]\n",
    "                wsd_list = [wasserstein_distance(chunk1[i], chunk2[i]) for i in range(chunk1.shape[0])]\n",
    "                average_wsd = sum(wsd_list) / len(wsd_list)  # Average Wasserstein distance for the current chunk\n",
    "                total_average_wsd += average_wsd  # Add to total\n",
    "                start = end\n",
    "            # Print the total sum of batch averages\n",
    "            #print(f\"Total Sum of Batch Averages across all chunks: {total_average_wsd}\")\n",
    "\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:total_average_wsd})\n",
    "            d_loos[Loss]=total_average_wsd\n",
    "        if Loss==\"FIM\":\n",
    "            # Example numpy arrays\n",
    "            output[0].detach().requires_grad=True\n",
    "            tg.requires_grad=True\n",
    "            # Step 1: Convert to probabilities and compute log-probabilities with gradient tracking\n",
    "            log_probs1 = torch.log_softmax(output[0], dim=1)\n",
    "            log_probs2 = torch.log_softmax(tg, dim=1)\n",
    "\n",
    "            # Step 2: Convert log-probs to probs while ensuring gradients are tracked\n",
    "            probs1 = log_probs1.exp()\n",
    "            probs2 = log_probs2.exp()\n",
    "\n",
    "            # Step 3: Compute gradients of log-probabilities with respect to original tensors\n",
    "            # Using create_graph=True to allow second-order gradients\n",
    "            grad_log_probs1 = torch.autograd.grad(outputs=log_probs1, inputs=output[0], grad_outputs=torch.ones_like(log_probs1), retain_graph=True, create_graph=True)[0]\n",
    "            grad_log_probs2 = torch.autograd.grad(outputs=log_probs2, inputs=tg, grad_outputs=torch.ones_like(log_probs2), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "            # Step 4: Approximate Fisher Information for each tensor\n",
    "            fim1 = probs1 * grad_log_probs1**2\n",
    "            fim2 = probs2 * grad_log_probs2**2\n",
    "\n",
    "            # Step 5: Compute Fisher Distance (use Frobenius norm)\n",
    "            fisher_distance = torch.norm(fim1 - fim2, p='fro')  # Frobenius norm for matrix difference\n",
    "            loss_tr=fisher_distance*100\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"log-norm\":\n",
    "            N1=NormDifferenceDistance()\n",
    "            loss_tr=N1(output[0],tg)\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"KL divergence\":\n",
    "            kl_loss = nn.KLDivLoss(reduction=\"batchmean\",log_target=True)\n",
    "            loss_tr=kl_loss(output[0],nn.functional.log_softmax(tg))*0.1\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "        if Loss==\"Forb_norm\":\n",
    "            f=FrobeniusNormJacobian(10)\n",
    "            loss_tr=f(mod,output[0],tg)*0.015\n",
    "            #print(Loss,\": \\t\",loss_tr)\n",
    "            wandb.log({Loss:loss_tr})\n",
    "            d_loos[Loss]=loss_tr\n",
    "    return d_loos,output,mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c11dbde-9d75-43cb-9222-15e17db78a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with profile(activities=[ProfilerActivity.CUDA],profile_memory=True, record_shapes=True) as prof:\n",
    "#         output=mod(vec1,vec2)\n",
    "#     print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fe67e89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vec1 = torch.rand(1,2464).cuda()\\nvec2 = torch.rand(1,2464).cuda()\\nvec3 = torch.rand(1,2464).cuda()\\nout = mod(vec1,vec2)[0]\\nprint(LossWS(vec3,out))\\nLossWS(vec3,out).backward()\\nS=0\\nfor l in mod.parameters():\\n    S += (l.grad**2).sum()\\nS = S**0.5\\nprint(S)'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"vec1 = torch.rand(1,2464).cuda()\n",
    "vec2 = torch.rand(1,2464).cuda()\n",
    "vec3 = torch.rand(1,2464).cuda()\n",
    "out = mod(vec1,vec2)[0]\n",
    "print(LossWS(vec3,out))\n",
    "LossWS(vec3,out).backward()\n",
    "S=0\n",
    "for l in mod.parameters():\n",
    "    S += (l.grad**2).sum()\n",
    "S = S**0.5\n",
    "print(S)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "23a6ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7745\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "Cols=[\"label task 1\",\"label task 2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(200)]+[\"Pred bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(208,1408)]+[\"Pred bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1414,1510)]+[\"Pred bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1514,2234)]+[\"Pred bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(2254,2454)]+[\"Pred bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(200)]+[\"FN bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(208,1408)]+[\"FN bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1414,1510)]+[\"FN bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1514,2234)]+[\"FN bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(2254,2454)]+[\"FN bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "[\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KLD\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "[\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "[\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "[\"WSD FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\\n",
    "[\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\",\"epoch 0\",\"epoch 1\",\"epoch 2\"]+\\\n",
    "[\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]+[\"Representation Z{}\".format(x) for x in range(256)]+[\"Z1 L2\",\"Z1 L2\",\"z1 L2\",\"z2 L2\"]\n",
    "#incoporate Norm                                                     \n",
    "#The Fisher information metric X norm of the target  \n",
    "#dataset distance , topological distance ,UMAP distance\n",
    "\n",
    "print(len(Cols))\n",
    "DF= pd.DataFrame(columns=Cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dcf74d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobnorm(mod):\n",
    "    S=0\n",
    "    for l in mod.parameters():\n",
    "        S += (l.grad**2).sum()\n",
    "    S = S**0.5\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "854f7368-a010-4142-9215-ab0a1266adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]\n",
    "\n",
    "\n",
    "\n",
    "# class LWLN_loss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LWLN_loss, self).__init__()\n",
    "#     def forward(self, vec1,vec2):\n",
    "#         loss = (torch.mean((vec1[0:208]-vec2[0:208])**2)/vec2[0:208].std() + \n",
    "#                  torch.mean((vec1[208:1414]-vec2[208:1414])**2)/vec2[208:1414].std()+ \n",
    "#                  torch.mean((vec1[1414:1514]-vec2[1414:1514])**2)/vec2[1414:1514].std()+\n",
    "#                  torch.mean((vec1[1514:2254]-vec2[1514:2254])**2)/vec2[1514:2254].std()+\n",
    "#                  torch.mean((vec1[2254:2464]-vec2[2254:2464])**2)/vec2[2254:2464].std())/(6)\n",
    "        \n",
    "#         return loss\n",
    "# LW=LWLN_loss()\n",
    "# LW(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1e593a3a-7aaa-4d2b-80b7-5fc6b7f681a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "def get_plot_object(vec1, tg):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))  # Create figure and axis objects\n",
    "    canvas = FigureCanvas(fig)  # Create a canvas to hold the figure\n",
    "\n",
    "    # Convert tensors to NumPy and flatten\n",
    "    vec1_np = vec1.cpu().numpy().flatten()\n",
    "    tg_np = tg[0].cpu().numpy().flatten()\n",
    "\n",
    "    # Plot data\n",
    "    ax.plot(vec1_np, label='Predicted', alpha=0.7)\n",
    "    ax.plot(tg_np, label='Target', alpha=0.7)\n",
    "\n",
    "    # Labels and legend\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Neurone Index')\n",
    "    ax.set_ylabel('Neurone Value')\n",
    "    ax.set_title('Plot of Predicted vs Target')\n",
    "\n",
    "    return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "027474e2-1710-457d-a3f6-898ea92c2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "# [\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]\n",
    "class LWLN_loss_single(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LWLN_loss_single, self).__init__()\n",
    "    def forward(self, vec1,vec2):\n",
    "        loss = (torch.mean((vec1[0:208]-vec2[0:208])**2)/vec2[0:208].std() + \n",
    "                 torch.mean((vec1[208:1414]-vec2[208:1414])**2)/vec2[208:1414].std()+ \n",
    "                 torch.mean((vec1[1414:1514]-vec2[1414:1514])**2)/vec2[1414:1514].std()+\n",
    "                 torch.mean((vec1[1514:2254]-vec2[1514:2254])**2)/vec2[1514:2254].std()+\n",
    "                 torch.mean((vec1[2254:2464]-vec2[2254:2464])**2)/vec2[2254:2464].std())/(6)\n",
    "        \n",
    "        return loss\n",
    "LWs=LWLN_loss_single()\n",
    "#LW(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "162bc603-a535-4cf6-9fe8-454e84ebcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loos=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7256da20-f0a0-4fd9-a398-7a4ef2973f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BrainTransformer= TransformerAE(max_seq_len=50,\n",
    "#             N=4,\n",
    "#             heads=4,\n",
    "#             d_model=800,#960\n",
    "#             d_ff=800,#960\n",
    "#             neck=180,#256\n",
    "#             dropout=0.07#0.12\n",
    "#             )\n",
    "# torch.save({'epoch':-1,'model_state_dict': BrainTransformer.state_dict()},\n",
    "#             './AE epoch{} {} {}.pth'.format( \"initial\" , \"original\", -1))\n",
    "# del(BrainTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9597ea69-a402-4397-909d-0a30501aacc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'Latent',\n",
       " 'sinkhorn',\n",
       " 'KL divergence',\n",
       " 'Q-quantile_loss',\n",
       " 'LWWS_scipy',\n",
       " 'AUTO',\n",
       " ['MAE', 'MAPE'],\n",
       " ['MAE', 'MSE'],\n",
       " ['MAE', 'Latent'],\n",
       " ['MAE', 'sinkhorn'],\n",
       " ['MAE', 'KL divergence'],\n",
       " ['MAE', 'CAE'],\n",
       " ['MAE', 'Q-quantile_loss'],\n",
       " ['MAE', 'LWLN'],\n",
       " ['MAE', 'LWWS'],\n",
       " ['MAE', 'log-norm'],\n",
       " ['MAE', 'Forb_norm'],\n",
       " ['MAE', 'LWWS_scipy'],\n",
       " ['MAE', 'AUTO'],\n",
       " ['MAPE', 'MSE'],\n",
       " ['MAPE', 'Latent'],\n",
       " ['MAPE', 'sinkhorn'],\n",
       " ['MAPE', 'KL divergence'],\n",
       " ['MAPE', 'CAE'],\n",
       " ['MAPE', 'Q-quantile_loss'],\n",
       " ['MAPE', 'LWLN'],\n",
       " ['MAPE', 'LWWS'],\n",
       " ['MAPE', 'log-norm'],\n",
       " ['MAPE', 'Forb_norm'],\n",
       " ['MAPE', 'LWWS_scipy'],\n",
       " ['MAPE', 'AUTO'],\n",
       " ['MSE', 'Latent'],\n",
       " ['MSE', 'sinkhorn'],\n",
       " ['MSE', 'KL divergence'],\n",
       " ['MSE', 'CAE'],\n",
       " ['MSE', 'Q-quantile_loss'],\n",
       " ['MSE', 'LWLN'],\n",
       " ['MSE', 'LWWS'],\n",
       " ['MSE', 'log-norm'],\n",
       " ['MSE', 'Forb_norm'],\n",
       " ['MSE', 'LWWS_scipy'],\n",
       " ['MSE', 'AUTO'],\n",
       " ['Latent', 'sinkhorn'],\n",
       " ['Latent', 'KL divergence'],\n",
       " ['Latent', 'CAE'],\n",
       " ['Latent', 'Q-quantile_loss'],\n",
       " ['Latent', 'LWLN'],\n",
       " ['Latent', 'LWWS'],\n",
       " ['Latent', 'log-norm'],\n",
       " ['Latent', 'Forb_norm'],\n",
       " ['Latent', 'LWWS_scipy'],\n",
       " ['Latent', 'AUTO'],\n",
       " ['sinkhorn', 'KL divergence'],\n",
       " ['sinkhorn', 'CAE'],\n",
       " ['sinkhorn', 'Q-quantile_loss'],\n",
       " ['sinkhorn', 'LWLN'],\n",
       " ['sinkhorn', 'LWWS'],\n",
       " ['sinkhorn', 'log-norm'],\n",
       " ['sinkhorn', 'Forb_norm'],\n",
       " ['sinkhorn', 'LWWS_scipy'],\n",
       " ['sinkhorn', 'AUTO'],\n",
       " ['KL divergence', 'CAE'],\n",
       " ['KL divergence', 'Q-quantile_loss'],\n",
       " ['KL divergence', 'LWLN'],\n",
       " ['KL divergence', 'LWWS'],\n",
       " ['KL divergence', 'log-norm'],\n",
       " ['KL divergence', 'Forb_norm'],\n",
       " ['KL divergence', 'LWWS_scipy'],\n",
       " ['KL divergence', 'AUTO'],\n",
       " ['CAE', 'Q-quantile_loss'],\n",
       " ['CAE', 'LWWS_scipy'],\n",
       " ['CAE', 'AUTO'],\n",
       " ['Q-quantile_loss', 'LWLN'],\n",
       " ['Q-quantile_loss', 'LWWS'],\n",
       " ['Q-quantile_loss', 'log-norm'],\n",
       " ['Q-quantile_loss', 'Forb_norm'],\n",
       " ['Q-quantile_loss', 'LWWS_scipy'],\n",
       " ['Q-quantile_loss', 'AUTO'],\n",
       " ['LWLN', 'LWWS_scipy'],\n",
       " ['LWLN', 'AUTO'],\n",
       " ['LWWS', 'LWWS_scipy'],\n",
       " ['LWWS', 'AUTO'],\n",
       " ['log-norm', 'LWWS_scipy'],\n",
       " ['log-norm', 'AUTO'],\n",
       " ['Forb_norm', 'LWWS_scipy'],\n",
       " ['Forb_norm', 'AUTO'],\n",
       " ['LWWS_scipy', 'AUTO'],\n",
       " 'CAE',\n",
       " 'LWLN',\n",
       " 'LWWS',\n",
       " 'log-norm',\n",
       " 'Forb_norm',\n",
       " ['CAE', 'LWLN'],\n",
       " ['CAE', 'LWWS'],\n",
       " ['CAE', 'log-norm'],\n",
       " ['CAE', 'Forb_norm'],\n",
       " ['LWLN', 'LWWS'],\n",
       " ['LWLN', 'log-norm'],\n",
       " ['LWLN', 'Forb_norm'],\n",
       " ['LWWS', 'log-norm'],\n",
       " ['LWWS', 'Forb_norm'],\n",
       " ['log-norm', 'Forb_norm']]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0110456-99c7-4c92-b5df-2e44664aea62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d0110456-99c7-4c92-b5df-2e44664aea62",
    "outputId": "02799af4-014c-4dab-ec46-33d719329174",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Scenario/overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7] 31 leakyrelu/train pair.npy 10878\n",
      "encoder droupout init 0.07\n",
      "encoder droupout init 0.07\n",
      "decoder droupout init 0.07\n",
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "TransformerAE                                      --\n",
      "â”œâ”€EncoderNeuronGroup: 1-1                          --\n",
      "â”‚    â””â”€EmbedderNeuronGroup: 2-1                    --\n",
      "â”‚    â”‚    â””â”€Linear: 3-1                            13,600\n",
      "â”‚    â”‚    â””â”€Linear: 3-2                            64,800\n",
      "â”‚    â””â”€PositionalEncoder: 2-2                      --\n",
      "â”‚    â””â”€ModuleList: 2-3                             --\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-3                      3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-4                      3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-5                      3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-6                      3,848,000\n",
      "â”‚    â””â”€Norm: 2-4                                   1,600\n",
      "â”œâ”€EncoderNeuronGroup: 1-2                          --\n",
      "â”‚    â””â”€EmbedderNeuronGroup: 2-5                    --\n",
      "â”‚    â”‚    â””â”€Linear: 3-7                            13,600\n",
      "â”‚    â”‚    â””â”€Linear: 3-8                            64,800\n",
      "â”‚    â””â”€PositionalEncoder: 2-6                      --\n",
      "â”‚    â””â”€ModuleList: 2-7                             --\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-9                      3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-10                     3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-11                     3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-12                     3,848,000\n",
      "â”‚    â””â”€Norm: 2-8                                   1,600\n",
      "â”œâ”€DecoderNeuronGroup: 1-3                          --\n",
      "â”‚    â””â”€Neck2Seq: 2-9                               --\n",
      "â”‚    â”‚    â””â”€ModuleList: 3-13                       7,240,000\n",
      "â”‚    â””â”€PositionalEncoder: 2-10                     --\n",
      "â”‚    â””â”€ModuleList: 2-11                            --\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-14                     3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-15                     3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-16                     3,848,000\n",
      "â”‚    â”‚    â””â”€EncoderLayer: 3-17                     3,848,000\n",
      "â”‚    â””â”€Norm: 2-12                                  1,600\n",
      "â”‚    â””â”€Seq2Vec: 2-13                               --\n",
      "â”‚    â”‚    â””â”€Linear: 3-18                           98,562,464\n",
      "â”œâ”€Linear: 1-4                                      288,180\n",
      "â”œâ”€Tanh: 1-5                                        --\n",
      "===========================================================================\n",
      "Total params: 152,428,244\n",
      "Trainable params: 152,428,244\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Federated-Continual-learning-/New/wandb/run-20250328_132631-gsvprwd3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aymentlili/WR4CL-project/runs/gsvprwd3' target=\"_blank\">overlapping 1 [0, 1, 2, 3, 4, 5, 6, 7] 31 leakyrelu 2 MAE 2025-03-28 13:26:31.266178</a></strong> to <a href='https://wandb.ai/aymentlili/WR4CL-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aymentlili/WR4CL-project' target=\"_blank\">https://wandb.ai/aymentlili/WR4CL-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aymentlili/WR4CL-project/runs/gsvprwd3' target=\"_blank\">https://wandb.ai/aymentlili/WR4CL-project/runs/gsvprwd3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "           \n",
    "track=0\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#from optuna.storages import JournalStorage, JournalFileStorage\n",
    "\n",
    "#storage = JournalStorage(JournalFileStorage(\"optuna-journal DDP 3 Losses.log\"))\n",
    "\n",
    "#def objective(trial):\n",
    "\n",
    "global track , output , Brain ,model\n",
    "grads=None\n",
    "# BrainTransformer= TransformerAE(max_seq_len=50,\n",
    "#                     N=4,\n",
    "#                     heads=4,\n",
    "#                     d_model=960,#960\n",
    "#                     d_ff=960,#960\n",
    "#                     neck=256,#256\n",
    "#                     dropout=0.07#0.12\n",
    "#                     )\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "nb_scen=len(os.listdir(\"./data/Scenario/\"))\n",
    "for t in range(nb_scen):\n",
    "    d_loos=dict()\n",
    "    if(len(os.listdir(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t])))==3:\n",
    "        namef=os.listdir(\"./data/Scenario/\")[t]\n",
    "        train_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/train pair.npy\", allow_pickle=True)\n",
    "        print(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/\"+\"train pair.npy\" ,len(train_pair2))\n",
    "        val_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/val pair.npy\", allow_pickle=True)\n",
    "        test_pair2 = np.load(\"./data/Scenario/\"+os.listdir(\"./data/Scenario/\")[t]+\"/test pair.npy\", allow_pickle=True)\n",
    "\n",
    "        train_pair2 = [ list(x) for x in train_pair2]\n",
    "        test_pair2 = [ list(x) for x in test_pair2]\n",
    "        val_pair2 = [ list(x) for x in val_pair2]\n",
    "        random.shuffle(train_pair2)\n",
    "        random.shuffle(test_pair2)\n",
    "        random.shuffle(val_pair2)\n",
    "    for combination in all_losses:\n",
    "        DF= pd.DataFrame(columns=Cols)\n",
    "        if combination in [\"gw_loss\",\"LWWS_scipy\",\"ws_scipy\",\"ws_scipy 0.9\",[\"LWWS_scipy\",\"ws_scipy\"],[\"LWWS_scipy\",\"ws_scipy 0.9\"],[\"ws_scipy\",\"ws_scipy 0.9\"]] :\n",
    "            continue\n",
    "        else :\n",
    "            \n",
    "                #Losses[\"MSE\",\"sinkhorn\",\"LWLN\",\"LWWS\",\"FIM\",\"log-norm\",\"KL divergence\",\"CAE\"]\n",
    "                #Loss=\"LWLN\" #trial.suggest_categorical(\"Loss function\",[\"MSE\"])#,\"LWLN\",\"Contractive\",])\n",
    "            Lambda=0\n",
    "    \n",
    "            results_path=f\"./Experiments/{combination}/\"\n",
    "            if not(os.path.isdir(results_path)):\n",
    "                os.mkdir(results_path)\n",
    "            if not(os.path.isdir(results_path+f'Tracking/')):\n",
    "                os.mkdir(results_path+f'Tracking/')\n",
    "            if not(os.path.isdir(results_path+f'Attention/')):\n",
    "                os.mkdir(results_path+f'Attention/')\n",
    "    \n",
    "            cnn_acc_ID=[]\n",
    "            cnn_acc_OOD=[]\n",
    "    \n",
    "            step_size=0\n",
    "            factor=0\n",
    "            threshhold=0\n",
    "            threshold_mode = 0\n",
    "            eps=0\n",
    "        \n",
    "            cnn_samples=30\n",
    "            num_epochs=2\n",
    "            batch_size=50\n",
    "            batch_limit=100\n",
    "            \n",
    "            ch=torch.load('./AE epoch{} {} {}.pth'.format( \"initial\" , \"original\", -1),map_location=\"cpu\")\n",
    "            mod=TransformerAE(max_seq_len=50,\n",
    "                N=4,\n",
    "                heads=4,\n",
    "                d_model=800,#960\n",
    "                d_ff=800,#960\n",
    "                neck=180,#256\n",
    "                dropout=0.07#0.12\n",
    "                )\n",
    "            mod.load_state_dict(ch['model_state_dict'])\n",
    "            del(ch)\n",
    "            minimal_loss=100000\n",
    "            #alpha = trial.suggest_float(\"grokalpha\",0.0,1.0)\n",
    "            #lamb = trial.suggest_int(\"groklamb\",1,15)\n",
    "            #define_model(trial)\n",
    "            print(summary(mod))\n",
    "            lrE1=0.15 #trial.suggest_float(\"Learning_rate\",0.0002,0.5)\n",
    "            lrE2=0.05 \n",
    "            lrL=0.15 \n",
    "            lrD=0.085\n",
    "            optimizerEnc1 = Adam(mod.enc1.parameters(), lr=lrE1,eps=1e-10,weight_decay=0.005)\n",
    "            optimizerEnc2 = Adadelta(mod.enc2.parameters(), lr=lrE2,eps=1e-10,weight_decay=0.005)\n",
    "            optimizerDense = SGD(mod.vec2neck.parameters(), lr=lrL,weight_decay=0.005)\n",
    "            optimizerDec = Adadelta(mod.dec.parameters(), lr=lrD,eps=1e-10,weight_decay=0.01)\n",
    "    \n",
    "    \n",
    "    \n",
    "            sched_name=\"CyclicLR\"#trial.suggest_categorical(\"scheduler\",[\"CyclicLR\"])#,\"ReduceLROnPlateau\"])\n",
    "            if sched_name==\"CyclicLR\" :\n",
    "                step_size=24000#trial.suggest_int(\"step_size_up\",900,2800)\n",
    "                schedulerEnc1 = torch.optim.lr_scheduler.CyclicLR(optimizerEnc1, base_lr=1e-4, max_lr=lrE1, step_size_up=step_size, step_size_down=80000,scale_mode=\"iterations\",mode=\"triangular2\",cycle_momentum=False)\n",
    "                schedulerEnc2 = torch.optim.lr_scheduler.CyclicLR(optimizerEnc2, base_lr=1e-4, max_lr=lrE2, step_size_up=step_size, step_size_down=80000,scale_mode=\"iterations\",mode=\"triangular2\",cycle_momentum=False)\n",
    "                scheduler = torch.optim.lr_scheduler.CyclicLR(optimizerDense, base_lr=1e-4, max_lr=lrD, step_size_up=8000, step_size_down=8000,scale_mode=\"iterations\",mode=\"triangular\",cycle_momentum=False)\n",
    "    \n",
    "    \n",
    "            # if sched_name==\"ReduceLROnPlateau\" :\n",
    "            #     factor=trial.suggest_float(\"R-lr-OP_factor\",0.001,0.5)\n",
    "            #     threshhold=trial.suggest_float(\"R-lr-OP_threshhold\",0.0001,0.001)\n",
    "            #     threshold_mode = trial.suggest_categorical(\"thresh_mod\",[\"rel\",\"abs\"])\n",
    "            #     eps=trial.suggest_float(\"R-lr-OP_eps\",1e-08,1e-05)\n",
    "            #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=5, threshold=threshhold, threshold_mode=threshold_mode, cooldown=2, min_lr=0, eps=eps)\n",
    "    \n",
    "    \n",
    "            resume_epoch=0\n",
    "            # ch=torch.load(\"/media/crns/ADATA HD330/Experiments/mixed model/AE epoch 0 600.pth\",map_location=\"cpu\")\n",
    "            # resume_epoch=ch[\"epoch\"]\n",
    "            # mod.load_state_dict(ch['model_state_dict'])\n",
    "            # optimizerEnc1.load_state_dict(ch['optimizerENC1_state_dict'])\n",
    "            # optimizerEnc2.load_state_dict(ch['optimizerENC2_state_dict'])\n",
    "            # optimizerDense.load_state_dict(ch['optimizerDense_state_dict'])\n",
    "            # optimizerDec.load_state_dict(ch['optimizerDec_state_dict'])\n",
    "    \n",
    "            #del(ch)\n",
    "    \n",
    "    \n",
    "             #trial.suggest_int(\"batch_size\",150)\n",
    "    \n",
    "            criterion = nn.MSELoss()\n",
    "            LW=LWLN_loss()\n",
    "            LossWS=SamplesLoss('sinkhorn')\n",
    "            \n",
    "    \n",
    "            device = accelerator.device\n",
    "            run = wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"WR4CL-project\",\n",
    "            name= f\"{namef} {num_epochs} {combination} {str(datetime.datetime.now())}\" ,\n",
    "            # Track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"Scenario\":namef,\n",
    "                \"instances in train\":len(train_pair2),\n",
    "                \"Loss\":combination,\n",
    "                \"lr Encoder 1\": lrE1,\n",
    "                \"lr Encoder 2\": lrE2,\n",
    "                \"lr Linear\": lrL,\n",
    "                \"lr Decoder\": lrD,\n",
    "                \"epochs\": num_epochs,\n",
    "                \"sched_name\": sched_name,\n",
    "                \"step_size\": step_size,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"step_size\":step_size,\n",
    "                \"Lambda_contractive\":Lambda\n",
    "            },)\n",
    "    \n",
    "            #run = wandb.init(project=\"aymen-project\", id=\"ar1497fk\", resume=\"must\")\n",
    "    \n",
    "            # \"factor\":factor,\n",
    "            #     \"threshhold\":threshhold,\n",
    "            #     \"threshold_mode\":threshold_mode,\n",
    "            #     \"eps\":eps,\n",
    "            #random.shuffle(train_pair2)\n",
    "            cs_tr=CustomDataset(train_pair2,batch_size=batch_size)\n",
    "            nb_batches = len(cs_tr)//batch_size\n",
    "            \n",
    "            cs_val=CustomDataset(val_pair2,batch_size=batch_size)\n",
    "            nb_val_batches = len(cs_val)//batch_size\n",
    "            \n",
    "            #optimizer_to(optimizer,device)\n",
    "            #scheduler_to(scheduler,device)\n",
    "    \n",
    "            mod, optimizerEnc1,optimizerEnc2,optimizerDense,optimizerDec, cs_tr= accelerator.prepare(mod, optimizerEnc1,optimizerEnc2,optimizerDense,optimizerDec,cs_tr)\n",
    "            # wandb.watch(mod, log_freq=10000 ,criterion=criterion,\n",
    "            #     log='parameters',\n",
    "            #     log_graph=True)\n",
    "            #mod.train()\n",
    "            i=0\n",
    "            mod.train()\n",
    "            cs_tr=CustomDataset(train_pair2,batch_size=batch_size,batch_limit=batch_limit)\n",
    "            cs_tr=accelerator.prepare(cs_tr)\n",
    "            \n",
    "            used_Loss=0.0\n",
    "            for epoch in tqdm(range(resume_epoch,num_epochs),total=600):\n",
    "                # for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(locals().items())), key= lambda x: -x[1])[:10]:\n",
    "                #     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "                \n",
    "                resume=max(i,0)\n",
    "                \n",
    "                #start_time_epoch = time.time()\n",
    "                for i in range(resume,nb_batches):#nb_batches for training'''\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if cs_tr[i] is None :\n",
    "                        break\n",
    "                    Dataset,EXP,ACC,U = cs_tr[i]\n",
    "                    \n",
    "                    x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]\n",
    "                    print(d_loosx1.shape,x2.shape,s3.shape)\n",
    "                    # if (Loss==\"Contractive\") and (i%50==0):\n",
    "                    #     for name, param in mod.named_parameters():\n",
    "                    #         if name == 'vec2neck.weight':\n",
    "                    #             W = param\n",
    "                    #             break\n",
    "    \n",
    "    \n",
    "                    x1=x1.cuda() #.to(torch.float32)\n",
    "                    x2=x2.cuda() #.to(torch.float32)\n",
    "                    tg=tg.cuda() #.to(torch.float32)\n",
    "    \n",
    "                    optimizerEnc1.zero_grad()\n",
    "                    optimizerEnc2.zero_grad()\n",
    "                    optimizerDense.zero_grad()\n",
    "                    optimizerDec.zero_grad()\n",
    "    \n",
    "                    #output = mod(x1,x2)\n",
    "                    d_loos,output,mod=update_all_metrics(d_loos,mod,x1,x2,tg,DF)\n",
    "                    torch.nn.utils.clip_grad_norm_(mod.parameters(), max_norm=1.0)\n",
    "    \n",
    "    \n",
    "                    used_Loss=0\n",
    "                    print(used_Loss,d_loos)\n",
    "                    if isinstance(combination, list):\n",
    "                        for pair in d_loos.items():\n",
    "                            if pair[0]==combination[0] or pair[0]==combination[1] :\n",
    "                                used_Loss=used_Loss+pair[1]\n",
    "                    if isinstance(combination, str):\n",
    "                        for pair in d_loos.items():\n",
    "                            if pair[0]==combination: \n",
    "                                used_Loss=used_Loss+pair[1]\n",
    "                    #print(combination ,\"\\t \\t\",used_Loss)\n",
    "    \n",
    "                    accelerator.backward(used_Loss)\n",
    "                    #loss_tr.backward()\n",
    "                    #grads = gradfilter_ema(mod, grads=grads, alpha=alpha, lamb=lamb)\n",
    "                    frob = frobnorm(mod)\n",
    "                    optimizerEnc1.step()\n",
    "                    optimizerEnc2.step()\n",
    "                    optimizerDense.step()\n",
    "                    optimizerDec.step()\n",
    "                    if sched_name==\"ReduceLROnPlateau\" :\n",
    "                        scheduler.step(used_Loss)\n",
    "                    else:\n",
    "                        scheduler.step()\n",
    "                        schedulerEnc1.step()\n",
    "                        schedulerEnc2.step()\n",
    "                    \n",
    "                    loss_to_save = float(used_Loss.detach().cpu().item())\n",
    "                    wandb.log({f\"Tracked Loss {combination}\":loss_to_save})\n",
    "                \n",
    "                \n",
    "                \n",
    "                if used_Loss.detach().cpu().item()<minimal_loss:\n",
    "                    minimal_loss=used_Loss.detach().cpu().item()\n",
    "                    torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),\n",
    "                                'optimizerENC1_state_dict':  optimizerEnc1.state_dict() ,\n",
    "                                'optimizerENC2_state_dict':optimizerEnc2.state_dict(),\n",
    "                                'optimizerDense_state_dict':optimizerDense.state_dict(),\n",
    "                                'optimizerDec_state_dict': optimizerDec.state_dict(),\n",
    "                                'Batch Loss':used_Loss.detach().cpu().item()},\n",
    "                                results_path+'AE epoch best_.pth')\n",
    "\n",
    "                end_time = time.time() \n",
    "                execution_time = end_time - start_time  \n",
    "                if execution_time>4000 :\n",
    "                        break\n",
    "                if (epoch+1)%50==0 or (epoch+1) in [5,10,30]:\n",
    "                    #print(\"validating\")\n",
    "                    for block in [2,3,4]:\n",
    "                            for head in range(4):\n",
    "                                plt.figure(figsize=(20, 20))\n",
    "                                hm=sns.heatmap(torch.mean( torch.mean(output[block][head], dim=1), dim=0).detach().cpu(), annot=False, cmap='cubehelix')\n",
    "                                plt.title('Attention Heatmap')\n",
    "                                heatmap_path = f'heatmap {block-2}_{head}_step_{i}.png'\n",
    "                                #plt.savefig(results_path+\"Attention/\"+heatmap_path)#,format='svg', dpi=800)\n",
    "                                wandb.log({f\"attention_heatmap {block-2}_{head}\":  wandb.Image(hm,caption=f\"attention_heatmap attention_heatmap {block-2}_{head}\")})\n",
    "                                plt.close()\n",
    "                    mod.eval()\n",
    "                    loss_val = []\n",
    "                    for i_val in range(nb_val_batches):#nb_val_batches batches for validation'''\n",
    "                        #start_time_batch = time.time()\n",
    "                        Dataset_val,EXP_val,ACC_val,U_val = cs_val[i_val]\n",
    "                        x1_val,x2_val,tg_val = Dataset_val[:,0,:], Dataset_val[:,1,:],Dataset_val[:,2,:]\n",
    "                        x1_val=x1_val.cuda() #.to(torch.float32)\n",
    "                        x2_val=x2_val.cuda() #.to(torch.float32)\n",
    "                        tg_val=tg_val.cuda() #.to(torch.float32)\n",
    "                        with torch.no_grad():\n",
    "                            #output_val = mod(x1_val,x2_val)\n",
    "                            loss_val.append(criterion(mod(x1_val,x2_val)[0],tg_val))\n",
    "                    loss_value = sum(loss_val)/len(loss_val)\n",
    "                    wandb.log({f\"val_loss\":loss_value})\n",
    "                    #print(\"entering test set\")\n",
    "                    cs_ts=CustomDataset(test_pair2,batch_size=batch_size)\n",
    "                    nb_test_batches = len(cs_ts)//batch_size\n",
    "                    \n",
    "                    mod,cs_ts= accelerator.prepare(mod, cs_ts)\n",
    "                    mod.eval()\n",
    "    \n",
    "                    loss_values = []\n",
    "                    for j in range(nb_test_batches): #nb_test_batches number of batches for testing'''\n",
    "                        Dataset,EXP,ACC,U = cs_ts[j]\n",
    "                        x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]\n",
    "    \n",
    "                        x1=x1.cuda() #.to(torch.float32)\n",
    "                        x2=x2.cuda() #.to(torch.float32)\n",
    "                        tg=tg.cuda() #.to(torch.float32)\n",
    "                        with torch.no_grad():\n",
    "                            output = mod(x1,x2)\n",
    "                            loss_values.append(criterion(output[0],tg))\n",
    "    \n",
    "                        for vec in range(len(x1)):\n",
    "                            DF.at[track,\"label task 1\"]=f'{EXP[vec][0]}'\n",
    "                            DF.at[track,\"label task 2\"]=f'{EXP[vec][1]}'\n",
    "                            #print(Cols[2:2466][-2:],Cols[2466:4930][-2:],Cols[4930:7394][-2:])\n",
    "    \n",
    "                            vec1=mod(x1_val,x2_val)[0][vec].detach().cpu()\n",
    "                            vec2=tg_val[vec].detach().cpu()\n",
    "                            DF.loc[track,Cols[2:2466]]=vec2.tolist()\n",
    "                            DF.loc[track,Cols[2466:4930]]=vec1.tolist()#Cols[4930:7394][-2:])\n",
    "    \n",
    "    \n",
    "    \n",
    "                            MSE=criterion(vec1,vec2).item()\n",
    "                            MS1=criterion(vec1[:208],vec2[:208]).item()\n",
    "                            MS2=criterion(vec1[208:1414],vec2[208:1414]).item()\n",
    "                            MS3=criterion(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "                            MS4=criterion(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "                            MS5=criterion(vec1[2254:],vec2[2254:]).item()\n",
    "                            #print(\"MSE :\",MS1,MS2,MS3,MS4,MS5,MSE)\n",
    "                            DF.at[track,\"MSE\"]=MSE\n",
    "                            DF.at[track,\"MSE 1\"]=MS1\n",
    "                            DF.at[track,\"MSE 2\"]=MS2\n",
    "                            DF.at[track,\"MSE 3\"]=MS3\n",
    "                            DF.at[track,\"MSE 4\"]=MS4\n",
    "                            DF.at[track,\"MSE 5\"]=MS5\n",
    "    \n",
    "    \n",
    "    \n",
    "                            kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "                            KLD=kl_loss(vec1,vec2).item() #pred, true\n",
    "                            KL1=kl_loss(vec1[:208],vec2[:208]).item()\n",
    "                            KL2=kl_loss(vec1[208:1414],vec2[208:1414]).item()\n",
    "                            KL3=kl_loss(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "                            KL4=kl_loss(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "                            KL5=kl_loss(vec1[2254:],vec2[2254:]).item()\n",
    "                            #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "                            DF.at[track,\"KLD\"]=KLD\n",
    "                            DF.at[track,\"KL 1\"]=KL1\n",
    "                            DF.at[track,\"KL 2\"]=KL2\n",
    "                            DF.at[track,\"KL 3\"]=KL3\n",
    "                            DF.at[track,\"KL 4\"]=KL4\n",
    "                            DF.at[track,\"KL 5\"]=KL5\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                            WSD=wasserstein_distance(vec1,vec2)\n",
    "                            WS1=wasserstein_distance(vec1[:208],vec2[:208])\n",
    "                            WS2=wasserstein_distance(vec1[208:1414],vec2[208:1414])\n",
    "                            WS3=wasserstein_distance(vec1[1414:1514],vec2[1414:1514])\n",
    "                            WS4=wasserstein_distance(vec1[1514:2254],vec2[1514:2254])\n",
    "                            WS5=wasserstein_distance(vec1[2254:],vec2[2254:])\n",
    "                            #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "                            DF.at[track,\"Wasserstein Loss\"]=WSD\n",
    "                            DF.at[track,\"WS 1\"]=WS1\n",
    "                            DF.at[track,\"WS 2\"]=WS2\n",
    "                            DF.at[track,\"WS 3\"]=WS3\n",
    "                            DF.at[track,\"WS 4\"]=WS4\n",
    "                            DF.at[track,\"WS 5\"]=WS5\n",
    "    \n",
    "                            for name, param in mod.named_parameters():\n",
    "                                if name == 'vec2neck.weight':\n",
    "                                    W = param\n",
    "                                    break\n",
    "    \n",
    "                            CL=loss_Contractive(W,vec1,vec2, output[1], 0.00001)\n",
    "                            DF.at[track,\"contractive distance\"]=CL.cpu().item()\n",
    "    \n",
    "                            #print(\"Contractive :\",CL.cpu().item())\n",
    "                            Norm1=np.sum(np.abs(vec1.numpy()))\n",
    "                            N11=np.sum(np.abs(vec1[:208].numpy()))\n",
    "                            N12=np.sum(np.abs(vec1[208:1414].numpy()))\n",
    "                            N13=np.sum(np.abs(vec1[1414:1514].numpy()))\n",
    "                            N14=np.sum(np.abs(vec1[1514:2254].numpy()))\n",
    "                            N15=np.sum(np.abs(vec1[2254:].numpy()))\n",
    "                            #print(\"Weight pred L1: \",Norm1, N11,N12,N13,N14,N15)\n",
    "                            DF.at[track,\"N1\"]=Norm1\n",
    "                            DF.at[track,\"N11\"]=N11\n",
    "                            DF.at[track,\"N12\"]=N12\n",
    "                            DF.at[track,\"N13\"]=N13\n",
    "                            DF.at[track,\"N14\"]=N14\n",
    "                            DF.at[track,\"N15\"]=N15\n",
    "    \n",
    "                            Norm2=np.sum(np.abs(vec2.numpy()))\n",
    "                            N21=np.sum(np.abs(vec2[:208].numpy()))\n",
    "                            N22=np.sum(np.abs(vec2[208:1414].numpy()))\n",
    "                            N23=np.sum(np.abs(vec2[1414:1514].numpy()))\n",
    "                            N24=np.sum(np.abs(vec2[1514:2254].numpy()))\n",
    "                            N25=np.sum(np.abs(vec2[2254:].numpy()))\n",
    "                            #print(\"Weight GT L1: \",Norm2, N21,N22,N23,N24,N25)\n",
    "                            DF.at[track,\"N2\"]=Norm2\n",
    "                            DF.at[track,\"N21\"]=N21\n",
    "                            DF.at[track,\"N22\"]=N22\n",
    "                            DF.at[track,\"N23\"]=N23\n",
    "                            DF.at[track,\"N24\"]=N24\n",
    "                            DF.at[track,\"N25\"]=N25\n",
    "    \n",
    "                            DF.at[track,\"saturated in pred(%)\"]=100*sum(1 for x in vec1 if x > 0.95 or x<0.05)/len(vec1)\n",
    "                            DF.at[track,\"saturated in GT(%)\"]=100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2)\n",
    "                            DF.at[track,\"LWLN\"]=LWs(vec1,vec2).cpu().item()\n",
    "                            if vec<cnn_samples and j==0:\n",
    "                                figure=get_plot_object(vec1, tg)\n",
    "                                wandb.log({\"Plot of pred vs target\":  wandb.Image(figure,caption=f\"Plot of pred vs target of model {vec}\")})\n",
    "                                plt.close(figure)\n",
    "    \n",
    "                            #import umap\n",
    "                            # reducer = umap.UMAP()\n",
    "                            # Batch_UMAP=torch.rand(80,2464)\n",
    "                            # fitted_reduced = reducer.fit(Batch_UMAP)\n",
    "                            # fitted_reduced.transform(vec1)\n",
    "    \n",
    "                            ###########RECONSTRUCTING##############\n",
    "                            y_pred=torch.unsqueeze(output[0][vec], 0) \n",
    "                            y =torch.unsqueeze(tg[vec], 0) \n",
    "    \n",
    "                            selected_row = cs_ts.df.iloc[int(U[vec][0]), 11:17]  \n",
    "                            columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "                            activ=columns_with_one\n",
    "                            epochCNN=cs_ts.df.loc[int(U[vec][0])]['epoch']\n",
    "    \n",
    "    \n",
    "                            checkpoint=OrderedDict()\n",
    "                            checkpoint1=OrderedDict()\n",
    "                            checkpoint2=OrderedDict()\n",
    "                            vector_aux= output[0][vec].detach()\n",
    "                            y_pred=vector_aux.cpu()\n",
    "    \n",
    "                            task1=[int(x) for x in EXP[vec][0]]\n",
    "                            task2=[int(x) for x in EXP[vec][1]]\n",
    "                            task3=sorted(task1+task2)\n",
    "    \n",
    "    \n",
    "                            All=list(range(10))\n",
    "                            L2=[k for k in All if k not in task3] # Out of distribution classes\n",
    "                            L_others=[k for k in All if k not in task3] #Classes to test on (In distribution)\n",
    "    \n",
    "                            checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(y_pred[0:200]).reshape([8, 1, 5, 5]))\n",
    "                            checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(y_pred[200:208]).reshape([8]))\n",
    "    \n",
    "                            checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(y_pred[208:1408]).reshape([6, 8, 5, 5]))\n",
    "                            checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(y_pred[1408:1414]).reshape([6]))\n",
    "    \n",
    "                            checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(y_pred[1414:1510]).reshape([4, 6, 2, 2]))\n",
    "                            checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(y_pred[1510:1514]).reshape([4]))\n",
    "    \n",
    "                            checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(y_pred[1514:2234]).reshape([20,36]))\n",
    "                            checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(y_pred[2234:2254]).reshape([20]))\n",
    "    \n",
    "                            checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(y_pred[2254:2454]).reshape([10,20]))\n",
    "                            checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(y_pred[2454:2464]).reshape([10]))\n",
    "    \n",
    "                            Brain = CNN(1,activ[0],0,\"kaiming_uniform\")\n",
    "    \n",
    "                            model=copy.deepcopy(Brain)\n",
    "                            model.load_state_dict(checkpoint)\n",
    "                            \n",
    "                            if vec<cnn_samples and j==0 :#predicted weights spectral histograms'''\n",
    "                                \n",
    "                                checkpoint1[\"module_list.0.weight\"]=torch.tensor(np.array(x1_val[vec][0:200].cpu()).reshape([8, 1, 5, 5]))\n",
    "                                checkpoint1[\"module_list.0.bias\"]=torch.tensor(np.array(x1_val[vec][200:208].cpu()).reshape([8]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.3.weight\"]=torch.tensor(np.array(x1_val[vec][208:1408].cpu()).reshape([6, 8, 5, 5]))\n",
    "                                checkpoint1[\"module_list.3.bias\"]=torch.tensor(np.array(x1_val[vec][1408:1414].cpu()).reshape([6]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.6.weight\"]=torch.tensor(np.array(x1_val[vec][1414:1510].cpu()).reshape([4, 6, 2, 2]))\n",
    "                                checkpoint1[\"module_list.6.bias\"]=torch.tensor(np.array(x1_val[vec][1510:1514].cpu()).reshape([4]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.9.weight\"]=torch.tensor(np.array(x1_val[vec][1514:2234].cpu()).reshape([20,36]))\n",
    "                                checkpoint1[\"module_list.9.bias\"]=torch.tensor(np.array(x1_val[vec][2234:2254].cpu()).reshape([20]))\n",
    "        \n",
    "                                checkpoint1[\"module_list.11.weight\"]=torch.tensor(np.array(x1_val[vec][2254:2454].cpu()).reshape([10,20]))\n",
    "                                checkpoint1[\"module_list.11.bias\"]=torch.tensor(np.array(x1_val[vec][2454:2464].cpu()).reshape([10]))\n",
    "                            \n",
    "                                CNN_x1=copy.deepcopy(Brain)\n",
    "                                CNN_x1.load_state_dict(checkpoint1)\n",
    "                                # Extract weights from a specific layer (e.g., 'fc' layer)\n",
    "                                # for name, param in CNN_x1.named_parameters():\n",
    "                                #     if len(param.shape)>1:\n",
    "                                #         weights = param\n",
    "                                #         weights = weights.view(weights.shape[0], -1)\n",
    "                                #         weights = weights.T @ weights\n",
    "                                #         # Compute eigenvalues of the weight matrix\n",
    "                                #         eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "    \n",
    "                                #         # Extract real and imaginary parts\n",
    "                                #         real_eigenvalues = eigenvalues.real\n",
    "                                #         imag_eigenvalues = eigenvalues.imag\n",
    "    \n",
    "                                #             # Plot histogram of the real parts of the eigenvalues\n",
    "                                #         plt.figure(figsize=(8, 6))\n",
    "                                #         plt.hist(real_eigenvalues, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "                                #         plt.xlabel('Real Part of Eigenvalues')\n",
    "                                #         plt.ylabel('Frequency')\n",
    "                                #         plt.title(f'Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}')\n",
    "                                        \n",
    "                                #         plt.grid(True)\n",
    "                                #         #plt.show()\n",
    "                                #         image_path = results_path+f\"Tracking/Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}.png\"\n",
    "                                #         plt.savefig(image_path)\n",
    "                                        \n",
    "                                #         wandb.log({f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\":  wandb.Image(image_path,caption=f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\")})\n",
    "                                #         plt.close()\n",
    "                                table = wandb.Table(columns=[\"Layer Name\", \"Parameter Shape\", \"Real Eigenvalues\"])\n",
    "\n",
    "                                for name, param in CNN_x1.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                        \n",
    "                                        # Add data to the table\n",
    "                                        table.add_data(name, tuple(param.shape), real_eigenvalues.tolist())\n",
    "                                \n",
    "                                checkpoint2[\"module_list.0.weight\"]=torch.tensor(np.array(x2_val[vec][0:200].cpu()).reshape([8, 1, 5, 5]))\n",
    "                                checkpoint2[\"module_list.0.bias\"]=torch.tensor(np.array(x2_val[vec][200:208].cpu()).reshape([8]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.3.weight\"]=torch.tensor(np.array(x2_val[vec][208:1408].cpu()).reshape([6, 8, 5, 5]))\n",
    "                                checkpoint2[\"module_list.3.bias\"]=torch.tensor(np.array(x2_val[vec][1408:1414].cpu()).reshape([6]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.6.weight\"]=torch.tensor(np.array(x2_val[vec][1414:1510].cpu()).reshape([4, 6, 2, 2]))\n",
    "                                checkpoint2[\"module_list.6.bias\"]=torch.tensor(np.array(x2_val[vec][1510:1514].cpu()).reshape([4]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.9.weight\"]=torch.tensor(np.array(x2_val[vec][1514:2234].cpu()).reshape([20,36]))\n",
    "                                checkpoint2[\"module_list.9.bias\"]=torch.tensor(np.array(x2_val[vec][2234:2254].cpu()).reshape([20]))\n",
    "        \n",
    "                                checkpoint2[\"module_list.11.weight\"]=torch.tensor(np.array(x2_val[vec][2254:2454].cpu()).reshape([10,20]))\n",
    "                                checkpoint2[\"module_list.11.bias\"]=torch.tensor(np.array(x2_val[vec][2454:2464].cpu()).reshape([10]))\n",
    "                            \n",
    "                                CNN_x2=copy.deepcopy(Brain)\n",
    "                                CNN_x2.load_state_dict(checkpoint2)\n",
    "                                # Extract weights from a specific layer (e.g., 'fc' layer)\n",
    "                                # for name, param in CNN_x2.named_parameters():\n",
    "                                #     if len(param.shape)>1:\n",
    "                                #         weights = param\n",
    "                                #         weights = weights.view(weights.shape[0], -1)\n",
    "                                #         weights = weights.T @ weights\n",
    "                                #         # Compute eigenvalues of the weight matrix\n",
    "                                #         eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "    \n",
    "                                #         # Extract real and imaginary parts\n",
    "                                #         real_eigenvalues = eigenvalues.real\n",
    "                                #         imag_eigenvalues = eigenvalues.imag\n",
    "    \n",
    "                                #             # Plot histogram of the real parts of the eigenvalues\n",
    "                                #         plt.figure(figsize=(8, 6))\n",
    "                                #         plt.hist(real_eigenvalues, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "                                #         plt.xlabel('Real Part of Eigenvalues')\n",
    "                                #         plt.ylabel('Frequency')\n",
    "                                #         plt.title(f'Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}')\n",
    "                                        \n",
    "                                #         plt.grid(True)\n",
    "                                #         #plt.show()\n",
    "                                #         image_path = results_path+f\"Tracking/Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}.png\"\n",
    "                                #         plt.savefig(image_path)\n",
    "                                        \n",
    "                                #         wandb.log({f\"Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\":  wandb.Image(image_path,caption=f\"Input 2 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\")})\n",
    "                                #         plt.close()\n",
    "                                table2 = wandb.Table(columns=[\"Layer Name\", \"Parameter Shape\", \"Real Eigenvalues\"])\n",
    "\n",
    "                                for name, param in CNN_x2.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                        \n",
    "                                        # Add data to the table\n",
    "                                        table2.add_data(name, tuple(param.shape), real_eigenvalues.tolist())\n",
    "    \n",
    "                            # for name, param in model.named_parameters():\n",
    "                            #         if len(param.shape)>1:\n",
    "                            #             weights = param\n",
    "                            #             weights = weights.view(weights.shape[0], -1)\n",
    "                            #             weights = weights.T @ weights\n",
    "                            #             # Compute eigenvalues of the weight matrix\n",
    "                            #             eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "    \n",
    "                            #             # Extract real and imaginary parts\n",
    "                            #             real_eigenvalues = eigenvalues.real\n",
    "                            #             imag_eigenvalues = eigenvalues.imag\n",
    "    \n",
    "                            #                 # Plot histogram of the real parts of the eigenvalues\n",
    "                            #             plt.figure(figsize=(8, 6))\n",
    "                            #             plt.hist(real_eigenvalues, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "                            #             plt.xlabel('Real Part of Eigenvalues')\n",
    "                            #             plt.ylabel('Frequency')\n",
    "                            #             plt.title(f'Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}')\n",
    "                                        \n",
    "                            #             plt.grid(True)\n",
    "                            #             #plt.show()\n",
    "                            #             image_path = results_path+f\"Tracking/Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}.png\"\n",
    "                            #             plt.savefig(image_path)\n",
    "                                        \n",
    "                            #             wandb.log({f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\":  wandb.Image(image_path,caption=f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec} \")})\n",
    "                            #             plt.close()\n",
    "                                tabl3 = wandb.Table(columns=[\"Layer Name\", \"Parameter Shape\", \"Real Eigenvalues\"])\n",
    "\n",
    "                                for name, param in model.named_parameters():\n",
    "                                    if len(param.shape) > 1:\n",
    "                                        weights = param\n",
    "                                        weights = weights.view(weights.shape[0], -1)\n",
    "                                        weights = weights.T @ weights\n",
    "                                        \n",
    "                                        # Compute eigenvalues of the weight matrix\n",
    "                                        eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                        \n",
    "                                        # Extract real and imaginary parts\n",
    "                                        real_eigenvalues = eigenvalues.real\n",
    "                                        imag_eigenvalues = eigenvalues.imag\n",
    "                                        \n",
    "                                        # Log histogram to wandb\n",
    "                                        wandb.log({\n",
    "                                            f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                            wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                        })\n",
    "                                        \n",
    "                                        # Add data to the table\n",
    "                                        table3.add_data(name, tuple(param.shape), real_eigenvalues.tolist())\n",
    "                            criterion_CNN0=CrossEntropyLoss()\n",
    "    \n",
    "                            test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                            Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=0, shuffle=False)\n",
    "    \n",
    "                            _, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "                            if len(task3)==10:\n",
    "                                valid_epoch_acc1=valid_epoch_acc0\n",
    "                                continue\n",
    "                            else:\n",
    "                                criterion_CNN1=CrossEntropyLoss()\n",
    "                                test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                                Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=120, num_workers=0, shuffle=False)\n",
    "    \n",
    "                            #valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n",
    "                            #print(\"Reconstructed cnn acc ID\",valid_epoch_acc0)\n",
    "                            #print(\"Reconstructed cnn acc OOD\",valid_epoch_acc1)\n",
    "                            DF.at[track,\"Reconstructed Accuracy ID\"]=valid_epoch_acc0\n",
    "    \n",
    "                            optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "                            schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "                            criterion_CNN=CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "                            train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                            Tr_DLr = DataLoader(dataset=train_IF0, batch_size=150, num_workers=0, shuffle=True)\n",
    "    \n",
    "    \n",
    "                            fine_tune_needed=0\n",
    "                            #FINETUNING\n",
    "                            for epoch_cnn in range(3):\n",
    "                                if epoch_cnn==0:\n",
    "                                    train_epoch_loss, train_epoch_acc,_ = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF,First=True)\n",
    "                                    valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "                                    DF.at[track,\"epoch 0\"]=valid_epoch_acc0FN\n",
    "                                else:\n",
    "                                    train_epoch_loss, train_epoch_acc,_ = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF)\n",
    "                                    valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "                                    DF.at[track,f\"epoch {epoch_cnn}\"]=valid_epoch_acc0FN\n",
    "                                schedulerCNN.step()\n",
    "                                fine_tune_needed+=1\n",
    "                            \n",
    "                            #Finetuned weights spectral histograms'''\n",
    "                            # for name, param in model.named_parameters():\n",
    "                            #     if len(param.shape)>1:\n",
    "                            #         weights = param\n",
    "                            #         weights = weights.view(weights.shape[0], -1)\n",
    "                            #         weights = weights.T @ weights\n",
    "                            #         # Compute eigenvalues of the weight matrix\n",
    "                            #         eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "    \n",
    "                            #         # Extract real and imaginary parts\n",
    "                            #         real_eigenvalues = eigenvalues.real\n",
    "                            #         imag_eigenvalues = eigenvalues.imag\n",
    "    \n",
    "                            #             # Plot histogram of the real parts of the eigenvalues\n",
    "                            #         plt.figure(figsize=(8, 6))\n",
    "                            #         plt.hist(real_eigenvalues, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "                            #         plt.xlabel('Real Part of Eigenvalues')\n",
    "                            #         plt.ylabel('Frequency')\n",
    "                            #         plt.title(f'Real Parts of Finetuned Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}')\n",
    "                                    \n",
    "                            #         plt.grid(True)\n",
    "                            #         #plt.show()\n",
    "                            #         image_path = results_path+f\"Tracking/Real Parts of Finetuned Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}.png\"\n",
    "                            #         plt.savefig(image_path)\n",
    "                                    \n",
    "                            #         wandb.log({f\"Real Parts of Finetuned  Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\":  wandb.Image(image_path,caption=f\"Real Parts of Finetuned Eigenvalues of layer {name} shaped {tuple(param.shape)} model number {vec}\")})\n",
    "                            #         plt.close()\n",
    "                            table4 = wandb.Table(columns=[\"Layer Name\", \"Parameter Shape\", \"Real Eigenvalues\"])\n",
    "\n",
    "                            for name, param in model.named_parameters():\n",
    "                                if len(param.shape) > 1:\n",
    "                                    weights = param\n",
    "                                    weights = weights.view(weights.shape[0], -1)\n",
    "                                    weights = weights.T @ weights\n",
    "                                    \n",
    "                                    # Compute eigenvalues of the weight matrix\n",
    "                                    eigenvalues = torch.linalg.eigvals(weights.detach().cpu()).numpy()\n",
    "                                    \n",
    "                                    # Extract real and imaginary parts\n",
    "                                    real_eigenvalues = eigenvalues.real\n",
    "                                    imag_eigenvalues = eigenvalues.imag\n",
    "                                    \n",
    "                                    # Log histogram to wandb\n",
    "                                    wandb.log({\n",
    "                                        f\"Input 1 Eigenvalues of layer {name} shaped {tuple(param.shape)}\": \n",
    "                                        wandb.Histogram(real_eigenvalues, num_bins=64)\n",
    "                                    })\n",
    "                                    \n",
    "                                    # Add data to the table\n",
    "                                    table4.add_data(name, tuple(param.shape), real_eigenvalues.tolist())\n",
    "    \n",
    "                            L_param=[]\n",
    "                            for param in model.parameters():\n",
    "                                m = nn.Flatten(0,-1)\n",
    "                                L_param.append(m(param))\n",
    "                            vec1FN = torch.Tensor()\n",
    "                            for idx in L_param:\n",
    "                                vec1FN = torch.cat((vec1FN, idx.view(-1)))\n",
    "                            vec1FN=vec1FN.detach().cpu()\n",
    "                            vec2=tg[vec].cpu()\n",
    "                            DF.loc[track,Cols[4930:7394]]=vec1FN.tolist()\n",
    "                            MSE=criterion(vec1FN,vec2).item()\n",
    "                            MS1=criterion(vec1FN[:208],vec2[:208]).item()\n",
    "                            MS2=criterion(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "                            MS3=criterion(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "                            MS4=criterion(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "                            MS5=criterion(vec1FN[2254:],vec2[2254:]).item()\n",
    "    \n",
    "    \n",
    "                            #print(\"MSE :\",MS1,MS2,MS3,MS4,MS5,MSE)\n",
    "                            DF.at[track,\"MSE FN\"]=MSE\n",
    "                            DF.at[track,\"MSE 1 FN\"]=MS1\n",
    "                            DF.at[track,\"MSE 2 FN\"]=MS2\n",
    "                            DF.at[track,\"MSE 3 FN\"]=MS3\n",
    "                            DF.at[track,\"MSE 4 FN\"]=MS4\n",
    "                            DF.at[track,\"MSE 5 FN\"]=MS5\n",
    "                            kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "                            KLD=kl_loss(vec1FN,vec2).item() #pred, true\n",
    "                            KL1=kl_loss(vec1FN[:208],vec2[:208]).item()\n",
    "                            KL2=kl_loss(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "                            KL3=kl_loss(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "                            KL4=kl_loss(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "                            KL5=kl_loss(vec1FN[2254:],vec2[2254:]).item()\n",
    "                            #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "                            DF.at[track,\"KL divergence FN\"]=KLD\n",
    "                            DF.at[track,\"KL 1 FN\"]=KL1\n",
    "                            DF.at[track,\"KL 2 FN\"]=KL2\n",
    "                            DF.at[track,\"KL 3 FN\"]=KL3\n",
    "                            DF.at[track,\"KL 4 FN\"]=KL4\n",
    "                            DF.at[track,\"KL 5 FN\"]=KL5\n",
    "    \n",
    "                            WSD=wasserstein_distance(vec1FN,vec2)\n",
    "                            WS1=wasserstein_distance(vec1FN[:208],vec2[:208])\n",
    "                            WS2=wasserstein_distance(vec1FN[208:1414],vec2[208:1414])\n",
    "                            WS3=wasserstein_distance(vec1FN[1414:1514],vec2[1414:1514])\n",
    "                            WS4=wasserstein_distance(vec1FN[1514:2254],vec2[1514:2254])\n",
    "                            WS5=wasserstein_distance(vec1FN[2254:],vec2[2254:])\n",
    "                            #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "    \n",
    "                            DF.at[track,\"WSD FN\"]=WSD\n",
    "                            DF.at[track,\"WS 1 FN\"]=WS1\n",
    "                            DF.at[track,\"WS 2 FN\"]=WS2\n",
    "                            DF.at[track,\"WS 3 FN\"]=WS3\n",
    "                            DF.at[track,\"WS 4 FN\"]=WS4\n",
    "                            DF.at[track,\"WS 5 FN\"]=WS5\n",
    "                            for name, param in mod.named_parameters():\n",
    "                                if name == 'vec2neck.weight':\n",
    "                                    W = param\n",
    "                                    break\n",
    "                            CL=loss_Contractive(W,vec1FN,vec2, output[1], 0.00001)        \n",
    "                            #print(\"Contractive :\",CL.cpu().item())\n",
    "                            DF.at[track,\"contractive distance FN\"]=CL.cpu().item()\n",
    "                            Norm1=np.sum(np.abs(vec1FN.numpy()))\n",
    "                            N11=np.sum(np.abs(vec1FN[:208].numpy()))\n",
    "                            N12=np.sum(np.abs(vec1FN[208:1414].numpy()))\n",
    "                            N13=np.sum(np.abs(vec1FN[1414:1514].numpy()))\n",
    "                            N14=np.sum(np.abs(vec1FN[1514:2254].numpy()))\n",
    "                            N15=np.sum(np.abs(vec1FN[2254:].numpy()))\n",
    "                            #print(\"Weight pred L1: \",Norm1, N11,N12,N13,N14,N15)\n",
    "                            DF.at[track,\"N1 FN\"]=Norm1\n",
    "                            DF.at[track,\"N11 FN\"]=N11\n",
    "                            DF.at[track,\"N12 FN\"]=N12\n",
    "                            DF.at[track,\"N13 FN\"]=N13\n",
    "                            DF.at[track,\"N14 FN\"]=N14\n",
    "                            DF.at[track,\"N15 FN\"]=N15\n",
    "                            Norm2=np.sum(np.abs(vec2.numpy()))\n",
    "                            N21=np.sum(np.abs(vec2[:208].numpy()))\n",
    "                            N22=np.sum(np.abs(vec2[208:1414].numpy()))\n",
    "                            N23=np.sum(np.abs(vec2[1414:1514].numpy()))\n",
    "                            N24=np.sum(np.abs(vec2[1514:2254].numpy()))\n",
    "                            N25=np.sum(np.abs(vec2[2254:].numpy()))\n",
    "                            #print(\"Weight GT L1: \",Norm2, N21,N22,N23,N24,N25)\n",
    "                            DF.at[track,\"N2 FN\"]=Norm2\n",
    "                            DF.at[track,\"N21 FN\"]=N21\n",
    "                            DF.at[track,\"N22 FN\"]=N22\n",
    "                            DF.at[track,\"N23 FN\"]=N23\n",
    "                            DF.at[track,\"N24 FN\"]=N24\n",
    "                            DF.at[track,\"N25 FN\"]=N25\n",
    "                            #print(\"Saturation in pred \",100*sum(1 for x in vec1FN if x > 0.95 or x<0.05)/len(vec1FN),\"% \\t saturaion in target\",100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2),\"%\")\n",
    "                            #print(\"LWLN \",LW(vec1FN,vec2).item())\n",
    "                            DF.at[track,\"saturated in pred FN(%)\"]=100*sum(1 for x in vec1FN if x > 0.95 or x<0.05)/len(vec1FN)\n",
    "                            DF.at[track,\"saturated in GT FN(%)\"]=100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2)\n",
    "                            DF.at[track,\"LWLN FN\"]=LWs(vec1FN,vec2).cpu().item()\n",
    "    \n",
    "                            \n",
    "                            \n",
    "                            DF.to_csv(results_path+f\"Tracking/_resumed {epoch}.csv\")\n",
    "                            track=track+1\n",
    "    \n",
    "                    # Define filter function â€“ can be any scikit-learn transformer\n",
    "                    filter_func = Projection(columns=[0,1])\n",
    "                    # Define cover\n",
    "                    cover = CubicalCover(n_intervals=8, overlap_frac=0.5)\n",
    "                    # Choose clustering algorithm â€“ default is DBSCAN\n",
    "                    clusterer = DBSCAN()\n",
    "    \n",
    "                    # Configure parallelism of clustering step\n",
    "                    n_jobs = 8\n",
    "    \n",
    "                    GT=DF[Cols[2:2466]]\n",
    "                    dataG=GT.to_numpy()\n",
    "                    #print(Cols[2],Cols[2466],Cols[4930])\n",
    "                    PD=DF[Cols[2466:4930]]\n",
    "                    dataP=PD.to_numpy()\n",
    "    \n",
    "                    FN=DF[Cols[4930:7394]]\n",
    "                    dataF=FN.to_numpy()\n",
    "                    # Initialise pipeline\n",
    "                    pipe = make_mapper_pipeline(filter_func=filter_func,cover=cover,clusterer=clusterer,verbose=False,n_jobs=n_jobs)\n",
    "    \n",
    "    \n",
    "                    plotly_params = {\"node_trace\": {\"marker_colorscale\": \"Blues\"}}\n",
    "                    fig = plot_static_mapper_graph(pipe, dataG, color_data=dataG, plotly_params=plotly_params)\n",
    "    \n",
    "                    fig1 = go.Figure(fig)\n",
    "                    fig1.write_image(results_path+f\"Tracking/Mapper GT Graph 10 test set batches {epoch}.png\")\n",
    "                    wandb.log({\"Mapper GT Graph 10 test set batches\": wandb.Image(results_path+f\"Tracking/Mapper GT Graph 10 test set batches {epoch}.png\")})\n",
    "    \n",
    "    \n",
    "    \n",
    "                    fig = plot_static_mapper_graph(pipe, dataP, color_data=dataP, plotly_params=plotly_params)\n",
    "    \n",
    "                    fig1 = go.Figure(fig)\n",
    "                    fig1.write_image(results_path+f\"Tracking/Mapper Pred Graph 10 test set batches {epoch}.png\")\n",
    "                    wandb.log({\"Mapper Pred Graph 10 test set batches\": wandb.Image(results_path+f\"Tracking/Mapper Pred Graph 10 test set batches {epoch}.png\")})\n",
    "    \n",
    "    \n",
    "                    fig = plot_static_mapper_graph(pipe, dataF, color_data=dataF, plotly_params=plotly_params)\n",
    "    \n",
    "                    fig1 = go.Figure(fig)\n",
    "                    fig1.write_image(results_path+f\"Tracking/Mapper FN Graph 10 test set batches {epoch}.png\")\n",
    "                    wandb.log({\"Mapper FN Graph 10 test set batches\": wandb.Image(results_path+f\"Tracking/Mapper FN Graph 10 test set batches {epoch}.png\")})\n",
    "    \n",
    "                    wandb.log({\"Epoch 0 Test CNN accuracy on 10 batches \": DF['epoch 0'].mean()})\n",
    "                    wandb.log({\"Epoch 1 Test CNN accuracy on 10 batches \": DF['epoch 1'].mean()})\n",
    "                    wandb.log({\"Epoch 2 Test CNN accuracy on 10 batches \": DF['epoch 2'].mean()})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                if (epoch+1)%50==0:        \n",
    "                    torch.save({'epoch':epoch,'model_state_dict': mod.state_dict(),\n",
    "                                'optimizerENC1_state_dict':  optimizerEnc1.state_dict() ,\n",
    "                                'optimizerENC2_state_dict':optimizerEnc2.state_dict(),\n",
    "                                'optimizerDense_state_dict':optimizerDense.state_dict(),\n",
    "                                'optimizerDec_state_dict': optimizerDec.state_dict(),\n",
    "                                'Batch Loss':used_Loss.detach().cpu().item(),\n",
    "                                'validation entire testset MSE':loss_value.detach().cpu().item()},\n",
    "                                results_path+'AE epoch{} {} {}.pth'.format(combination,track,epoch))\n",
    "                    torch.cuda.empty_cache()\n",
    "                if (epoch+1)==num_epochs:\n",
    "                    del(mod)\n",
    "                    del(optimizerEnc1)\n",
    "                    del(optimizerEnc2) \n",
    "                    del(optimizerDense)\n",
    "                    del(optimizerDec)\n",
    "                \n",
    "            wandb.finish()\n",
    "            track=+1\n",
    "            #     return loss_tr\n",
    "            # study= optuna.create_study(direction=\"minimize\",storage=storage)\n",
    "            # study.optimize(objective,n_trials=2,callbacks=[lambda study, trial: gc.collect()]+[logging_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "367e8738-059b-482e-9460-a3d9d48ff3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd63514-0e51-4233-87a6-a3d322e9bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d9d4d-1fa7-4720-98fb-1552f197bbd7",
   "metadata": {},
   "source": [
    "Testing loop :\n",
    "```[tasklist]\n",
    "### track MSE \n",
    "- [ ] add Classification report for 1 CNN\n",
    "- [ ] per layer MSE\n",
    "- [ ] Deeplift\n",
    "- [ ] Grackel\n",
    "### Loss functions + apply per layer\n",
    "- [ ] KL divergence\n",
    "- [ ] Wassertsiein\n",
    "- [ ] dataset distance\n",
    "- [ ] Layerwise normalisations\n",
    "- [ ] Cook's distance \n",
    "### Track weights and their saturation\n",
    "- [ ] Contractive\n",
    "- [ ] L2\n",
    "- [ ] saturated percentage\n",
    "- [ ] Experiment with pruning the CNNs\n",
    "### Topology of learned weights\n",
    "- [ ] Reebs + Smole complex\n",
    "- [ ] Persistant homolgy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778b57a-7cdc-416c-8dc1-5a30fa8d9f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608b875-a0ba-499b-ab69-543c156ce475",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cols=[\"label task 1\",\"label task 2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(200)]+[\"Pred bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(208,1408)]+[\"Pred bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1414,1510)]+[\"Pred bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1514,2234)]+[\"Pred bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(2254,2454)]+[\"Pred bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(200)]+[\"FN bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(208,1408)]+[\"FN bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1414,1510)]+[\"FN bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1514,2234)]+[\"FN bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(2254,2454)]+[\"FN bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "[\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KLD\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "[\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "[\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "[\"WSD FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\\n",
    "[\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\",\"epoch 0\",\"epoch 1\",\"epoch 2\"]+\\\n",
    "[\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]\n",
    "#incoporate Norm                                                     \n",
    "#The Fisher information metric X norm of the target  \n",
    "#dataset distance , topological distance ,UMAP distance\n",
    "\n",
    "print(len(Cols))\n",
    "DF= pd.DataFrame(columns=Cols)\n",
    "\n",
    "# row=[\"\".format(task1),int(ind[0]),ACC[0],\"\".format(task2),ACC[1]]+vector_aux.to_list()+[\"train\",valid_epoch_acc0,ACC[2],valid_epoch_acc1,L_train[-1]]\n",
    "# predicted_Weights.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98643644-b4fc-4271-9aaf-c2eb44be2bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fb0cf-d13d-4729-ad50-e68214bf5846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e820345-4dfa-448c-bd4e-b528f52ea90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990acfd-7770-473a-a0fa-c492920eb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy==1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b5c5a-65b0-4dba-a538-cf0dbdc762d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c6c11-2cbc-4a49-a88e-d894ef052778",
   "metadata": {},
   "outputs": [],
   "source": [
    "int('AE epoch 0 1230.pth'.split(' ')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ae4db-f17c-479d-b626-b6faaf739cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66eb2d5-b52f-4c0a-848e-c46dc632a12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562651c-24f9-459f-928c-51fb37c675bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vec1.tolist()\n",
    "\n",
    "# L_files=os.listdir(results_path)\n",
    "# L_files=[ x for x in L_files if 'AE epoch' in x ]\n",
    "# def extract_epoch(filename):\n",
    "#     epoch_str = filename.split(' ')[-1].split('.')[0]\n",
    "#     return int(epoch_str) if epoch_str.isdigit() else float('inf')\n",
    "# # Sort the list using the epoch number as the key\n",
    "# L_files = sorted(L_files, key=extract_epoch,reverse=True)\n",
    "# L_files[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf35b4-115b-4b79-bc05-bb0855a393a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "from scipy.stats import wasserstein_distance\n",
    "#WSD=wasserstein_distance(vec1,vec2)\n",
    "\n",
    "track=0\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#from optuna.storages import JournalStorage, JournalFileStorage\n",
    "\n",
    "#storage = JournalStorage(JournalFileStorage(\"optuna-journal DDP 3 Losses.log\"))\n",
    "\n",
    "#def objective(trial):\n",
    "global track , output , model\n",
    "grads=None\n",
    "accelerator = Accelerator()\n",
    "Loss=\"LWLN\" #trial.suggest_categorical(\"Loss function\",[\"MSE\"])#,\"LWLN\",\"Contractive\",])\n",
    "Lambda=0\n",
    "if Loss==\"MSE\":\n",
    "    results_path=\"/media/crns/ADATA HD330/Experiments/model MSE/\"\n",
    "if Loss==\"Contractive\":\n",
    "    results_path=\"./Contractive model/\"\n",
    "    Lambda=0 #trial.suggest_float(\"Contractive_Lambda\",0.00001,0.0001)\n",
    "if Loss==\"LWLN\":\n",
    "    results_path=\"/media/crns/ADATA HD330/Experiments//mixed model/\"\n",
    "\n",
    "\n",
    "step_size=0\n",
    "factor=0\n",
    "threshhold=0\n",
    "threshold_mode = 0\n",
    "eps=0\n",
    "\n",
    "minimal_loss=10\n",
    "#alpha = trial.suggest_float(\"grokalpha\",0.0,1.0)\n",
    "#lamb = trial.suggest_int(\"groklamb\",1,15)\n",
    "mod= TransformerAE(max_seq_len=50,\n",
    "                        N=4,\n",
    "                        heads=4,\n",
    "                        d_model=940,\n",
    "                        d_ff=940,\n",
    "                        neck=256,\n",
    "                        dropout=0.12\n",
    "                       ) #define_model(trial)\n",
    "mod=mod.to(\"cpu\")\n",
    "\n",
    "print(summary(mod))\n",
    "\n",
    "\n",
    "L_files=os.listdir(results_path)\n",
    "L_files=[ x for x in L_files if 'AE epoch' in x ]\n",
    "def extract_epoch(filename):\n",
    "    epoch_str = filename.split(' ')[-1].split('.')[0]\n",
    "    return int(epoch_str) if epoch_str.isdigit() else float('inf')\n",
    "# Sort the list using the epoch number as the key\n",
    "L_files = sorted(L_files, key=extract_epoch,reverse=True)\n",
    "\n",
    "epoch=600\n",
    "for file in ['AE epoch 0 600.pth'] :\n",
    "    DF= pd.DataFrame(columns=Cols)\n",
    "    ch=torch.load(f\"{file}\", map_location='cpu')#/media/crns/ADATA HD330/Experiments/mixed model/\n",
    "    resume_epoch=ch[\"epoch\"]\n",
    "    l=ch[\"Batch Loss\"]\n",
    "    mod.load_state_dict(ch['model_state_dict'])\n",
    "    print(file ,\"Checkpoint Epoch \\t\" , resume_epoch,\"Loss \\t\",l)\n",
    "    cnn_acc_ID=[]\n",
    "    cnn_acc_OOD=[]\n",
    "    DF.at[track,\"Transformer train Loss\"]=l\n",
    "    del(ch)\n",
    "    mod=mod.to('cpu')\n",
    "\n",
    "    batch_size=80 #trial.suggest_int(\"batch_size\",150)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    LW=LWLN_loss()\n",
    "\n",
    "    cs_ts=CustomDataset(test_pair2,batch_size=batch_size)\n",
    "    nb_test_batches = len(cs_ts)//batch_size\n",
    "\n",
    "    mod,cs_ts= accelerator.prepare(mod, cs_ts)\n",
    "    mod.eval()\n",
    "\n",
    "    loss_values = []\n",
    "    for i in tqdm(range(43,nb_test_batches)):\n",
    "\n",
    "        Dataset,EXP,ACC,U = cs_ts[i]\n",
    "        x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]\n",
    "\n",
    "        x1=x1.cuda() #.to(torch.float32)\n",
    "        x2=x2.cuda() #.to(torch.float32)\n",
    "        tg=tg.cuda() #.to(torch.float32)\n",
    "        with torch.no_grad():\n",
    "            output = mod(x1,x2)\n",
    "            loss_values.append(criterion(output[0],tg))\n",
    "        for vec in range(len(x1)):\n",
    "            DF.at[track,\"label task 1\"]=f'{EXP[vec][0]}'\n",
    "            DF.at[track,\"label task 2\"]=f'{EXP[vec][1]}'\n",
    "            #print(Cols[2:2466][-2:],Cols[2466:4930][-2:],Cols[4930:7394][-2:])\n",
    "            \n",
    "            vec1=output[0][vec].cpu()\n",
    "            vec2=tg[vec].cpu()\n",
    "            DF.loc[track,Cols[2:2466]]=vec2.tolist()\n",
    "            DF.loc[track,Cols[2466:4930]]=vec1.tolist()#Cols[4930:7394][-2:])\n",
    "            \n",
    "            \n",
    "            \n",
    "            MSE=criterion(vec1,vec2).item()\n",
    "            MS1=criterion(vec1[:208],vec2[:208]).item()\n",
    "            MS2=criterion(vec1[208:1414],vec2[208:1414]).item()\n",
    "            MS3=criterion(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "            MS4=criterion(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "            MS5=criterion(vec1[2254:],vec2[2254:]).item()\n",
    "            #print(\"MSE :\",MS1,MS2,MS3,MS4,MS5,MSE)\n",
    "            DF.at[track,\"MSE\"]=MSE\n",
    "            DF.at[track,\"MSE 1\"]=MS1\n",
    "            DF.at[track,\"MSE 2\"]=MS2\n",
    "            DF.at[track,\"MSE 3\"]=MS3\n",
    "            DF.at[track,\"MSE 4\"]=MS4\n",
    "            DF.at[track,\"MSE 5\"]=MS5\n",
    "            \n",
    "            \n",
    "            \n",
    "            kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "            KLD=kl_loss(vec1,vec2).item() #pred, true\n",
    "            KL1=kl_loss(vec1[:208],vec2[:208]).item()\n",
    "            KL2=kl_loss(vec1[208:1414],vec2[208:1414]).item()\n",
    "            KL3=kl_loss(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "            KL4=kl_loss(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "            KL5=kl_loss(vec1[2254:],vec2[2254:]).item()\n",
    "            #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "            DF.at[track,\"KLD\"]=KLD\n",
    "            DF.at[track,\"KL 1\"]=KL1\n",
    "            DF.at[track,\"KL 2\"]=KL2\n",
    "            DF.at[track,\"KL 3\"]=KL3\n",
    "            DF.at[track,\"KL 4\"]=KL4\n",
    "            DF.at[track,\"KL 5\"]=KL5\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            WSD=wasserstein_distance(vec1,vec2)\n",
    "            WS1=wasserstein_distance(vec1[:208],vec2[:208])\n",
    "            WS2=wasserstein_distance(vec1[208:1414],vec2[208:1414])\n",
    "            WS3=wasserstein_distance(vec1[1414:1514],vec2[1414:1514])\n",
    "            WS4=wasserstein_distance(vec1[1514:2254],vec2[1514:2254])\n",
    "            WS5=wasserstein_distance(vec1[2254:],vec2[2254:])\n",
    "            #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "            DF.at[track,\"Wasserstein Loss\"]=WSD\n",
    "            DF.at[track,\"WS 1\"]=WS1\n",
    "            DF.at[track,\"WS 2\"]=WS2\n",
    "            DF.at[track,\"WS 3\"]=WS3\n",
    "            DF.at[track,\"WS 4\"]=WS4\n",
    "            DF.at[track,\"WS 5\"]=WS5\n",
    "            \n",
    "            for name, param in mod.named_parameters():\n",
    "                if name == 'vec2neck.weight':\n",
    "                    W = param\n",
    "                    break\n",
    "                    \n",
    "            CL=loss_Contractive(W,vec1,vec2, output[1], 0.00001)\n",
    "            DF.at[track,\"contractive distance\"]=CL.cpu().item()\n",
    "            \n",
    "            #print(\"Contractive :\",CL.cpu().item())\n",
    "            Norm1=np.sum(np.abs(vec1.numpy()))\n",
    "            N11=np.sum(np.abs(vec1[:208].numpy()))\n",
    "            N12=np.sum(np.abs(vec1[208:1414].numpy()))\n",
    "            N13=np.sum(np.abs(vec1[1414:1514].numpy()))\n",
    "            N14=np.sum(np.abs(vec1[1514:2254].numpy()))\n",
    "            N15=np.sum(np.abs(vec1[2254:].numpy()))\n",
    "            #print(\"Weight pred L1: \",Norm1, N11,N12,N13,N14,N15)\n",
    "            DF.at[track,\"N1\"]=Norm1\n",
    "            DF.at[track,\"N11\"]=N11\n",
    "            DF.at[track,\"N12\"]=N12\n",
    "            DF.at[track,\"N13\"]=N13\n",
    "            DF.at[track,\"N14\"]=N14\n",
    "            DF.at[track,\"N15\"]=N15\n",
    "            \n",
    "            Norm2=np.sum(np.abs(vec2.numpy()))\n",
    "            N21=np.sum(np.abs(vec2[:208].numpy()))\n",
    "            N22=np.sum(np.abs(vec2[208:1414].numpy()))\n",
    "            N23=np.sum(np.abs(vec2[1414:1514].numpy()))\n",
    "            N24=np.sum(np.abs(vec2[1514:2254].numpy()))\n",
    "            N25=np.sum(np.abs(vec2[2254:].numpy()))\n",
    "            #print(\"Weight GT L1: \",Norm2, N21,N22,N23,N24,N25)\n",
    "            DF.at[track,\"N2\"]=Norm2\n",
    "            DF.at[track,\"N21\"]=N21\n",
    "            DF.at[track,\"N22\"]=N22\n",
    "            DF.at[track,\"N23\"]=N23\n",
    "            DF.at[track,\"N24\"]=N24\n",
    "            DF.at[track,\"N25\"]=N25\n",
    "            \n",
    "            DF.at[track,\"saturated in pred(%)\"]=100*sum(1 for x in vec1 if x > 0.95 or x<0.05)/len(vec1)\n",
    "            DF.at[track,\"saturated in GT(%)\"]=100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2)\n",
    "            DF.at[track,\"LWLN\"]=LWs(vec1,vec2).cpu().item()\n",
    "            if vec<5:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(vec1.cpu().numpy().flatten(), label='Predicted', alpha=0.7)\n",
    "                plt.plot(tg[0].cpu().numpy().flatten(), label='target', alpha=0.7)\n",
    "    \n",
    "                # Add vertical lines\n",
    "                # for x in range(len(vec1_np)):\n",
    "                #     plt.axvline(x=x, color='gray', linestyle='--', alpha=0.2)\n",
    "    \n",
    "                # Add legend and labels\n",
    "                plt.legend()\n",
    "                plt.xlabel('Neurone Index')\n",
    "                plt.ylabel('Neurone Value')\n",
    "                plt.title('Plot of pred vs target')\n",
    "\n",
    "\n",
    "            #import umap\n",
    "            # reducer = umap.UMAP()\n",
    "            # Batch_UMAP=torch.rand(80,2464)\n",
    "            # fitted_reduced = reducer.fit(Batch_UMAP)\n",
    "            # fitted_reduced.transform(vec1)\n",
    "\n",
    "            ###########RECONSTRUCTING##############\n",
    "            y_pred=torch.unsqueeze(output[0][vec], 0) \n",
    "            y =torch.unsqueeze(tg[vec], 0) \n",
    "\n",
    "            selected_row = cs_ts.df.iloc[int(U[vec][0]), 11:17]  \n",
    "            columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "            activ=columns_with_one\n",
    "            epochCNN=cs_ts.df.loc[int(U[vec][0])]['epoch']\n",
    "\n",
    "\n",
    "            checkpoint=OrderedDict()\n",
    "            vector_aux= output[0][vec].detach()\n",
    "            y_pred=vector_aux.cpu()\n",
    "\n",
    "            task1=[int(x) for x in EXP[vec][0]]\n",
    "            task2=[int(x) for x in EXP[vec][1]]\n",
    "            task3=sorted(task1+task2)\n",
    "\n",
    "\n",
    "            All=list(range(10))\n",
    "            L2=[k for k in All if k not in task3] # Out of distribution classes\n",
    "            L_others=[k for k in All if k not in task3] #Classes to test on (In distribution)\n",
    "\n",
    "            checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(y_pred[0:200]).reshape([8, 1, 5, 5]))\n",
    "            checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(y_pred[200:208]).reshape([8]))\n",
    "\n",
    "            checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(y_pred[208:1408]).reshape([6, 8, 5, 5]))\n",
    "            checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(y_pred[1408:1414]).reshape([6]))\n",
    "\n",
    "            checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(y_pred[1414:1510]).reshape([4, 6, 2, 2]))\n",
    "            checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(y_pred[1510:1514]).reshape([4]))\n",
    "\n",
    "            checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(y_pred[1514:2234]).reshape([20,36]))\n",
    "            checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(y_pred[2234:2254]).reshape([20]))\n",
    "\n",
    "            checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(y_pred[2254:2454]).reshape([10,20]))\n",
    "            checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(y_pred[2454:2464]).reshape([10]))\n",
    "\n",
    "            Brain = CNN(1,activ[0],0,\"kaiming_uniform\")\n",
    "\n",
    "            model=copy.deepcopy(Brain)\n",
    "            model.load_state_dict(checkpoint)\n",
    "\n",
    "            criterion_CNN0=CrossEntropyLoss()\n",
    "\n",
    "            test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "            Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "            _, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "            if len(task3)==10:\n",
    "                valid_epoch_acc1=valid_epoch_acc0\n",
    "                continue\n",
    "            else:\n",
    "                criterion_CNN1=CrossEntropyLoss()\n",
    "                test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "            #valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n",
    "            #print(\"Reconstructed cnn acc ID\",valid_epoch_acc0)\n",
    "            #print(\"Reconstructed cnn acc OOD\",valid_epoch_acc1)\n",
    "            DF.at[track,\"Reconstructed Accuracy ID\"]=valid_epoch_acc0\n",
    "            \n",
    "            optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "            schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "            criterion_CNN=CrossEntropyLoss()\n",
    "\n",
    "\n",
    "            train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "            Tr_DLr = DataLoader(dataset=train_IF0, batch_size=150, num_workers=0, shuffle=True)\n",
    "\n",
    "\n",
    "            fine_tune_needed=0\n",
    "            #FINETUNING\n",
    "            for epoch_cnn in range(3):\n",
    "                if epoch_cnn==0:\n",
    "                    train_epoch_loss, train_epoch_acc,_ = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF,First=True)\n",
    "                    valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "                    DF.at[track,\"epoch 0\"]=valid_epoch_acc0FN\n",
    "                else:\n",
    "                    train_epoch_loss, train_epoch_acc,_ = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF)\n",
    "                    valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "                    DF.at[track,f\"epoch {epoch_cnn}\"]=valid_epoch_acc0FN\n",
    "                schedulerCNN.step()\n",
    "                fine_tune_needed+=1\n",
    "            L_param=[]\n",
    "            for param in model.parameters():\n",
    "                m = nn.Flatten(0,-1)\n",
    "                L_param.append(m(param))\n",
    "            vec1FN = torch.Tensor()\n",
    "            for idx in L_param:\n",
    "                vec1FN = torch.cat((vec1FN, idx.view(-1)))\n",
    "            vec1FN=vec1FN.detach().cpu()\n",
    "            vec2=tg[vec].cpu()\n",
    "            DF.loc[track,Cols[4930:7394]]=vec1FN.tolist()\n",
    "            MSE=criterion(vec1FN,vec2).item()\n",
    "            MS1=criterion(vec1FN[:208],vec2[:208]).item()\n",
    "            MS2=criterion(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "            MS3=criterion(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "            MS4=criterion(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "            MS5=criterion(vec1FN[2254:],vec2[2254:]).item()\n",
    "            \n",
    "            \n",
    "            #print(\"MSE :\",MS1,MS2,MS3,MS4,MS5,MSE)\n",
    "            DF.at[track,\"MSE FN\"]=MSE\n",
    "            DF.at[track,\"MSE 1 FN\"]=MS1\n",
    "            DF.at[track,\"MSE 2 FN\"]=MS2\n",
    "            DF.at[track,\"MSE 3 FN\"]=MS3\n",
    "            DF.at[track,\"MSE 4 FN\"]=MS4\n",
    "            DF.at[track,\"MSE 5 FN\"]=MS5\n",
    "            kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "            KLD=kl_loss(vec1FN,vec2).item() #pred, true\n",
    "            KL1=kl_loss(vec1FN[:208],vec2[:208]).item()\n",
    "            KL2=kl_loss(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "            KL3=kl_loss(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "            KL4=kl_loss(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "            KL5=kl_loss(vec1FN[2254:],vec2[2254:]).item()\n",
    "            #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "            DF.at[track,\"KL divergence FN\"]=KLD\n",
    "            DF.at[track,\"KL 1 FN\"]=KL1\n",
    "            DF.at[track,\"KL 2 FN\"]=KL2\n",
    "            DF.at[track,\"KL 3 FN\"]=KL3\n",
    "            DF.at[track,\"KL 4 FN\"]=KL4\n",
    "            DF.at[track,\"KL 5 FN\"]=KL5\n",
    "\n",
    "            WSD=wasserstein_distance(vec1FN,vec2)\n",
    "            WS1=wasserstein_distance(vec1FN[:208],vec2[:208])\n",
    "            WS2=wasserstein_distance(vec1FN[208:1414],vec2[208:1414])\n",
    "            WS3=wasserstein_distance(vec1FN[1414:1514],vec2[1414:1514])\n",
    "            WS4=wasserstein_distance(vec1FN[1514:2254],vec2[1514:2254])\n",
    "            WS5=wasserstein_distance(vec1FN[2254:],vec2[2254:])\n",
    "            #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "            \n",
    "            DF.at[track,\"WSD FN\"]=WSD\n",
    "            DF.at[track,\"WS 1 FN\"]=WS1\n",
    "            DF.at[track,\"WS 2 FN\"]=WS2\n",
    "            DF.at[track,\"WS 3 FN\"]=WS3\n",
    "            DF.at[track,\"WS 4 FN\"]=WS4\n",
    "            DF.at[track,\"WS 5 FN\"]=WS5\n",
    "            for name, param in mod.named_parameters():\n",
    "                if name == 'vec2neck.weight':\n",
    "                    W = param\n",
    "                    break\n",
    "            CL=loss_Contractive(W,vec1FN,vec2, output[1], 0.00001)        \n",
    "            #print(\"Contractive :\",CL.cpu().item())\n",
    "            DF.at[track,\"contractive distance FN\"]=CL.cpu().item()\n",
    "            Norm1=np.sum(np.abs(vec1FN.numpy()))\n",
    "            N11=np.sum(np.abs(vec1FN[:208].numpy()))\n",
    "            N12=np.sum(np.abs(vec1FN[208:1414].numpy()))\n",
    "            N13=np.sum(np.abs(vec1FN[1414:1514].numpy()))\n",
    "            N14=np.sum(np.abs(vec1FN[1514:2254].numpy()))\n",
    "            N15=np.sum(np.abs(vec1FN[2254:].numpy()))\n",
    "            #print(\"Weight pred L1: \",Norm1, N11,N12,N13,N14,N15)\n",
    "            DF.at[track,\"N1 FN\"]=Norm1\n",
    "            DF.at[track,\"N11 FN\"]=N11\n",
    "            DF.at[track,\"N12 FN\"]=N12\n",
    "            DF.at[track,\"N13 FN\"]=N13\n",
    "            DF.at[track,\"N14 FN\"]=N14\n",
    "            DF.at[track,\"N15 FN\"]=N15\n",
    "            Norm2=np.sum(np.abs(vec2.numpy()))\n",
    "            N21=np.sum(np.abs(vec2[:208].numpy()))\n",
    "            N22=np.sum(np.abs(vec2[208:1414].numpy()))\n",
    "            N23=np.sum(np.abs(vec2[1414:1514].numpy()))\n",
    "            N24=np.sum(np.abs(vec2[1514:2254].numpy()))\n",
    "            N25=np.sum(np.abs(vec2[2254:].numpy()))\n",
    "            #print(\"Weight GT L1: \",Norm2, N21,N22,N23,N24,N25)\n",
    "            DF.at[track,\"N2 FN\"]=Norm2\n",
    "            DF.at[track,\"N21 FN\"]=N21\n",
    "            DF.at[track,\"N22 FN\"]=N22\n",
    "            DF.at[track,\"N23 FN\"]=N23\n",
    "            DF.at[track,\"N24 FN\"]=N24\n",
    "            DF.at[track,\"N25 FN\"]=N25\n",
    "            #print(\"Saturation in pred \",100*sum(1 for x in vec1FN if x > 0.95 or x<0.05)/len(vec1FN),\"% \\t saturaion in target\",100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2),\"%\")\n",
    "            #print(\"LWLN \",LW(vec1FN,vec2).item())\n",
    "            DF.at[track,\"saturated in pred FN(%)\"]=100*sum(1 for x in vec1FN if x > 0.95 or x<0.05)/len(vec1FN)\n",
    "            DF.at[track,\"saturated in GT FN(%)\"]=100*sum(1 for x in vec2 if x > 0.95 or x<0.05)/len(vec2)\n",
    "            DF.at[track,\"LWLN FN\"]=LW(vec1FN,vec2).cpu().item()\n",
    "            \n",
    "            #print(\"----------------------\")\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(vec1FN.cpu().numpy().flatten(), label='Predicted', alpha=0.7)\n",
    "            plt.plot(tg[vec].cpu().numpy().flatten(), label='target', alpha=0.7)\n",
    "\n",
    "            # Add vertical lines\n",
    "            # for x in range(len(vec1_np)):\n",
    "            #     plt.axvline(x=x, color='gray', linestyle='--', alpha=0.2)\n",
    "\n",
    "            # Add legend and labels\n",
    "            plt.legend()\n",
    "            plt.xlabel('Neurone Index')\n",
    "            plt.ylabel('Neurone Value')\n",
    "            plt.title(f'{file} {EXP[vec][0]} {EXP[vec][1]} {valid_epoch_acc0FN} %')\n",
    "            DF.to_csv(results_path+\"Tracking/\"+file[:-3]+\"csv\")\n",
    "            track=track+1\n",
    "\n",
    "            \n",
    "\n",
    "        if i%100==0 and i>0 :\n",
    "            print(loss_values[-1])\n",
    "            for block in [2,3,4]:\n",
    "                    for head in range(4):\n",
    "                        plt.figure(figsize=(20, 20))\n",
    "                        hm=sns.heatmap(torch.mean( torch.mean(output[block][head], dim=1), dim=0).detach().cpu(), annot=False, cmap='cubehelix')\n",
    "                        plt.title('Attention Heatmap')\n",
    "                        heatmap_path = f'heatmap {file} {block-2}_{head}_step_{i}.png'\n",
    "                        plt.savefig(results_path+\"Attention/\"+heatmap_path)#,format='svg', dpi=800)\n",
    "                        #wandb.log({f\"attention_heatmap {block-2}_{head}\":  wandb.Image(hm,caption=f\"attention_heatmap attention_heatmap {block-2}_{head}\")})\n",
    "                        plt.close()\n",
    "                        \n",
    "            GT=DF[Cols[2:2466]]\n",
    "            dataG=GT.to_numpy()\n",
    "            print(Cols[2],Cols[2466],Cols[4930])\n",
    "            PD=DF[Cols[2466:4930]]\n",
    "            dataP=PD.to_numpy()\n",
    "\n",
    "            FN=DF[Cols[4930:7394]]\n",
    "            dataF=FN.to_numpy()\n",
    "            # Initialise pipeline\n",
    "            pipe = make_mapper_pipeline(\n",
    "                filter_func=filter_func,\n",
    "                cover=cover,\n",
    "                clusterer=clusterer,\n",
    "                verbose=True,\n",
    "               n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "            plotly_params = {\"node_trace\": {\"marker_colorscale\": \"Blues\"}}\n",
    "            fig = plot_static_mapper_graph(\n",
    "                pipe, dataG, color_data=dataG, plotly_params=plotly_params\n",
    "            )\n",
    "\n",
    "            fig1 = go.Figure(fig)\n",
    "            fig1.write_image(results_path+f\"Tracking/Mapper GT Graph 10 test set batches {epoch}.png\")\n",
    "            #wandb.log({\"Mapper GT Graph 10 test set batches\": wandb.Image(results_path+f\"Tracking/Mapper GT Graph 10 test set batches {epoch}.png\")})\n",
    "\n",
    "\n",
    "\n",
    "            fig = plot_static_mapper_graph(\n",
    "                pipe, dataP, color_data=dataP, plotly_params=plotly_params\n",
    "            )\n",
    "\n",
    "            fig1 = go.Figure(fig)\n",
    "            fig1.write_image(results_path+f\"Tracking/Mapper Pred Graph 10 test set batches {epoch}.png\")\n",
    "            #wandb.log({\"Mapper Pred Graph 10 test set batches\": wandb.Image(results_path+f\"Tracking/Mapper Pred Graph 10 test set batches {epoch}.png\")})\n",
    "\n",
    "\n",
    "            fig = plot_static_mapper_graph(\n",
    "                pipe, dataF, color_data=dataF, plotly_params=plotly_params\n",
    "            )\n",
    "\n",
    "            fig1 = go.Figure(fig)\n",
    "            fig1.write_image(results_path+f\"Tracking/Mapper FN Graph 10 test set batches {epoch}.png\")\n",
    "            #\"wandb.log({\"Mapper FN Graph 10 test set batches\": wandb.Image(results_path+f\"Tracking/Mapper FN Graph 10 test set batches {epoch}.png\")})\n",
    "    print(\"Transformer Test set: \\t\",file, (sum(loss_values)/len(loss_values)).cpu().item())\n",
    "    # Create the plot\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7183863-2349-479e-b839-a6b0c044007e",
   "metadata": {},
   "source": [
    "# Noticed some Columns were not Logged properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829b984-3ac9-47a4-bced-7f704f162e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cols=[\"label task 1\",\"label task 2\"]+ \\\n",
    "[\"weight {}\".format(x) for x in range(200)]+[\"bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(208,1408)]+[\"bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1414,1510)]+[\"bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(1514,2234)]+[\"bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"weight {}\".format(x) for x in range(2254,2454)]+[\"bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(200)]+[\"Pred bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(208,1408)]+[\"Pred bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1414,1510)]+[\"Pred bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(1514,2234)]+[\"Pred bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"Pred weight {}\".format(x) for x in range(2254,2454)]+[\"Pred bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(200)]+[\"FN bias {}\".format(x) for x in range(200,208)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(208,1408)]+[\"FN bias {}\".format(x) for x in range(1408,1414)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1414,1510)]+[\"FN bias {}\".format(x) for x in range(1510,1514)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(1514,2234)]+[\"FN bias {}\".format(x) for x in range(2234,2254)]+ \\\n",
    "[\"FN weight {}\".format(x) for x in range(2254,2454)]+[\"FN bias {}\".format(x) for x in range(2454,2464)]+ \\\n",
    "[\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "[\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KLD\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "[\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "[\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "[\"WSD FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\\n",
    "[\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\",\"epoch 0\",\"epoch 1\",\"epoch 2\"]+\\\n",
    "[\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]\n",
    "#GT=DF[Cols[2:2466]]PD=DF[Cols[2466:4930]]\n",
    "#incoporate Norm                                                     \n",
    "#The Fisher information metric X norm of the target  \n",
    "#dataset distance , topological distance ,UMAP distance\n",
    "\n",
    "print(len(Cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653ca4d-06cc-41a5-ae3f-4670bd5fbc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/media/crns/ADATA HD330/Experiments/mixed model/Tracking/_AE epoch 0 600.csv\")\n",
    "df=df[df.columns[1:]]\n",
    "print(df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf4a3d-98eb-4ee3-9895-d25482546dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"/media/crns/ADATA HD330/Experiments/mixed model/Tracking/__AE epoch 0 600.csv\")\n",
    "df1=df1[df1.columns[1:]]\n",
    "print(df1.shape)\n",
    "df1.head(2)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2218075-17ae-4064-bc39-bf75445fa6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"/media/crns/ADATA HD330/Experiments/mixed model/Tracking/AE epoch 0 600.csv\")\n",
    "df1=df1[df1.columns[1:]]\n",
    "#df=df1\n",
    "print(df1.shape)\n",
    "df1.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd283b1-7ca3-4f62-959b-22208ffcb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "merged_df = pd.concat([df, df1])\n",
    "\n",
    "# Drop duplicates (keeping the first occurrence)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Optional: Reset index if you want a clean index\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "print(merged_df.shape)\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730684d-3a54-4071-9f77-6bb2d1202c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=merged_df\n",
    "columns_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88b455-0b95-4e89-afd4-96cf6bd4fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08104f22-5d20-4840-87fc-992de44648dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "#result_cleaned = result.dropna(axis=1, how='any')\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f400711-d71f-44fd-b716-f348354a876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PD=df[Cols[2466:4930]]\n",
    "PD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7e0c2-7dbc-40ba-9cfb-52f2202de4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT=df[Cols[2:2466]]\n",
    "GT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38974312-4570-4d17-a937-d11c9f9a3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN=df[Cols[4930:7394]]\n",
    "FN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259a398-8a99-40a7-b9f4-d6fecfa5d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8d47b-732e-43fd-b85a-0564dc8bfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "DF=df\n",
    "track=0\n",
    "for i in range(len(GT)):\n",
    "    row=PD.iloc[i]\n",
    "    vec1=row.to_numpy()\n",
    "    vec1t = torch.from_numpy(vec1)\n",
    "    \n",
    "    row=GT.iloc[i]\n",
    "    vec2=row.to_numpy()\n",
    "    vec2t = torch.from_numpy(vec2)\n",
    "    \n",
    "    row=FN.iloc[i]\n",
    "    vec1FN=row.to_numpy()\n",
    "    vec1FNt = torch.from_numpy(vec1FN)\n",
    "    \n",
    "    kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "    KLD=kl_loss(vec1t,vec2t).item() #pred, true\n",
    "    KL1=kl_loss(vec1t[:208],vec2t[:208]).item()\n",
    "    KL2=kl_loss(vec1t[208:1414],vec2t[208:1414]).item()\n",
    "    KL3=kl_loss(vec1t[1414:1514],vec2t[1414:1514]).item()\n",
    "    KL4=kl_loss(vec1t[1514:2254],vec2t[1514:2254]).item()\n",
    "    KL5=kl_loss(vec1t[2254:],vec2t[2254:]).item()\n",
    "    #print(\"KLD :\",i,KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "    DF.at[track,\"KLD\"]=KLD\n",
    "    DF.at[track,\"KL 1\"]=KL1\n",
    "    DF.at[track,\"KL 2\"]=KL2\n",
    "    DF.at[track,\"KL 3\"]=KL3\n",
    "    DF.at[track,\"KL 4\"]=KL4\n",
    "    DF.at[track,\"KL 5\"]=KL5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    WSD=wasserstein_distance(vec1,vec2)\n",
    "    WS1=wasserstein_distance(vec1[:208],vec2[:208])\n",
    "    WS2=wasserstein_distance(vec1[208:1414],vec2[208:1414])\n",
    "    WS3=wasserstein_distance(vec1[1414:1514],vec2[1414:1514])\n",
    "    WS4=wasserstein_distance(vec1[1514:2254],vec2[1514:2254])\n",
    "    WS5=wasserstein_distance(vec1[2254:],vec2[2254:])\n",
    "    #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "    DF.at[track,\"Wasserstein Loss\"]=WSD\n",
    "    DF.at[track,\"WS 1\"]=WS1\n",
    "    DF.at[track,\"WS 2\"]=WS2\n",
    "    DF.at[track,\"WS 3\"]=WS3\n",
    "    DF.at[track,\"WS 4\"]=WS4\n",
    "    DF.at[track,\"WS 5\"]=WS5\n",
    "\n",
    "\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "    KLD=kl_loss(vec1FNt,vec2t).item() #pred, true\n",
    "    KL1=kl_loss(vec1FNt[:208],vec2t[:208]).item()\n",
    "    KL2=kl_loss(vec1FNt[208:1414],vec2t[208:1414]).item()\n",
    "    KL3=kl_loss(vec1FNt[1414:1514],vec2t[1414:1514]).item()\n",
    "    KL4=kl_loss(vec1FNt[1514:2254],vec2t[1514:2254]).item()\n",
    "    KL5=kl_loss(vec1FNt[2254:],vec2t[2254:]).item()\n",
    "    #print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "    DF.at[track,\"KL divergence FN\"]=KLD\n",
    "    DF.at[track,\"KL 1 FN\"]=KL1\n",
    "    DF.at[track,\"KL 2 FN\"]=KL2\n",
    "    DF.at[track,\"KL 3 FN\"]=KL3\n",
    "    DF.at[track,\"KL 4 FN\"]=KL4\n",
    "    DF.at[track,\"KL 5 FN\"]=KL5\n",
    "\n",
    "    WSD=wasserstein_distance(vec1FN,vec2)\n",
    "    WS1=wasserstein_distance(vec1FN[:208],vec2[:208])\n",
    "    WS2=wasserstein_distance(vec1FN[208:1414],vec2[208:1414])\n",
    "    WS3=wasserstein_distance(vec1FN[1414:1514],vec2[1414:1514])\n",
    "    WS4=wasserstein_distance(vec1FN[1514:2254],vec2[1514:2254])\n",
    "    WS5=wasserstein_distance(vec1FN[2254:],vec2[2254:])\n",
    "    #print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "\n",
    "    DF.at[track,\"WSD FN\"]=WSD\n",
    "    DF.at[track,\"WS 1 FN\"]=WS1\n",
    "    DF.at[track,\"WS 2 FN\"]=WS2\n",
    "    DF.at[track,\"WS 3 FN\"]=WS3\n",
    "    DF.at[track,\"WS 4 FN\"]=WS4\n",
    "    DF.at[track,\"WS 5 FN\"]=WS5\n",
    "    track=track+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a578025-d603-4f7b-ab98-d9988b36ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=DF\n",
    "DF[['Actual Accuracy','KLD','KL 1','KL 2','KL 3','KL 4','KL 5','KL divergence FN','KL 1 FN','KL 2 FN','KL 3 FN','KL 4 FN','KL 5 FN','WSD FN']].tail(50)\n",
    "\n",
    "# nan_indices = DF[['Actual Accuracy','KL divergence','KL 1','KL 2','KL 3','KL 4','KL 5','KL divergence FN','KL 1 FN','KL 2 FN','KL 3 FN','KL 4 FN','KL 5 FN','Wasserstein Loss FN']].index[DF[['Actual Accuracy','KL divergence','KL 1','KL 2','KL 3','KL 4','KL 5','KL divergence FN','KL 1 FN','KL 2 FN','KL 3 FN','KL 4 FN','KL 5 FN','Wasserstein Loss FN']].isna().any(axis=1)].tolist()\n",
    "\n",
    "# print(min(nan_indices),max(nan_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f8f44-d089-40ea-bacd-12bb99296b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "KLD=kl_loss(vec1,vec2).item() #pred, true\n",
    "KL1=kl_loss(vec1[:208],vec2[:208]).item()\n",
    "KL2=kl_loss(vec1[208:1414],vec2[208:1414]).item()\n",
    "KL3=kl_loss(vec1[1414:1514],vec2[1414:1514]).item()\n",
    "KL4=kl_loss(vec1[1514:2254],vec2[1514:2254]).item()\n",
    "KL5=kl_loss(vec1[2254:],vec2[2254:]).item()\n",
    "#print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "DF.at[track,\"KLD\"]=KLD\n",
    "DF.at[track,\"KL 1\"]=KL1\n",
    "DF.at[track,\"KL 2\"]=KL2\n",
    "DF.at[track,\"KL 3\"]=KL3\n",
    "DF.at[track,\"KL 4\"]=KL4\n",
    "DF.at[track,\"KL 5\"]=KL5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WSD=wasserstein_distance(vec1,vec2)\n",
    "WS1=wasserstein_distance(vec1[:208],vec2[:208])\n",
    "WS2=wasserstein_distance(vec1[208:1414],vec2[208:1414])\n",
    "WS3=wasserstein_distance(vec1[1414:1514],vec2[1414:1514])\n",
    "WS4=wasserstein_distance(vec1[1514:2254],vec2[1514:2254])\n",
    "WS5=wasserstein_distance(vec1[2254:],vec2[2254:])\n",
    "#print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "DF.at[track,\"Wasserstein Loss\"]=WSD\n",
    "DF.at[track,\"WS 1\"]=WS1\n",
    "DF.at[track,\"WS 2\"]=WS2\n",
    "DF.at[track,\"WS 3\"]=WS3\n",
    "DF.at[track,\"WS 4\"]=WS4\n",
    "DF.at[track,\"WS 5\"]=WS5\n",
    "\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction=\"mean\")\n",
    "KLD=kl_loss(vec1FN,vec2).item() #pred, true\n",
    "KL1=kl_loss(vec1FN[:208],vec2[:208]).item()\n",
    "KL2=kl_loss(vec1FN[208:1414],vec2[208:1414]).item()\n",
    "KL3=kl_loss(vec1FN[1414:1514],vec2[1414:1514]).item()\n",
    "KL4=kl_loss(vec1FN[1514:2254],vec2[1514:2254]).item()\n",
    "KL5=kl_loss(vec1FN[2254:],vec2[2254:]).item()\n",
    "#print(\"KLD :\",KLD,KL1,KL2,KL3,KL4,KL5)\n",
    "DF.at[track,\"KL divergence FN\"]=KLD\n",
    "DF.at[track,\"KL 1 FN\"]=KL1\n",
    "DF.at[track,\"KL 2 FN\"]=KL2\n",
    "DF.at[track,\"KL 3 FN\"]=KL3\n",
    "DF.at[track,\"KL 4 FN\"]=KL4\n",
    "DF.at[track,\"KL 5 FN\"]=KL5\n",
    "\n",
    "WSD=wasserstein_distance(vec1FN,vec2)\n",
    "WS1=wasserstein_distance(vec1FN[:208],vec2[:208])\n",
    "WS2=wasserstein_distance(vec1FN[208:1414],vec2[208:1414])\n",
    "WS3=wasserstein_distance(vec1FN[1414:1514],vec2[1414:1514])\n",
    "WS4=wasserstein_distance(vec1FN[1514:2254],vec2[1514:2254])\n",
    "WS5=wasserstein_distance(vec1FN[2254:],vec2[2254:])\n",
    "#print(\"WSD :\",WSD,WS1,WS2,WS3,WS4,WS5)\n",
    "\n",
    "DF.at[track,\"WSD FN\"]=WSD\n",
    "DF.at[track,\"WS 1 FN\"]=WS1\n",
    "DF.at[track,\"WS 2 FN\"]=WS2\n",
    "DF.at[track,\"WS 3 FN\"]=WS3\n",
    "DF.at[track,\"WS 4 FN\"]=WS4\n",
    "DF.at[track,\"WS 5 FN\"]=WS5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2cb10-6a65-4c91-afaf-dc6bc02e97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 80\n",
    "cs_ts=CustomDataset(test_pair2,batch_size=batch_size)\n",
    "nb_test_batches = len(cs_ts)//batch_size\n",
    "track=0\n",
    "L_acc=[]\n",
    "for i in tqdm(range(nb_test_batches)):\n",
    "    Dataset,EXP,ACC,U = cs_ts[i]\n",
    "    x1,x2,tg = Dataset[:,0,:], Dataset[:,1,:],Dataset[:,2,:]\n",
    "    for vec in range(len(x1)):\n",
    "        L_acc.append(ACC[vec][2])\n",
    "        #df.at[track,'Actual Accuracy']=ACC[vec][2]\n",
    "        #track=track+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef1204-1be1-4f7e-baf3-4bb4db23d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Actual Accuracy\"]=L_acc[:3472]\n",
    "columns_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "#result_cleaned = result.dropna(axis=1, how='any')\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cec5f-ed9e-4b28-97f0-cbafbb120b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"AE epoch 600 MSE clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792d10b-7edb-489d-970c-5f456a3d1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = df.columns[df.isnull().any()].tolist()\n",
    "#result_cleaned = result.dropna(axis=1, how='any')\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e777a72-3b56-4d52-8c59-91f97548b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd809bb-befa-4ad9-aba1-ff7fc6649845",
   "metadata": {},
   "source": [
    "# Quantify data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50b87d-aed6-4f67-a0c8-159f64352c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global track \n",
    "track=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9af98-0661-41b4-a23b-6c1cf6b05ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PD=df[Cols[2466:4930]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6f4fd-ff83-451c-bc71-cc0242cf8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2= pd.DataFrame(columns=[\"task1\",\"task2\",'epoch', 'step', 'images_needed', 'target test acc', 'reached test acc'] + [\"Label index\"+str(i) for i in range(10)])\n",
    "DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbdd7ad-f79d-4ca0-ad4a-8e4f871ce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_elements(elements):\n",
    "    return dict(Counter([int(x) for x in elements]))\n",
    "def add_dicts(dict1, dict2):\n",
    "    return dict(Counter(dict1) + Counter(dict2))\n",
    "def train(model, trainloader, optimizer, criterion,nb_classes,epochs=8,testloader=None,targetacc=None,n_splits=20,First=False,df=None,verbose=False,log_freq_steps=25):\n",
    "    List_mx=[]\n",
    "    model.train()\n",
    "    #print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    \n",
    "    #skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=42)\n",
    "    nb_images_needed=0\n",
    "    reached_reconstruction=False\n",
    "    nb_images=0\n",
    "    d=dict()\n",
    "    \n",
    "    if verbose==True:\n",
    "        print(f\"loaded {100/n_splits} % of data for training\")\n",
    "    for epoch in range(epochs):\n",
    "        for k,(image,label) in enumerate(Tr_DLr):\n",
    "            d_aux=count_elements([int(x) for x in label])\n",
    "            if k ==(len(Tr_DLr)-1):\n",
    "                continue\n",
    "            elif k==int(len(Tr_DLr)/n_splits):\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "                nb_images=nb_images+len(label)\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass\n",
    "                outputs = model(image)\n",
    "                # calculate the loss\n",
    "                loss = criterion(outputs, label)\n",
    "                train_running_loss += loss.item()\n",
    "                # calculate the accuracy\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                train_running_correct += (preds == label).sum().item()\n",
    "                #mx=multiclass_confusion_matrix(preds ,labels,nb_classes,normalize=\"pred\")\n",
    "                #List_mx.append(mx)\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                # update the optimizer parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            if k%log_freq_steps==0 :\n",
    "                epoch_loss = train_running_loss / counter\n",
    "                epoch_acc = 100. * (train_running_correct / nb_images)\n",
    "                if verbose==True:\n",
    "                    print(\"Train_acc \",epoch_acc,\"%\")\n",
    "                    print(d)  \n",
    "                #print(f\"step {i}:\",epoch_loss, epoch_acc)\n",
    "                if testloader!=None and targetacc!=None:\n",
    "                    valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, testloader,  criterion_CNN,10)\n",
    "                    model.train()\n",
    "                    if reached_reconstruction==False:\n",
    "                        if valid_epoch_acc0FN <targetacc :\n",
    "                            reached_reconstruction=False\n",
    "                        else :\n",
    "                            if First==True :\n",
    "                                continue\n",
    "                            else:\n",
    "                                nb_images_needed=nb_images\n",
    "                                reached_reconstruction=True\n",
    "                                First=True\n",
    "                                if verbose==True :\n",
    "                                    print(\"reached reconstruction in \",nb_images_needed,\"images\",valid_epoch_acc0FN ,targetacc)\n",
    "                                break\n",
    "            if reached_reconstruction==True :\n",
    "                break\n",
    "        d=add_dicts(d,d_aux)\n",
    "    if not(df is None):\n",
    "        df.at[track,\"epoch\"]=int(epoch)\n",
    "        df.at[track,\"step\"]=int(k)\n",
    "        df.at[track,\"images_needed\"]=int(nb_images_needed)\n",
    "        df.at[track,\"target test acc\"]=targetacc\n",
    "        df.at[track,\"reached test acc\"]=valid_epoch_acc0FN\n",
    "        # Populate columns 0 to 9\n",
    "        row_values = [int(d.get(i, 0)) for i in list(range(10))]\n",
    "        # print(d,row_values)\n",
    "        for h in range(10):\n",
    "            for key in d.keys():\n",
    "                if h ==key :\n",
    "                    DF2.at[track,\"Label index\"+str(h)]=d[key]\n",
    "                elif  DF2.at[track,\"Label index\"+str(h)]>0:\n",
    "                    continue\n",
    "                else:\n",
    "                    DF2.at[0,\"Label index\"+str(h)]=0\n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / nb_images)\n",
    "        \n",
    "    return epoch_loss, epoch_acc,List_mx,nb_images_needed,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac2c96-5459-467c-87e5-4ef79c17df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    " type(df.at[0,\"Actual Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e800f-701a-4771-b696-67901cfd658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in tqdm(range(len(df))):\n",
    "    y_pred=PD.iloc[i].to_numpy()\n",
    "    checkpoint=OrderedDict()\n",
    "\n",
    "    checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(y_pred[0:200]).reshape([8, 1, 5, 5]))\n",
    "    checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(y_pred[200:208]).reshape([8]))\n",
    "\n",
    "    checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(y_pred[208:1408]).reshape([6, 8, 5, 5]))\n",
    "    checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(y_pred[1408:1414]).reshape([6]))\n",
    "\n",
    "    checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(y_pred[1414:1510]).reshape([4, 6, 2, 2]))\n",
    "    checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(y_pred[1510:1514]).reshape([4]))\n",
    "\n",
    "    checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(y_pred[1514:2234]).reshape([20,36]))\n",
    "    checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(y_pred[2234:2254]).reshape([20]))\n",
    "\n",
    "    checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(y_pred[2254:2454]).reshape([10,20]))\n",
    "    checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(y_pred[2454:2464]).reshape([10]))\n",
    "\n",
    "    Brain = CNN(1,\"gelu\",0,\"kaiming_uniform\")\n",
    "\n",
    "    model=copy.deepcopy(Brain)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    DF2.at[i,\"task1\"] = df.at[i,\"label task 1\"]\n",
    "    DF2.at[i,\"task2\"] = df.at[i,\"label task 2\"]\n",
    "    \n",
    "    task1=ast.literal_eval(df.at[i,\"label task 1\"])\n",
    "    task2=ast.literal_eval(df.at[i,\"label task 2\"])\n",
    "    task3=sorted(task1+task2)\n",
    "    targetacc=df.at[i,\"Actual Accuracy\"]\n",
    "    \n",
    "    L2=[x for x in range(10) if x not in task3]\n",
    "    \n",
    "\n",
    "    criterion_CNN0=CrossEntropyLoss()\n",
    "\n",
    "    test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "    Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "    _, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "    if len(task3)==10:\n",
    "        valid_epoch_acc1=valid_epoch_acc0\n",
    "        #continue\n",
    "    else:\n",
    "        criterion_CNN1=CrossEntropyLoss()\n",
    "        test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "        Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "    #valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n",
    "    #print(\"Reconstructed cnn acc ID\",valid_epoch_acc0)\n",
    "    #print(\"Reconstructed cnn acc OOD\",valid_epoch_acc1)\n",
    "    #DF.at[track,\"Reconstructed Accuracy ID\"]=valid_epoch_acc0\n",
    "\n",
    "    optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "    schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "    criterion_CNN=CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "    Tr_DLr = DataLoader(dataset=train_IF0, batch_size=80, num_workers=0, shuffle=True)\n",
    "    \n",
    "    #fine_tune_needed=0\n",
    "    #FINETUNING\n",
    "    # for epoch_cnn in range(8):\n",
    "        # if epoch_cnn==0:\n",
    "        #     train_epoch_loss, train_epoch_acc,_,images_needed= train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,testloader=Ts_DL0,targetacc=90,df=None,First=True,verbose=True,log_freq_steps=10)\n",
    "        #     valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "        # else:\n",
    "        #      train_epoch_loss, train_epoch_acc,_,images_needed= train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=None,First=False\n",
    "    \n",
    "    train_epoch_loss, train_epoch_acc,_,images_needed,d= train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,n_splits=10,epochs=20,testloader=Ts_DL0,targetacc=targetacc,df=DF2,First=False,verbose=False,log_freq_steps=100)\n",
    "    track=track+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698a883-bca6-4072-97fd-661bc8a39d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF2= pd.read_csv(\"Finetune with 5% of data.csv\")\n",
    "# DF2=DF2[DF2.columns[1:]]\n",
    "# DF2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5a628-60e5-48b6-8cb2-cc412d18b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.to_csv(\"Finetune with 10% of data.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646260c-24a9-47b6-954e-8db94df8511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ded55f-367e-422b-92e7-93f088f95040",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(DF2[['reached test acc','target test acc']])\n",
    "plt.xlabel('Finetuned on 20epochs of 5% vs target')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Boxplot of accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d92fba-e31d-43e3-bdd5-28046deeefea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03968a66-b3ca-4dce-95a4-4b77f9db47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "Brain = CNN(1,\"gelu\",0,\"kaiming_uniform\")\n",
    "task1=[7,1,9,3]\n",
    "task2=[4,5]\n",
    "task3=sorted(task1+task2)\n",
    "L2=[x for x in range(10) if not (x in task3)]\n",
    "model=copy.deepcopy(Brain)\n",
    "#model.load_state_dict(checkpoint)\n",
    "\n",
    "criterion_CNN0=CrossEntropyLoss()\n",
    "\n",
    "test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "_, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "if len(task3)==10:\n",
    "    valid_epoch_acc1=valid_epoch_acc0\n",
    "    #continue\n",
    "else:\n",
    "    criterion_CNN1=CrossEntropyLoss()\n",
    "    test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "    Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "#valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n",
    "#print(\"Reconstructed cnn acc ID\",valid_epoch_acc0)\n",
    "#print(\"Reconstructed cnn acc OOD\",valid_epoch_acc1)\n",
    "#DF.at[track,\"Reconstructed Accuracy ID\"]=valid_epoch_acc0\n",
    "\n",
    "optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "criterion_CNN=CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Tr_DLr = DataLoader(dataset=train_IF0, batch_size=150, num_workers=0, shuffle=True)\n",
    "\n",
    "\n",
    "fine_tune_needed=0\n",
    "#FINETUNING\n",
    "# for epoch_cnn in range(8):\n",
    "    # if epoch_cnn==0:\n",
    "    #     train_epoch_loss, train_epoch_acc,_,images_needed= train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,testloader=Ts_DL0,targetacc=90,df=None,First=True,verbose=True,log_freq_steps=10)\n",
    "    #     valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "    # else:\n",
    "    #      train_epoch_loss, train_epoch_acc,_,images_needed= train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=None,First=False)\n",
    "train_epoch_loss, train_epoch_acc,_,images_needed,d= train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,,epochs=8,testloader=Ts_DL0,targetacc=95,df=DF,First=False,verbose=True,log_freq_steps=100)\n",
    "print(train_epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107863e-2807-45d9-8f63-4285ead0f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bd537-8bf7-4c3a-a37e-80f864b45c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9bfc5-a66c-4a82-b0fe-59817be9dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=20)\n",
    "print(len(Tr_DLr))\n",
    "nb_images=0\n",
    "d={}\n",
    "for k,(image,label) in enumerate(Tr_DLr):\n",
    "    if k ==(len(Tr_DLr)-1):\n",
    "        continue\n",
    "    else:\n",
    "        print(label)\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(image,label)):\n",
    "            if i==0:\n",
    "                #print(f\"batch {k} Fold {i}:\")\n",
    "                #print(f\"  Train: index={train_index}\")\n",
    "                #print(f\"  Test:  index={test_index}\")\n",
    "                #print(f\"Added {label[train_index].shape[0]} to {nb_images} \")\n",
    "                nb_images=nb_images+int(label[test_index].shape[0])\n",
    "                d=add_dicts(d,count_elements(label[test_index]))\n",
    "                #print(\"\\t\",count_elements(label[test_index]) )\n",
    "        break\n",
    "print(d)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e06b7-b537-462e-80e2-7f0bf111fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tr_DLr.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc026a9e-d8db-41ee-8785-716f99be7018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5ce5f-c2d4-4cb6-848a-443fc388d034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d9acc-d9c3-417c-914f-3451f353f759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7269e-6af7-4f9f-81df-ae314941ce34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25f2dd-b07e-43cc-a4cc-6ca7193c3c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819eeac0-6d86-4fbb-bece-c5543d942f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774ae0d-b84b-44ec-ac7d-e1350ed84efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U giotto-tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d33ddd-388d-458a-a1fe-3263fb431120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b3238-2977-46df-b1ef-4d9d2574b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df8b60-e9bd-4d73-82da-e3f64f244850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446f8dc-681b-402e-a2dd-487a3398ff64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3220325-daad-4083-9c74-49a607083fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9813d61-67a6-40ad-8e35-06c6e86b680e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ee762-7139-47d7-9623-3370ed2b2eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da11c5-baed-48bb-9085-d164dd82712d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229593a-6130-4074-95b9-7528ca016fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3112e-0b6c-41c3-b233-1e57447f56d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb1902-fefe-48e1-8e1f-bfc86f689129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8438a-8da1-42f6-a8a0-e395fbe0f59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28b409-eeca-4cef-9210-354bfafa09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d859b-66c9-47ee-a12e-ea03f1d064eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219ac75-ba09-46af-a7a2-5acddb8f8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=df1[df1.columns[7394:]]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdebd87-d0df-44a8-b02a-f593737880c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "DF=df\n",
    "\n",
    "# [\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "# [\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KL divergence\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "# [\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "# [\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "# [\"Wasserstein Loss FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\\n",
    "# [\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\",\"epoch 0\",\"epoch 1\",\"epoch 2\"]+\\\n",
    "# [\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]\n",
    "\n",
    "columns_to_plot = [\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\"]\n",
    "#,[\"epoch 0\",\"epoch 1\",\"epoch 2\",\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=DF[columns_to_plot])\n",
    "plt.title('Boxplot of Selected Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423059c-6090-4cd4-a57e-8204b772d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[columns_to_plot].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3987a-e956-4d0c-8c6a-ad6a17ff29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "DF=df\n",
    "\n",
    "# [\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "# [\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KL divergence\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "# [\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "# [\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "# [\"Wasserstein Loss FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\\n",
    "# [\"Step 0\",\"Step 25\",\"Step 50\",\"Step 75\",\"Step 100\",\"Step 125\",\"Step 150\",\"Step 175\",\"Step 200\",\"Step 225\",\"Step 250\",\"Step 275\",\"epoch 0\",\"epoch 1\",\"epoch 2\"]+\\\n",
    "# [\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]\n",
    "\n",
    "columns_to_plot = [\"epoch 0\",\"epoch 1\",\"epoch 2\",\"epoch 3\",\"epoch 4\",\"epoch 5\",\"epoch 6\",\"epoch 7\"]\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=DF[columns_to_plot])\n",
    "plt.title('Boxplot of Selected Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1019910-a506-4700-9864-25a67001ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[columns_to_plot].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a28cb3-d30d-49fc-8f7d-7e6fd4899439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "#df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\"]\n",
    "group2_columns = [\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\"]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = df[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = df[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(6):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of evolution of norm')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bf43c-316b-4519-8ff5-1ea2930b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"Actual Accuracy\",\"Reconstructed Accuracy ID\",\"Transformer train Loss\"]+\\\n",
    "# [\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\",\"KL divergence\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\",\"LWLN\"]+\\\n",
    "# [\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\",\"contractive distance\",\"N1\",\"N11\",\"N12\",\"N13\",\"N14\",\"N15\",\"N2\",\"N21\",\"N22\",\"N23\",\"N24\",\"N25\",\"saturated in pred(%)\",\"saturated in GT(%)\"]+\\\n",
    "# [\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\",\"KL divergence FN\",\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\",\"LWLN FN\"]+\\\n",
    "# [\"Wasserstein Loss FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\",\"contractive distance FN\",\"N1 FN\",\"N11 FN\",\"N12 FN\",\"N13 FN\",\"N14 FN\",\"N15 FN\",\"N2 FN\",\"N21 FN\",\"N22 FN\",\"N23 FN\",\"N24 FN\",\"N25 FN\",\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]+\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945c548-7860-40df-865f-efd040568ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "#df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"MSE\",\"MSE 1\",\"MSE 2\",\"MSE 3\",\"MSE 4\",\"MSE 5\"]\n",
    "group2_columns = [\"MSE FN\",\"MSE 1 FN\",\"MSE 2 FN\",\"MSE 3 FN\",\"MSE 4 FN\",\"MSE 5 FN\"]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = DF[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = DF[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(6):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of MSE and MSE per layer')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff2a37-61fe-45e2-a38f-d0590680b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56c238-ee11-48ca-b358-fa51442f9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"KLD\",\"KL 1\",\"KL 2\",\"KL 3\",\"KL 4\",\"KL 5\"]\n",
    "group2_columns = [\"KL divergence FN\" ,\"KL 1 FN\",\"KL 2 FN\",\"KL 3 FN\",\"KL 4 FN\",\"KL 5 FN\"]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = results[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = results[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(6):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of KLD and KLD per layer')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013b705-28e8-47b9-a5e9-c43e3a0bffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"Wasserstein Loss\",\"WS 1\",\"WS 2\",\"WS 3\",\"WS 4\",\"WS 5\"]\n",
    "group2_columns = [\"WSD FN\",\"WS 1 FN\",\"WS 2 FN\",\"WS 3 FN\",\"WS 4 FN\",\"WS 5 FN\" ]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = results[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = results[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(6):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of Wasserstein loss and WSD per layer')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca009f1-d516-4036-94cc-6b095516882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "#df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"contractive distance\"]\n",
    "group2_columns = [\"contractive distance FN\"]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = results[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = results[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(1):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of Contractive Loss')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5644c65-3e20-4ad8-8500-2a96d5dff3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"LWLN\"]\n",
    "group2_columns = [\"LWLN FN\"]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = results[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = results[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(1):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of Layer wise Loss normalisation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d19d34-20af-4b6d-8d0b-3df72519acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups of columns\n",
    "#df= pd.DataFrame(columns=Cols[7394:])\n",
    "group1_columns = [\"saturated in pred(%)\",\"saturated in GT(%)\"]\n",
    "group2_columns = [\"saturated in pred FN(%)\",\"saturated in GT FN(%)\"]\n",
    "\n",
    "# Create a long-format DataFrame for seaborn\n",
    "group1_melted = results[group1_columns].melt(var_name='Columns', value_name='Values')\n",
    "group1_melted['Group'] = 'Before finetuning'\n",
    "\n",
    "group2_melted = results[group2_columns].melt(var_name='Columns', value_name='Values')\n",
    "group2_melted['Group'] = 'After Finetuning'\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "melted_df = pd.concat([group1_melted, group2_melted])\n",
    "\n",
    "# Create a custom order for columns to interleave the groups\n",
    "custom_order=[]\n",
    "for i in range(2):\n",
    "    custom_order.append(group1_columns[i])\n",
    "    custom_order.append(group2_columns[i])\n",
    "melted_df['Columns'] = pd.Categorical(melted_df['Columns'], categories=custom_order, ordered=True)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Columns', y='Values', hue='Group', data=melted_df)\n",
    "plt.title('Comparaison of % of saturated (above 0.95 or below 0.05) neurones')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febe195-65a6-46f6-a4b0-c221bde3a3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940d730-4d47-42f9-89e4-1cb8d9c05321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37640116-c76f-4bcb-9261-684ecf4600c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c8d8c-f17a-413f-a4bb-7f982d74307d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfd2de-e1b7-4d1e-8928-e17b04e3edfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75422eba-5450-454c-8a61-89b1d6d42963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999e801-be68-46c0-b6ff-7ad3adfcd9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f10ff7-810b-495a-baec-b6b8a88733c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b1571-6232-4b7a-a27a-44818f88a51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9099d-fee3-444f-9db5-64bd78110071",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP[40][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d0df8-f15c-41c9-8fbb-fdc74274c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " if (i in [nb_batches-1] and (epoch%5==0)) or (i==0 and epoch==0):\n",
    "            \n",
    "            \n",
    "            #print(f\"Validating ... {loss_val}\")\n",
    "            for vect in [0,10,40,50,70]:\n",
    "                y_pred=torch.unsqueeze(output[0][vect], 0) \n",
    "                y =torch.unsqueeze(tg[vect], 0) \n",
    "\n",
    "                selected_row = cs_tr.df.iloc[int(U[vect][0]), 11:17]  \n",
    "                columns_with_one = selected_row[selected_row == 1].index.tolist()\n",
    "                activ=columns_with_one\n",
    "                epochCNN=cs_tr.df.loc[int(U[vect][0])]['epoch']\n",
    "\n",
    "\n",
    "                checkpoint=OrderedDict()\n",
    "                vector_aux= output[0][vect].detach()\n",
    "                y_pred=vector_aux.cpu()\n",
    "\n",
    "                task1=[int(x) for x in EXP[vect][0]]\n",
    "                task2=[int(x) for x in EXP[vect][1]]\n",
    "                task3=sorted(task1+task2)\n",
    "\n",
    "\n",
    "                All=list(range(10))\n",
    "                L2=[k for k in All if k not in task3] # Out of distribution classes\n",
    "                L_others=[k for k in All if k not in task3] #Classes to test on (In distribution)\n",
    "\n",
    "                checkpoint[\"module_list.0.weight\"]=torch.tensor(np.array(y_pred[0:200]).reshape([8, 1, 5, 5]))\n",
    "                checkpoint[\"module_list.0.bias\"]=torch.tensor(np.array(y_pred[200:208]).reshape([8]))\n",
    "\n",
    "                checkpoint[\"module_list.3.weight\"]=torch.tensor(np.array(y_pred[208:1408]).reshape([6, 8, 5, 5]))\n",
    "                checkpoint[\"module_list.3.bias\"]=torch.tensor(np.array(y_pred[1408:1414]).reshape([6]))\n",
    "\n",
    "                checkpoint[\"module_list.6.weight\"]=torch.tensor(np.array(y_pred[1414:1510]).reshape([4, 6, 2, 2]))\n",
    "                checkpoint[\"module_list.6.bias\"]=torch.tensor(np.array(y_pred[1510:1514]).reshape([4]))\n",
    "\n",
    "                checkpoint[\"module_list.9.weight\"]=torch.tensor(np.array(y_pred[1514:2234]).reshape([20,36]))\n",
    "                checkpoint[\"module_list.9.bias\"]=torch.tensor(np.array(y_pred[2234:2254]).reshape([20]))\n",
    "\n",
    "                checkpoint[\"module_list.11.weight\"]=torch.tensor(np.array(y_pred[2254:2454]).reshape([10,20]))\n",
    "                checkpoint[\"module_list.11.bias\"]=torch.tensor(np.array(y_pred[2454:2464]).reshape([10]))\n",
    "\n",
    "                Brain = CNN(1,activ[0],0,\"kaiming_uniform\")\n",
    "\n",
    "                model=copy.deepcopy(Brain)\n",
    "                model.load_state_dict(checkpoint)\n",
    "\n",
    "                criterion_CNN0=CrossEntropyLoss()\n",
    "\n",
    "                test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnistDistilled/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "                _, valid_epoch_acc0,_= validate(model, Ts_DL0,  criterion_CNN0,10)\n",
    "                if len(task3)==10:\n",
    "                    valid_epoch_acc1=valid_epoch_acc0\n",
    "                    continue\n",
    "                else:\n",
    "                    criterion_CNN1=CrossEntropyLoss()\n",
    "                    test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnistDistilled/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                    Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=120, num_workers=0, shuffle=False)\n",
    "\n",
    "                valid_epoch_loss0, valid_epoch_acc1,L_mx= validate(model, Ts_DL1,  criterion_CNN1,10)\n",
    "                cnn_acc_ID.append(valid_epoch_acc0)\n",
    "                cnn_acc_OOD.append(valid_epoch_acc1)\n",
    "                #lr = optimizer.param_groups[0][\"lr\"]\n",
    "                lrE1=optimizerEnc1.param_groups[0][\"lr\"]\n",
    "                lrE2=optimizerEnc2.param_groups[0][\"lr\"]\n",
    "                lrL=optimizerDense.param_groups[0][\"lr\"]\n",
    "                lrD=optimizerDec.param_groups[0][\"lr\"]\n",
    "                \n",
    "                \n",
    "                \n",
    "            #print(f\"Reconstructing ... {valid_epoch_acc0}\")\n",
    "                optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "                schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "                criterion_CNN=CrossEntropyLoss()\n",
    "                \n",
    "                \n",
    "                train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnistDistilled/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "                Tr_DLr = DataLoader(dataset=train_IF0, batch_size=100, num_workers=0, shuffle=True)\n",
    "                \n",
    "                \n",
    "                wandb.define_metric(f\"finetune_step for epoch {epoch}\")\n",
    "                wandb.define_metric(f\"[{epoch}]CNN_fn_loss train {task1}{task2}{task3}\", step_metric=f\"finetune_step for epoch {epoch}\")\n",
    "                wandb.define_metric(f\"[{epoch}]CNN_fn_acc train{task1}{task2}{task3}\", step_metric=f\"finetune_step for epoch {epoch}\")\n",
    "                \n",
    "                wandb.define_metric(f\"[{epoch}]CNN_fn_loss test {task1}{task2}{task3}\", step_metric=f\"finetune_step for epoch {epoch}\")\n",
    "                wandb.define_metric(f\"[{epoch}]CNN_fn_acc test{task1}{task2}{task3}\", step_metric=f\"finetune_step for epoch {epoch}\")\n",
    "                fine_tune_needed=0\n",
    "                #print(task1,task2,task3)\n",
    "                for epoch_cnn in range(15):\n",
    "                    \n",
    "                    # for param in model.parameters():\n",
    "                    #     print(f\"Param before step: {param.data[0]}\")\n",
    "                    #     break\n",
    "                    train_epoch_loss, train_epoch_acc,_ = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10)\n",
    "                    valid_epoch_loss0FN, valid_epoch_acc0FN,_= validate(model, Ts_DL0,  criterion_CNN,10)\n",
    "                    schedulerCNN.step()\n",
    "                    # for param in model.parameters():\n",
    "                    #     print(f\"Param after step: {param.data[0]}\")\n",
    "                    #     break\n",
    "                    fine_tune_needed+=1\n",
    "                    #if fine_tune_needed%5==0:\n",
    "                        #print(f\"Accuracy of {train_epoch_acc:.2f} / ACC[vect][2] after {fine_tune_needed} epochs\")\n",
    "                    wandb.log({f\"[{epoch}]CNN_fn_loss train {task1}{task2}{task3}\":train_epoch_loss,f\"[{epoch}]CNN_fn_acc train {task1}{task2}{task3}\":train_epoch_acc , f\"finetune_step for epoch {epoch}\": fine_tune_needed})\n",
    "                    wandb.log({f\"[{epoch}]CNN_fn_loss test {task1}{task2}{task3}\":valid_epoch_loss0FN,f\"[{epoch}]CNN_fn_acc test {task1}{task2}{task3}\":valid_epoch_acc0FN , f\"finetune_step for epoch {epoch}\": fine_tune_needed})\n",
    "                    \n",
    "                          \n",
    "                    wandb.log({f\"[{epoch}]finetune ratio {task1}{task2}{task3}\":valid_epoch_acc0FN/ACC[vect][2], f\"finetune_step for epoch {epoch}\": fine_tune_needed})\n",
    "                    wandb.log({f\"combined target accuracy {task1}{task2}{task3}\":ACC[vect][2], f\"finetune_step for epoch {epoch}\": fine_tune_needed})\n",
    "                # define our custom x axis metric\n",
    "\n",
    "            \n",
    "        wandb.log({\"Loss\":loss_to_save})\n",
    "        wandb.log({\"CNN_IID_no_fine_tune\":(np.mean(cnn_acc_ID))})\n",
    "        wandb.log({\"CNN_OOD_no_fine_tune\":(np.mean(cnn_acc_OOD))})\n",
    "        wandb.log({\"lr Encoder 1\": lrE1,\"lr Encoder 2\": lrE2,\"lr Linear\": lrL,\"lr Decoder\": lrD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddb1dd-7ba0-4d3a-b94a-132b7c9ac4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787d678-66e0-4fdc-b4b3-11f1c42fca65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f9ea2-757f-4c08-a838-968f9b27d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "hm=sns.heatmap(torch.mean( torch.mean(output[4][-1], dim=1), dim=0).detach().cpu(), annot=False, cmap='cubehelix')\n",
    "plt.title('Attention Heatmap')\n",
    "\n",
    "heatmap_path = f'heatmap 1 _step_{0}.svg'\n",
    "plt.savefig(heatmap_path,format='svg', dpi=800)\n",
    "plt.close()\n",
    "\n",
    "#wandb.log({\"attention_heatmap 1\": wandb.Image(heatmap_path)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2949b-41d1-4cb9-90e5-ec389c2e6e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38362539-dd58-4e3c-925e-f313b21e4819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54092b-8cfd-4691-a8f9-caa1b00eb39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7993d-f994-485c-9169-827e94211812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbfa86-331e-4c39-92f4-e35133be58bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699aba2-6928-4425-b344-ad810f9fc144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4966281-ab38-4bac-aaef-98d1ff59e94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df29ecd-9235-45f4-951a-ecfe9cc1a2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616cff7-b29e-4984-9dc5-f8b13aec2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Loss==\"LWLN\":\n",
    "                # vec1_np = output[0].detach().cpu().numpy()\n",
    "                # vec2_np = tg.detach().cpu().numpy()\n",
    "                # wsd_list = [wasserstein_distance(vec1_np[i], vec2_np[i]) for i in range(vec1_np.shape[0])]\n",
    "                # average_wsd = sum(wsd_list) / len(wsd_list)\n",
    "                ### loss_tr = criterion(output[0],tg)+average_wsd\n",
    "\n",
    "\n",
    "                #fnj=frobenius_norm_jacobian(mod, x1,x2).cuda()\n",
    "                #qql=q_quantile_loss(output[0],tg,0.9).cuda()\n",
    "                ###loss_tr=0.15*fnj+qql\n",
    "\n",
    "                \"\"\"vec1_np = output[0].detach().cpu().numpy()\n",
    "                vec2_np = tg.detach().cpu().numpy()\n",
    "                fid_values = np.array([fisher_information_difference(vec1_np[i, :], vec2_np[i, :]) for i in range(vec1_np.shape[0])])\n",
    "                abs_diff_sum = np.sum(np.abs(vec1_np), axis=1) - np.sum(np.abs(vec2_np), axis=1)\n",
    "                log_abs_sum_diff = np.log(np.abs(abs_diff_sum))\n",
    "                wsd_list = fid_values + log_abs_sum_diff\n",
    "                average_FIM_N1_Loss = np.mean(wsd_list)\n",
    "                loss_tr = criterion(output[0], tg) + average_FIM_N1_Loss\n",
    "                time_batch = time.time()\"\"\"\n",
    "\n",
    "                # wsd_list = [fisher_information_difference(vec1_np[i,:], vec2_np[i,:])\n",
    "                #             +np.log(abs(np.sum(np.abs(vec1_np[i,:])-np.sum(np.abs(vec2_np[i,:])))))\n",
    "                #             for i in range(vec1_np.shape[0])]\n",
    "                # average_FIM_N1_Loss= sum(wsd_list) / len(wsd_list)\n",
    "                # loss_tr = criterion(output[0],tg)+average_FIM_N1_Loss\n",
    "\n",
    "\n",
    "    #         except Exception as e:\n",
    "    #             # Print the exception\n",
    "    #             print(\"An exception occurred:\", e)\n",
    "    #             traceback.print_exc()\n",
    "    #             # Continue with the loop\n",
    "    #             continue\n",
    "\n",
    "                loss_tr = criterion(output[0],tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73672723-0e7f-42b2-8c86-1dad689519f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bfa23f52-93fa-410f-918e-d9da64b9145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           All: 190.1 KiB\n",
      "                   train_pair2: 133.4 KiB\n",
      "                           _ii: 128.3 KiB\n",
      "                         _i157: 128.3 KiB\n",
      "                           _74: 64.8 KiB\n",
      "                           _82: 64.8 KiB\n",
      "                           _83: 64.8 KiB\n",
      "                           _89: 64.8 KiB\n",
      "                           _91: 64.8 KiB\n",
      "                           _92: 64.8 KiB\n"
     ]
    }
   ],
   "source": [
    "#https://discuss.pytorch.org/t/memory-management-using-pytorch-cuda-alloc-conf/157850\n",
    "#https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-can-i-set-max-split-size-mb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047af7-0c3c-485c-8bcb-f605b1ac0064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
