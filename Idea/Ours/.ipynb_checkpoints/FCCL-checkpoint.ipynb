{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53edd752-fc69-4841-a49b-310d3078e087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/crns/Desktop/PFE/PFE/Code', '/home/crns/anaconda3/envs/torch10/lib/python39.zip', '/home/crns/anaconda3/envs/torch10/lib/python3.9', '/home/crns/anaconda3/envs/torch10/lib/python3.9/lib-dynload', '', '/home/crns/anaconda3/envs/torch10/lib/python3.9/site-packages', '/home/crns/Desktop/PFE/PFE/Code', '/home/crns/Desktop/PFE/PFE/Code', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Dataset', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Idea', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Network', '../../', '/home/crns/Desktop/PFE/PFE/Code', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Dataset', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Idea', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Network', '../../', '/home/crns/Desktop/PFE/PFE/Code', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Dataset', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Idea', '/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Network']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--Dataset_Dir DATASET_DIR]\n",
      "                             [--Project_Dir PROJECT_DIR]\n",
      "                             [--Original_Path ORIGINAL_PATH]\n",
      "                             [--CommunicationEpoch COMMUNICATIONEPOCH]\n",
      "                             [--Seed SEED] [--device_ids DEVICE_IDS]\n",
      "                             [--N_Participants N_PARTICIPANTS]\n",
      "                             [--Scenario SCENARIO]\n",
      "                             [--Public_Dataset_Name PUBLIC_DATASET_NAME]\n",
      "                             [--Private_Net_Name_List PRIVATE_NET_NAME_LIST]\n",
      "                             [--Private_Dataset_Name_List PRIVATE_DATASET_NAME_LIST]\n",
      "                             [--Private_Data_Total_Len_List PRIVATE_DATA_TOTAL_LEN_LIST]\n",
      "                             [--Private_Data_Len_List PRIVATE_DATA_LEN_LIST]\n",
      "                             [--Private_Training_Epoch PRIVATE_TRAINING_EPOCH]\n",
      "                             [--Private_Dataset_Classes PRIVATE_DATASET_CLASSES]\n",
      "                             [--Private_Net_Feature_Dim_List PRIVATE_NET_FEATURE_DIM_LIST]\n",
      "                             [--TrainBatchSize TRAINBATCHSIZE]\n",
      "                             [--Local_TrainBatchSize LOCAL_TRAINBATCHSIZE]\n",
      "                             [--TestBatchSize TESTBATCHSIZE]\n",
      "                             [--Public_Training_Epoch PUBLIC_TRAINING_EPOCH]\n",
      "                             [--Public_Dataset_Length PUBLIC_DATASET_LENGTH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/crns/.local/share/jupyter/runtime/kernel-f577c766-22a4-4e8a-a54b-edb843872949.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() )\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL\")\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Dataset\")\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Idea\")\n",
    "sys.path.append(\"/home/crns/Desktop/PFE/PFE/Code/FCCL/FCCL/Network\")\n",
    "print(sys.path)\n",
    "from Network.utils_network import init_nets\n",
    "from Dataset.utils_dataset import init_logs, get_dataloader,generate_public_data_idxs\n",
    "from Idea.utils_idea import update_model_via_private_data_with_two_model,evaluate_network,mkdirs\n",
    "from Idea.params import args_parser\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import os\n",
    "\n",
    " \n",
    "\n",
    "args = args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9cc869-3345-475e-8e33-63912ee3bd62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m Ablation_Name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFCCL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m Temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m Scenario \u001b[38;5;241m=\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mScenario\n\u001b[1;32m      9\u001b[0m Seed \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mSeed\n\u001b[1;32m     10\u001b[0m N_Participants \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mN_Participants\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Global Parameters\n",
    "'''\n",
    "Method_Name = 'Ours'\n",
    "Ablation_Name='FCCL'\n",
    "\n",
    "Temperature = 1\n",
    "Scenario = args.Scenario\n",
    "Seed = args.Seed\n",
    "N_Participants = args.N_Participants\n",
    "CommunicationEpoch = args.CommunicationEpoch\n",
    "TrainBatchSize = args.TrainBatchSize\n",
    "TestBatchSize = args.TestBatchSize\n",
    "Dataset_Dir = args.Dataset_Dir\n",
    "Project_Dir = args.Project_Dir\n",
    "Idea_Ours_Dir = args.Project_Dir + 'Idea/Ours/'\n",
    "Private_Net_Name_List = args.Private_Net_Name_List\n",
    "Pariticpant_Params = {\n",
    "    'loss_funnction' : 'KLDivLoss',\n",
    "    'optimizer_name' : 'Adam',\n",
    "    'learning_rate'  : 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e064c7d-c732-4160-850b-c62d31292cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scenario for large domain gap\n",
    "'''\n",
    "Private_Dataset_Name_List = args.Private_Dataset_Name_List\n",
    "Private_Data_Total_Len_List = args.Private_Data_Total_Len_List\n",
    "Private_Data_Len_List = args.Private_Data_Len_List\n",
    "Private_Training_Epoch = args.Private_Training_Epoch\n",
    "Private_Dataset_Classes = args.Private_Dataset_Classes\n",
    "Output_Channel = len(Private_Dataset_Classes)\n",
    "'''\n",
    "Public data parameters\n",
    "'''\n",
    "Public_Dataset_Name = args.Public_Dataset_Name\n",
    "Public_Dataset_Length = args.Public_Dataset_Length\n",
    "Public_Dataset_Dir = Dataset_Dir+Public_Dataset_Name\n",
    "Public_Training_Epoch = args.Public_Training_Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650bca5-a4bd-420b-a8d8-acff90ce0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bba2c-f670-4c36-8a16-ccfb8a365cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = init_logs(sub_name=Ablation_Name)\n",
    "logger.info('Method Name : '+Method_Name + ' Ablation Name : '+Ablation_Name)\n",
    "logger.info(\"Random Seed and Server Config\")\n",
    "seed = Seed\n",
    "np.random.seed(seed)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "device_ids = args.device_ids\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901429c-d894-430b-8c04-9a40716ca0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Initialize Participants' Data idxs and Model\")\n",
    "# For Digits scenario\n",
    "private_dataset_idxs_dict = {}\n",
    "for index in range(N_Participants):\n",
    "    idxes = np.random.permutation(Private_Data_Total_Len_List[index])\n",
    "    idxes = idxes[0:Private_Data_Len_List[index]]\n",
    "    private_dataset_idxs_dict[Private_Dataset_Name_List[index]]= idxes\n",
    "logger.info(private_dataset_idxs_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece8c1e-147b-4b03-82df-ef40059706c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_list = init_nets(n_parties=N_Participants,nets_name_list=Private_Net_Name_List,num_classes=Output_Channel)\n",
    "logger.info(\"Load Participants' Models\")\n",
    "for i in range(N_Participants):\n",
    "    network = net_list[i]\n",
    "    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "    network = network.to(device)\n",
    "    netname = Private_Net_Name_List[i]\n",
    "    private_dataset_name = Private_Dataset_Name_List[i]\n",
    "    private_model_path = Project_Dir + 'Network/Model_Storage/' + netname + '_' + str(i) + '_' + private_dataset_name + '.ckpt'\n",
    "    #network.load_state_dict(torch.load(private_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68ba3a-109b-40aa-a8a2-ab3ef4ddce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_net_list = init_nets(n_parties=N_Participants,nets_name_list=Private_Net_Name_List,num_classes=Output_Channel)\n",
    "logger.info(\"Load Frozen Participants' Models\")\n",
    "for i in range(N_Participants):\n",
    "    network = frozen_net_list[i]\n",
    "    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "    network = network.to(device)\n",
    "    netname = Private_Net_Name_List[i]\n",
    "    private_dataset_name = Private_Dataset_Name_List[i]\n",
    "    private_model_path = Project_Dir + 'Network/Model_Storage/' + netname + '_' + str(i) + '_' + private_dataset_name + '.ckpt'\n",
    "    #network.load_state_dict(torch.load(private_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577e8de-c54a-4fbd-a909-6bcbd26b9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "progressive_net_list = init_nets(n_parties=N_Participants,nets_name_list=Private_Net_Name_List,num_classes=Output_Channel)\n",
    "logger.info(\"Load Progressive Participants' Models\")\n",
    "for i in range(N_Participants):\n",
    "    network = progressive_net_list[i]\n",
    "    #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "    network = network.to(device)\n",
    "    netname = Private_Net_Name_List[i]\n",
    "    private_dataset_name = Private_Dataset_Name_List[i]\n",
    "    private_model_path = Project_Dir + 'Network/Model_Storage/' + netname + '_' + str(i) + '_' + private_dataset_name + '.ckpt'\n",
    "    #network.load_state_dict(torch.load(private_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442c084-2cb0-4824-911d-a9254586d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Initialize Public Data Parameters\")\n",
    "print(Scenario+Public_Dataset_Name)\n",
    "public_data_indexs = generate_public_data_idxs(dataset=Public_Dataset_Name,datadir=Public_Dataset_Dir,size=Public_Dataset_Length)\n",
    "\n",
    "public_train_dl, _, _, _ = get_dataloader(dataset=Public_Dataset_Name, datadir=Public_Dataset_Dir,\n",
    "                                                            train_bs=TrainBatchSize, test_bs=TestBatchSize,\n",
    "                                                            dataidxs=public_data_indexs)\n",
    "logger.info('Initialize Private Data Loader')\n",
    "private_train_data_loader_list = []\n",
    "private_test_data_loader_list = []\n",
    "for participant_index in range(N_Participants):\n",
    "    private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "    private_dataidx = private_dataset_idxs_dict[private_dataset_name]\n",
    "    private_dataset_dir = Dataset_Dir + private_dataset_name\n",
    "    train_dl_local, test_dl_local, _, _ = get_dataloader(dataset=private_dataset_name,\n",
    "                                                 datadir=private_dataset_dir,\n",
    "                                                 train_bs=TrainBatchSize, test_bs=TestBatchSize,\n",
    "                                                 dataidxs=private_dataidx)\n",
    "    private_train_data_loader_list.append(train_dl_local)\n",
    "    private_test_data_loader_list.append(test_dl_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d18ed-3226-4967-8a15-9aaf067b119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_loss_list = []\n",
    "local_loss_list = []\n",
    "acc_list = []\n",
    "for epoch_index in range(CommunicationEpoch):\n",
    "    logger.info(\"The \"+str(epoch_index)+\" th Communication Epoch\")\n",
    "    logger.info('Evaluate Models')\n",
    "    acc_epoch_list = []\n",
    "    for participant_index in range(N_Participants):\n",
    "        netname = Private_Net_Name_List[participant_index]\n",
    "        private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "        private_dataset_dir = Dataset_Dir + private_dataset_name\n",
    "        print(netname + '_' + private_dataset_name + '_' + private_dataset_dir)\n",
    "            _, test_dl, _, _ = get_dataloader(dataset=private_dataset_name, datadir=private_dataset_dir,\n",
    "                                              train_bs=TrainBatchSize,\n",
    "                                              test_bs=TestBatchSize, dataidxs=None)\n",
    "        network = net_list[participant_index]\n",
    "        #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "        network = network.to(device)\n",
    "        acc_epoch_list.append(evaluate_network(network=network, dataloader=test_dl, logger=logger))\n",
    "    acc_list.append(acc_epoch_list)\n",
    "\n",
    "    a = datetime.now()\n",
    "    for _ in range(Public_Training_Epoch):\n",
    "        for batch_idx, (images, _) in enumerate(public_train_dl):\n",
    "            linear_output_list = []\n",
    "            linear_output_target_list = [] # Save other participants' linear output\n",
    "            linear_output_progressive_list = [] # Save itself progressive model's linear output\n",
    "            col_loss_batch_list = []\n",
    "            '''\n",
    "             Calculate Linear Output\n",
    "            '''\n",
    "            for participant_index in range(N_Participants):\n",
    "                network = net_list[participant_index]\n",
    "                #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "                network = network.to(device)\n",
    "                network.train()\n",
    "                images = images.to(device)\n",
    "                linear_output = network(x=images)\n",
    "                linear_output_target_list.append(linear_output.clone().detach())\n",
    "                linear_output_list.append(linear_output)\n",
    "            '''\n",
    "            Calculate Progressive Linear Output\n",
    "            '''\n",
    "            for progressive_participant_index in range(N_Participants):\n",
    "                progressive_network = progressive_net_list[progressive_participant_index]\n",
    "                #progressive_network = nn.DataParallel(progressive_network,device_ids=device_ids).to(device)\n",
    "                progressive_network = progressive_network.to(device)\n",
    "                progressive_network.eval()\n",
    "                with torch.no_grad():\n",
    "                    images = images.to(device)\n",
    "                    progressive_linear_output = progressive_network(images)\n",
    "                    linear_output_progressive_list.append(progressive_linear_output)\n",
    "                '''\n",
    "                Update Participants' Models via Col Loss\n",
    "                '''\n",
    "            for participant_index in range(N_Participants):\n",
    "                '''\n",
    "                Calculate the Loss with others\n",
    "                '''\n",
    "                network = net_list[participant_index]\n",
    "                #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "                network = network.to(device)\n",
    "                network.train()\n",
    "                optimizer = optim.Adam(network.parameters(), lr=Pariticpant_Params['learning_rate'])\n",
    "                optimizer.zero_grad()\n",
    "                linear_output_target_avg_list = []\n",
    "                for i in range(N_Participants):\n",
    "                    if i != participant_index:\n",
    "                        linear_output_target_avg_list.append(linear_output_target_list[i])\n",
    "                    if i ==participant_index:\n",
    "                        linear_output_target_avg_list.append(linear_output_progressive_list[i])\n",
    "\n",
    "                linear_output_target_avg = torch.mean(torch.stack(linear_output_target_avg_list), 0)\n",
    "                linear_output = linear_output_list[participant_index]\n",
    "                z_1_bn = (linear_output-linear_output.mean(0))/linear_output.std(0)\n",
    "                z_2_bn = (linear_output_target_avg-linear_output_target_avg.mean(0))/linear_output_target_avg.std(0)\n",
    "                # empirical cross-correlation matrix\n",
    "                c = z_1_bn.T @ z_2_bn\n",
    "                # sum the cross-correlation matrix between all gpus\n",
    "                c.div_(len(images))\n",
    "\n",
    "                on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "                off_diag = off_diagonal(c).add_(1).pow_(2).sum()\n",
    "                col_loss = on_diag + 0.0051 * off_diag\n",
    "                col_loss_batch_list.append(col_loss.item())\n",
    "                col_loss.backward()\n",
    "                optimizer.step()\n",
    "            col_loss_list.append(col_loss_batch_list)\n",
    "\n",
    "        '''\n",
    "        Update Participants' Models via Private Data\n",
    "        '''\n",
    "    local_loss_batch_list = []\n",
    "    for participant_index in range(N_Participants):\n",
    "        network = net_list[participant_index]\n",
    "        #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "        network = network.to(device)\n",
    "        network.train()\n",
    "\n",
    "        frozen_network = frozen_net_list[participant_index]\n",
    "        #frozen_network = nn.DataParallel(frozen_network,device_ids=device_ids).to(device)\n",
    "        frozen_network = frozen_network.to(device)\n",
    "        frozen_network.eval()\n",
    "\n",
    "        progressive_network = progressive_net_list[participant_index]\n",
    "        #progressive_network = nn.DataParallel(progressive_network,device_ids=device_ids).to(device)\n",
    "        progressive_network = progressive_network.to(device)\n",
    "        progressive_network.eval()\n",
    "\n",
    "        private_dataset_name=  Private_Dataset_Name_List[participant_index]\n",
    "        private_dataidx = private_dataset_idxs_dict[private_dataset_name]\n",
    "        private_dataset_dir = Dataset_Dir+private_dataset_name\n",
    "        train_dl_local, _, train_ds_local, _ = get_dataloader(dataset=private_dataset_name,\n",
    "                                                                  datadir=private_dataset_dir,\n",
    "                                                                  train_bs=TrainBatchSize, test_bs=TestBatchSize,\n",
    "                                                                  dataidxs=private_dataidx)\n",
    "\n",
    "        private_epoch = max(int(Public_Dataset_Length / len(train_ds_local)), 1)\n",
    "        private_epoch = Private_Training_Epoch[participant_index]\n",
    "\n",
    "        network, private_loss_batch_list = update_model_via_private_data_with_two_model(network=network,\n",
    "        frozen_network=frozen_network,progressive_network=progressive_network,\n",
    "        temperature = Temperature,private_epoch=private_epoch,private_dataloader=train_dl_local,\n",
    "        loss_function=Pariticpant_Params['loss_funnction'],optimizer_method=Pariticpant_Params['optimizer_name'],\n",
    "        learing_rate=Pariticpant_Params['learning_rate'],logger=logger)\n",
    "        mean_private_loss_batch = mean(private_loss_batch_list)\n",
    "        local_loss_batch_list.append(mean_private_loss_batch)\n",
    "    local_loss_list.append(local_loss_batch_list)\n",
    "\n",
    "    b = datetime.now()\n",
    "    temp = b-a\n",
    "    print(temp)\n",
    "        '''\n",
    "        用于迭代 Progressive 模型\n",
    "        '''\n",
    "    for j in range(N_Participants):\n",
    "        progressive_net_list[j] = copy.deepcopy(net_list[j])\n",
    "\n",
    "    if epoch_index ==CommunicationEpoch-1:\n",
    "        acc_epoch_list = []\n",
    "        logger.info('Final Evaluate Models')\n",
    "        for participant_index in range(N_Participants):\n",
    "            netname = Private_Net_Name_List[participant_index]\n",
    "            private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "            private_dataset_dir = Dataset_Dir + private_dataset_name\n",
    "            print(netname+'_'+private_dataset_name+'_'+private_dataset_dir)\n",
    "            _, test_dl, _, _ = get_dataloader(dataset=private_dataset_name, datadir=private_dataset_dir,\n",
    "                                                  train_bs=TrainBatchSize,\n",
    "                                                  test_bs=TestBatchSize, dataidxs=None)\n",
    "                \n",
    "            network = net_list[participant_index]\n",
    "            #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "            network = network.to(device)\n",
    "            acc_epoch_list.append(evaluate_network(network=network, dataloader=test_dl, logger=logger))\n",
    "        acc_list.append(acc_epoch_list)\n",
    "\n",
    "    if epoch_index % 5 == 3 or epoch_index == CommunicationEpoch - 1:\n",
    "        mkdirs(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario)\n",
    "        mkdirs(Idea_Ours_Dir + '/Model_Storage/' + Scenario)\n",
    "        mkdirs(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name)\n",
    "        mkdirs(Idea_Ours_Dir + '/Model_Storage/' + Scenario + '/' + Ablation_Name)\n",
    "        mkdirs(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name)\n",
    "        mkdirs(Idea_Ours_Dir + '/Model_Storage/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name)\n",
    "\n",
    "        logger.info('Save Loss')\n",
    "        col_loss_array = np.array(col_loss_list)\n",
    "        np.save(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                    + '/collaborative_loss.npy', col_loss_array)\n",
    "        local_loss_array = np.array(local_loss_list)\n",
    "        np.save(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                    + '/local_loss.npy', local_loss_array)\n",
    "        logger.info('Save Acc')\n",
    "        acc_array = np.array(acc_list)\n",
    "        np.save(Idea_Ours_Dir + '/Performance_Analysis/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                    + '/acc.npy', acc_array)\n",
    "\n",
    "        logger.info('Save Models')\n",
    "        for participant_index in range(N_Participants):\n",
    "            netname = Private_Net_Name_List[participant_index]\n",
    "            private_dataset_name = Private_Dataset_Name_List[participant_index]\n",
    "            network = net_list[participant_index]\n",
    "            #network = nn.DataParallel(network, device_ids=device_ids).to(device)\n",
    "            network = network.to(device)\n",
    "            torch.save(network.state_dict(),\n",
    "                           Idea_Ours_Dir + '/Model_Storage/' + Scenario + '/' + Ablation_Name + '/' + Public_Dataset_Name\n",
    "                           + '/' + netname + '_' + str(participant_index) + '_' + private_dataset_name + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127f039-b9d5-4727-b224-8273c9fb3749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e6b5f-d256-4e33-ac35-31a3109500bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
