{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63570d4a-0e1a-4a96-8607-36f3dec8a7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%load_ext cudf.pandas\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "import cudf as pd\n",
    "import shap\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, ConcatDataset\n",
    "import torchaudio\n",
    "\n",
    "import random\n",
    "import ast\n",
    "        \n",
    "import sys\n",
    "\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "from torch.optim import Adam ,SGD ,Adadelta\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "import optuna\n",
    "\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation ,FFMpegWriter ,PillowWriter\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os \n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "import wandb\n",
    "\n",
    "\n",
    "import random\n",
    "import csv \n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import traceback\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA magic\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "# ML tools\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from gtda.mapper.filter import Projection,Entropy,Eccentricity\n",
    "from gtda.mapper.cover import CubicalCover\n",
    "# scikit-learn method\n",
    "# giotto-tda method\n",
    "from gtda.mapper.cluster import FirstSimpleGap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='threadpoolctl')\n",
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "# Redirect stderr to null to suppress the exception messages\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "import ctypes\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress low-level warnings from C code\n",
    "libc = ctypes.CDLL(None)\n",
    "libc.prctl(15, 0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c4bc32-f5d1-477e-8694-beb1bb56fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from torcheval.metrics.functional import multiclass_confusion_matrix\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in,\n",
    "        nlin=\"leakyrelu\",\n",
    "        dropout=0.0,\n",
    "        init_type=\"uniform\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init module list\n",
    "        self.module_list = nn.ModuleList()\n",
    "        ### ASSUMES 28x28 image size\n",
    "        ## compose layer 1\n",
    "        self.module_list.append(nn.Conv2d(channels_in, 8, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        # apply dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 2\n",
    "        self.module_list.append(nn.Conv2d(8, 6, 5))\n",
    "        self.module_list.append(nn.MaxPool2d(2, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## compose layer 3\n",
    "        self.module_list.append(nn.Conv2d(6, 4, 2))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add flatten layer\n",
    "        self.module_list.append(nn.Flatten())\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(3 * 3 * 4, 20))\n",
    "        self.module_list.append(self.get_nonlin(nlin))\n",
    "        ## add dropout\n",
    "        if dropout > 0:\n",
    "            self.module_list.append(nn.Dropout(dropout))\n",
    "        ## add linear layer 1\n",
    "        self.module_list.append(nn.Linear(20, 10))\n",
    "\n",
    "        ### initialize weights with se methods\n",
    "        self.initialize_weights(init_type)\n",
    "\n",
    "    def initialize_weights(self, init_type):\n",
    "        # print(\"initialze model\")\n",
    "        for m in self.module_list:\n",
    "            if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "                if init_type == \"xavier_uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if init_type == \"xavier_normal\":\n",
    "                    torch.nn.init.xavier_normal_(m.weight)\n",
    "                if init_type == \"uniform\":\n",
    "                    torch.nn.init.uniform_(m.weight)\n",
    "                if init_type == \"normal\":\n",
    "                    torch.nn.init.normal_(m.weight)\n",
    "                if init_type == \"kaiming_normal\":\n",
    "                    torch.nn.init.kaiming_normal_(m.weight)\n",
    "                if init_type == \"kaiming_uniform\":\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight)\n",
    "                # set bias to some small non-zero value\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "    def get_nonlin(self, nlin):\n",
    "        # apply nonlinearity\n",
    "        if nlin == \"leakyrelu\":\n",
    "            return nn.LeakyReLU()\n",
    "        if nlin == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        if nlin == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        if nlin == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        if nlin == \"silu\":\n",
    "            return nn.SiLU()\n",
    "        if nlin == \"gelu\":\n",
    "            return nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward prop through module_list\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward_activations(self, x):\n",
    "        # forward prop through module_list\n",
    "        activations = []\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "            if (\n",
    "                isinstance(layer, nn.Tanh)\n",
    "                or isinstance(layer, nn.Sigmoid)\n",
    "                or isinstance(layer, nn.ReLU)\n",
    "                or isinstance(layer, nn.LeakyReLU)\n",
    "                or isinstance(layer, nn.SiLU)\n",
    "                or isinstance(layer, nn.GELU)\n",
    "                or isinstance(layer, ORU)\n",
    "                or isinstance(layer, ERU)\n",
    "            ):\n",
    "                activations.append(x)\n",
    "        return x, activations\n",
    "def train(model, trainloader, optimizer, criterion,nb_classes,First=False,df=None,verbose=False,log_freq_steps=25):\n",
    "    List_mx=[]\n",
    "    #model=torch.compile(model)\n",
    "    model.train()\n",
    "    #print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image\n",
    "        labels = labels\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        #mx=multiclass_confusion_matrix(preds ,labels,nb_classes,normalize=\"pred\")\n",
    "        #List_mx.append(mx)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "        if First==True and i%log_freq_steps==0 :\n",
    "            epoch_loss = train_running_loss / counter\n",
    "            epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "            if verbose==True:\n",
    "                print(epoch_acc,\"%\")\n",
    "            #print(f\"step {i}:\",epoch_loss, epoch_acc)\n",
    "            if type(df)!='NoneType':\n",
    "                df.at[track,f\"Step {i}\"]=epoch_acc\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "# def compute_shap_values(model, images, baseline):\n",
    "#     shap.KernelExplainer(model(images).detach().numpy(), images.detach().numpy(), link=\"logit\")\n",
    "\n",
    "    \n",
    "#         explainer = shap.KernelExplainer(f, np.reshape(reference, (1, len(reference))))\n",
    "#         shap_values = explainer.shap_values(x)\n",
    "#         print(\"shap_values =\", shap_values)\n",
    "#     return shap_values\n",
    "\n",
    "def validate(model, testloader, criterion, nb_classes,epoch_nb):\n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    correct_counts = {str(i): 0 for i in range(nb_classes)}\n",
    "    total_counts = {str(i): 0 for i in range(nb_classes)}\n",
    "    #shapley_values_per_class = {str(i): [] for i in range(nb_classes)}\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            if data is None:\n",
    "                continue\n",
    "            counter += 1\n",
    "            image, labels = data\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "            # Update correct/total counts\n",
    "            for j in range(len(labels)):\n",
    "                class_label = str(labels[j].item())\n",
    "                total_counts[class_label] += 1\n",
    "                if preds[j] == labels[j]:\n",
    "                    correct_counts[class_label] += 1\n",
    "            \n",
    "            # Compute Shapley values for a batch\n",
    "            #baseline = torch.zeros_like(image)  # Use black image as baseline\n",
    "            #image = image.requires_grad_(True)\n",
    "            #print(image,image.shape)\n",
    "            #baseline = baseline.requires_grad_(True)\n",
    "            #shap_values = compute_shap_values(model, image, baseline)\n",
    "\n",
    "            \n",
    "            \n",
    "            for j in range(len(labels)):\n",
    "                class_label = str(labels[j].item())\n",
    "                #shapley_values_per_class[class_label].append(np.mean(shap_values[j].numpy()))\n",
    "    \n",
    "    # Compute final metrics\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "\n",
    "    QQQ = pd.DataFrame({\n",
    "    \"Label\": f\"epoch {epoch_nb}\",\n",
    "    \"Correct pred\": [correct_counts[k] for k in correct_counts],\n",
    "    \"Target Label\": [total_counts[k] for k in total_counts],\n",
    "    })\n",
    "    \n",
    "    # Add ratio column\n",
    "    QQQ[\"Ratio\"] = QQQ[\"Correct pred\"] / QQQ[\"Target Label\"]\n",
    "    test_table1 = wandb.Table(data=QQQ, columns=list(QQQ.columns))\n",
    "    \n",
    "    run.log({f\"Wholistic acc: {epoch_nb}\":test_table1})\n",
    "\n",
    "    SSS = pd.DataFrame({\n",
    "    \"Class\": [str(h) for h in range(10)],\n",
    "    \"Predicted\": [correct_counts[k] for k in correct_counts],\n",
    "    \"Target\": [total_counts[k] for k in total_counts]})\n",
    "\n",
    "    # Add a column for the prediction ratio\n",
    "    SSS[\"Pred/Target\"] = SSS.apply(lambda row: row[\"Predicted\"] / row[\"Target\"] if row[\"Target\"] != 0 else None, axis=1)\n",
    "        \n",
    "    SSS = SSS.set_index(\"Class\")\n",
    "\n",
    "    test_table2 = wandb.Table(data=SSS, columns=list(SSS.columns))\n",
    "    run.log({f\"per-class acc:({epoch_nb} \":test_table2})\n",
    "    # Prepare data for wandb histograms\n",
    "    # correct_data = [[correct_counts[str(i)]] for i in range(nb_classes)]\n",
    "    # total_data = [[total_counts[str(i)]] for i in range(nb_classes)]\n",
    "    \n",
    "    # correct_table = wandb.Table(data=correct_data, columns=[\"Correct Predictions\"])\n",
    "    # total_table = wandb.Table(data=total_data, columns=[\"Total Instances\"])\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "        \"Epoch Loss\": epoch_loss,\n",
    "        \"Epoch Accuracy\": epoch_acc})\n",
    "    \n",
    "    # wandb.log({\n",
    "    #     \"Prediction Histogram\": wandb.plot.histogram(correct_table, \"Correct Predictions\", title=\"Correct Predictions Per Class\"),\n",
    "    #     \"Total Histogram\": wandb.plot.histogram(total_table, \"Total Instances\", title=\"Total Instances Per Class\")\n",
    "    # })\n",
    "    \n",
    "    # Log Shapley value histograms\n",
    "    # for class_label, shap_vals in shapley_values_per_class.items():\n",
    "    #     if shap_vals:\n",
    "    #         shap_table = wandb.Table(data=[[val] for val in shap_vals], columns=[\"Shapley Values\"])\n",
    "    #         wandb.log({f\"Shapley Values - Class {class_label}\": wandb.plot.histogram(shap_table, \"Shapley Values\", title=f\"Shapley Values for Class {class_label}\")})\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "def create_frame(step,ax,data):\n",
    "    ax=ax.cla()\n",
    "    sns.heatmap(data[step][-1].cpu(),annot=True,cmap=\"cubehelix\",ax=ax,cbar=False)\n",
    "    plt.title('Epoch {} training {}'.format(step,exp)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc15b34-b20c-447c-84ca-4198c0e02802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"ab631efc36e2c87f5f54d82b5cdbd6c501d5221f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48509ae6-3aa7-437b-9d2a-90e356518fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "class ClassSpecificImageFolder(datasets.DatasetFolder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root,\n",
    "            dropped_classes=[],\n",
    "            transform = None,\n",
    "            target_transform = None,\n",
    "            loader = datasets.folder.default_loader,\n",
    "            is_valid_file = None,\n",
    "    ):\n",
    "        self.dropped_classes = dropped_classes\n",
    "        super(ClassSpecificImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                                       transform=transform,\n",
    "                                                       target_transform=target_transform,\n",
    "                                                       is_valid_file=is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def find_classes(self, directory):\n",
    "        classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "        classes = [c for c in classes if c not in self.dropped_classes]\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5364c094-fd6e-40a8-b248-4bbf387ec07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Federated-Continual-learning-/New/wandb/run-20250425_105127-vnzynvws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aymentlili/utils-project/runs/vnzynvws' target=\"_blank\">Histogram plot</a></strong> to <a href='https://wandb.ai/aymentlili/utils-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aymentlili/utils-project' target=\"_blank\">https://wandb.ai/aymentlili/utils-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aymentlili/utils-project/runs/vnzynvws' target=\"_blank\">https://wandb.ai/aymentlili/utils-project/runs/vnzynvws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Brain = CNN(1,\"silu\",0,\"kaiming_uniform\")\n",
    "model=copy.deepcopy(Brain)\n",
    "\n",
    "run = wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"utils-project\",\n",
    "            name= \"Histogram plot\" ,\n",
    "            # Track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"batch_size\":36\n",
    "            },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a3d4a-9dea-436e-9af5-7c896c6be9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed cnn acc ID 14.427520109532773\n",
      "Reconstructed cnn acc OOD 11.360709686650452\n"
     ]
    }
   ],
   "source": [
    "criterion_CNN0=CrossEntropyLoss()\n",
    "DF=pd.DataFrame()\n",
    "track=1\n",
    "L2=[0,1,2,3]\n",
    "task3=[7,8,9]\n",
    "test_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL0 = DataLoader(dataset=test_IF0, batch_size=36, num_workers=0, shuffle=False)\n",
    "\n",
    "_, valid_epoch_acc0= validate(model, Ts_DL0,  criterion_CNN0,10,-1)\n",
    "\n",
    "criterion_CNN1=CrossEntropyLoss()\n",
    "test_IF1=ClassSpecificImageFolder( root=\"./data/SplitMnist/test/\",dropped_classes=[str(x) for x in task3],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Ts_DL1 = DataLoader(dataset=test_IF1, batch_size=36, num_workers=0, shuffle=False)\n",
    "\n",
    "valid_epoch_loss0, valid_epoch_acc1= validate(model, Ts_DL1,  criterion_CNN1,10,-1)\n",
    "print(\"Reconstructed cnn acc ID\",valid_epoch_acc0)\n",
    "print(\"Reconstructed cnn acc OOD\",valid_epoch_acc1)\n",
    "\n",
    "optimizerCNN = Adam(model.parameters(), lr=0.05)\n",
    "schedulerCNN = torch.optim.lr_scheduler.CyclicLR(optimizerCNN ,base_lr=1e-3, max_lr=0.1, step_size_up=400, mode=\"triangular2\", cycle_momentum=False)\n",
    "criterion_CNN=CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_IF0=ClassSpecificImageFolder( root=\"./data/SplitMnist/train/\",dropped_classes=[str(x) for x in L2],transform=transforms.Compose([ transforms.ToTensor(),transforms.Grayscale(1)]))\n",
    "Tr_DLr = DataLoader(dataset=train_IF0, batch_size=36, num_workers=0, shuffle=True)\n",
    "\n",
    "\n",
    "fine_tune_needed=0\n",
    "epoch_n=50\n",
    "#FINETUNING\n",
    "for epoch_cnn in range(epoch_n):\n",
    "    if epoch_cnn==0:\n",
    "        train_epoch_loss, train_epoch_acc = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF,First=True)\n",
    "        valid_epoch_loss0FN, valid_epoch_acc0FN= validate(model, Ts_DL0,  criterion_CNN,10,epoch_cnn)\n",
    "    else:\n",
    "        train_epoch_loss, train_epoch_acc = train(model, Tr_DLr, optimizerCNN, criterion_CNN,10,df=DF)\n",
    "        valid_epoch_loss0FN, valid_epoch_acc0FN= validate(model, Ts_DL0,  criterion_CNN,10,epoch_cnn)\n",
    "        \n",
    "    schedulerCNN.step()\n",
    "    fine_tune_needed+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec79ca-ae60-4c7d-a7ae-17e248815e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b2a0b-ddf4-4805-a705-fd08213412f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07bf72-289a-4e92-bff7-aff1b4d5e280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
