{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952f3db9-b782-4405-b357-b26d8afdb3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: torchview in /home/crns/anaconda3/lib/python3.9/site-packages (0.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a663b57-c4c6-40c9-83e3-dfd0ca1eb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# # Transformer Shared Layers\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80, device=\"cuda:0\"):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device =torch.device('cuda:0')\n",
    "\n",
    "        # create constant 'pe' matrix with values dependant on\n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len], requires_grad=False).to(self.device)\n",
    "        return x\n",
    "\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "\n",
    "    return output, scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000e016e-ab0e-444b-aadf-e7eb5a3a8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = (\n",
    "            self.alpha\n",
    "            * (x - x.mean(dim=-1, keepdim=True))\n",
    "            / (x.std(dim=-1, keepdim=True) + self.eps)\n",
    "            + self.bias\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores, sc = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output, sc\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, normalize=True, dropout=0.1, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.normalize = normalize\n",
    "        if normalize:\n",
    "            self.norm_1 = Norm(d_model)\n",
    "            self.norm_2 = Norm(d_model)\n",
    "        dropout=0.1\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=0.1)\n",
    "        self.ff = FeedForward(d_model, d_ff=d_ff, dropout=0.1)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        if self.normalize:\n",
    "            x2 = self.norm_1(x)\n",
    "        else:\n",
    "            x2 = x.clone()\n",
    "        res, sc = self.attn(x2, x2, x2, mask)\n",
    "        # x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x = x + self.dropout_1(res)\n",
    "        if self.normalize:\n",
    "            x2 = self.norm_2(x)\n",
    "        else:\n",
    "            x2 = x.clone()\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        # return x\n",
    "        return x, sc\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, d_model, N, heads, max_seq_len, dropout, d_ff, device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.embed = Embedder(input_dim, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, max_seq_len, device)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout, d_ff), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        scores = []\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x, sc = self.layers[i](x, mask)\n",
    "            scores.append(sc)\n",
    "        return self.norm(x), scores\n",
    "\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, seed=22):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed = nn.Linear(1, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = []\n",
    "        # use the same embedder to embedd all weights\n",
    "        for idx in range(self.input_dim):\n",
    "            # embedd single input / feature dimension\n",
    "            tmp = self.embed(x[:, idx].unsqueeze(dim=1))\n",
    "            y.append(tmp)\n",
    "        # stack along dimension 1\n",
    "        y = torch.stack(y, dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Debedder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, seed=22):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.weight_debedder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.weight_debedder(x)\n",
    "        y = y.squeeze()\n",
    "        return y\n",
    "\n",
    "\n",
    "# # Tranformer Encoder\n",
    "class EmbedderNeuron(nn.Module):\n",
    "    # collects all weights connected to one neuron / kernel\n",
    "    def __init__(self, index_dict, d_model, seed=22):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_lst = nn.ModuleList()\n",
    "        self.index_dict = index_dict\n",
    "        self.get_kernel_slices()\n",
    "\n",
    "        for idx, kernel_lst in enumerate(self.slice_lst):\n",
    "            i_dim = len(kernel_lst[0])\n",
    "            # check sanity of slices\n",
    "            for slice in kernel_lst:\n",
    "                assert (\n",
    "                    len(slice) == i_dim\n",
    "                ), f\"layer-wise slices are not of the same lenght: {i_dim} vs {len(slice)}\"\n",
    "            # get layers\n",
    "            self.layer_lst.append(nn.Linear(i_dim, d_model))\n",
    "            # print(f\"layer {layer} - nn.Linear({i_dim},embed_dim)\")\n",
    "\n",
    "    def get_kernel_slices(\n",
    "        self,\n",
    "    ):\n",
    "        slice_lst = []\n",
    "        # loop over layers\n",
    "        for idx, layer in enumerate(self.index_dict[\"layer\"]):\n",
    "            # print(f\"### layer {layer} ###\")\n",
    "            kernel_slice_lst = []\n",
    "            for kernel_dx in range(self.index_dict[\"kernel_no\"][idx]):\n",
    "                # get current kernel index\n",
    "                kernel_start = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + kernel_dx\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                kernel_end = (\n",
    "                    kernel_start\n",
    "                    + self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                bias = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + self.index_dict[\"kernel_no\"][idx]\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                    + kernel_dx\n",
    "                )\n",
    "                index_kernel = list(range(kernel_start, kernel_end))\n",
    "                index_kernel.append(bias)\n",
    "\n",
    "                # get next_layers connected weights\n",
    "                if idx < len(self.index_dict[\"layer\"]) - 1:\n",
    "                    # -> find corresponding indices\n",
    "                    # -> get offset to beginning of next layer\n",
    "                    for kernel_dx_next in range(self.index_dict[\"kernel_no\"][idx + 1]):\n",
    "                        kernel_next_start = (\n",
    "                            # get start of next layer\n",
    "                            self.index_dict[\"idx_start\"][idx + 1]\n",
    "                            # offset by current kernel*dim of kernel_size (columns)\n",
    "                            + kernel_dx * self.index_dict[\"kernel_size\"][idx + 1]\n",
    "                            # offset by rows: overall parameters per channel out\n",
    "                            + kernel_dx_next\n",
    "                            * self.index_dict[\"channels_in\"][idx + 1]\n",
    "                            * self.index_dict[\"kernel_size\"][idx + 1]\n",
    "                        )\n",
    "                        kernel_next_end = (\n",
    "                            kernel_next_start + self.index_dict[\"kernel_size\"][idx + 1]\n",
    "                        )\n",
    "\n",
    "                        # extend\n",
    "                        kernel_next_idx = list(\n",
    "                            range(kernel_next_start, kernel_next_end)\n",
    "                        )\n",
    "                        index_kernel.extend(kernel_next_idx)\n",
    "\n",
    "                kernel_slice_lst.append(index_kernel)\n",
    "                # print(index_kernel)\n",
    "            slice_lst.append(kernel_slice_lst)\n",
    "        self.slice_lst = slice_lst\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        counter = 0\n",
    "        for layer_embeddings in self.slice_lst:\n",
    "            counter += len(layer_embeddings)\n",
    "        return counter\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_lst = []\n",
    "        # loop over layers\n",
    "        for idx, kernel_slice_lst in enumerate(self.slice_lst):\n",
    "            # loop over kernels in layer\n",
    "            for kdx, kernel_index in enumerate(kernel_slice_lst):\n",
    "                # print(index_kernel)\n",
    "                y_tmp = self.layer_lst[idx](x[:, kernel_index])\n",
    "                y_lst.append(y_tmp)\n",
    "        y = torch.stack(y_lst, dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DebedderNeuron(nn.Module):\n",
    "    def __init__(self, index_dict, d_model, seed=22, layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_lst = nn.ModuleList()\n",
    "        self.index_dict = index_dict\n",
    "        self.get_kernel_slices()\n",
    "\n",
    "        for idx, kernel_lst in enumerate(self.slice_lst):\n",
    "            i_dim = len(kernel_lst[0])\n",
    "            # check sanity of slices\n",
    "            for slice in kernel_lst:\n",
    "                assert (\n",
    "                    len(slice) == i_dim\n",
    "                ), f\"layer-wise slices are not of the same lenght: {i_dim} vs {len(slice)}\"\n",
    "            # get layers\n",
    "            if layers == 1:\n",
    "                self.layer_lst.append(nn.Linear(d_model, i_dim))\n",
    "            else:\n",
    "                from model_definitions.def_net import MLP\n",
    "\n",
    "                layertmp = MLP(\n",
    "                    i_dim=d_model,\n",
    "                    h_dim=[d_model for _ in range(layers - 2)],\n",
    "                    o_dim=i_dim,\n",
    "                    nlin=\"leakyrelu\",\n",
    "                    dropout=dropout,\n",
    "                    init_type=\"kaiming_normal\",\n",
    "                    use_bias=True,\n",
    "                )\n",
    "                self.layer_lst.append(layertmp)\n",
    "            # print(f\"layer {layer} - nn.Linear({i_dim},embed_dim)\")\n",
    "            # self.layer_lst.append(nn.Linear(d_model, i_dim))\n",
    "\n",
    "    def get_kernel_slices(\n",
    "        self,\n",
    "    ):\n",
    "        slice_lst = []\n",
    "        # loop over layers\n",
    "        for idx, layer in enumerate(self.index_dict[\"layer\"]):\n",
    "            # print(f\"### layer {layer} ###\")\n",
    "            kernel_slice_lst = []\n",
    "            for kernel_dx in range(self.index_dict[\"kernel_no\"][idx]):\n",
    "                # get current kernel index\n",
    "                kernel_start = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + kernel_dx\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                kernel_end = (\n",
    "                    kernel_start\n",
    "                    + self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                bias = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + self.index_dict[\"kernel_no\"][idx]\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                    + kernel_dx\n",
    "                )\n",
    "                index_kernel = list(range(kernel_start, kernel_end))\n",
    "                index_kernel.append(bias)\n",
    "\n",
    "                # get next_layers connected weights\n",
    "                if idx < len(self.index_dict[\"layer\"]) - 1:\n",
    "                    # -> find corresponding indices\n",
    "                    # -> get offset to beginning of next layer\n",
    "                    for kernel_dx_next in range(self.index_dict[\"kernel_no\"][idx + 1]):\n",
    "                        kernel_next_start = (\n",
    "                            # get start of next layer\n",
    "                            self.index_dict[\"idx_start\"][idx + 1]\n",
    "                            # offset by current kernel*dim of kernel_size (columns)\n",
    "                            + kernel_dx * self.index_dict[\"kernel_size\"][idx + 1]\n",
    "                            # offset by rows: overall parameters per channel out\n",
    "                            + kernel_dx_next\n",
    "                            * self.index_dict[\"channels_in\"][idx + 1]\n",
    "                            * self.index_dict[\"kernel_size\"][idx + 1]\n",
    "                        )\n",
    "                        kernel_next_end = (\n",
    "                            kernel_next_start + self.index_dict[\"kernel_size\"][idx + 1]\n",
    "                        )\n",
    "\n",
    "                        # extend\n",
    "                        kernel_next_idx = list(\n",
    "                            range(kernel_next_start, kernel_next_end)\n",
    "                        )\n",
    "                        index_kernel.extend(kernel_next_idx)\n",
    "\n",
    "                kernel_slice_lst.append(index_kernel)\n",
    "                # print(index_kernel)\n",
    "            slice_lst.append(kernel_slice_lst)\n",
    "        self.slice_lst = slice_lst\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        counter = 0\n",
    "        for layer_embeddings in self.slice_lst:\n",
    "            counter += len(layer_embeddings)\n",
    "        return counter\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        # get last value of last layer last kernel last index - zero based -> +1\n",
    "        i_dim = self.slice_lst[-1][-1][-1] + 1\n",
    "        y = torch.zeros((x.shape[0], i_dim)).to(device)\n",
    "\n",
    "        # loop over layers\n",
    "        embed_dx = 0\n",
    "        for idx, kernel_slice_lst in enumerate(self.slice_lst):\n",
    "            # loop over kernels in layer\n",
    "            for kdx, kernel_index in enumerate(kernel_slice_lst):\n",
    "                # print(index_kernel)\n",
    "                # get values for this embedding\n",
    "                y_tmp = self.layer_lst[idx](x[:, embed_dx])\n",
    "                # !!add!! values in right places\n",
    "                y[:, kernel_index] += y_tmp\n",
    "                # raise counter\n",
    "                embed_dx += 1\n",
    "\n",
    "        # first layer and last layer get only embedded once,\n",
    "        # while all middle layers overlap.\n",
    "        # -> get index list for beginning of second and ending of second to last layer\n",
    "        # -> devide embedded values by 2\n",
    "        if len(self.index_dict[\"idx_start\"]) > 2:\n",
    "            index_start = self.index_dict[\"idx_start\"][1]\n",
    "            index_end = self.index_dict[\"idx_start\"][-1]\n",
    "            idx = list(range(index_start, index_end))\n",
    "            # create tensor of same shape with 0.5 values put it on device\n",
    "            factor = torch.ones(y[:, idx].shape) * 0.5\n",
    "            factor = factor.to(y.device)\n",
    "            # multiply with 0.5\n",
    "            y[:, idx] = y[:, idx] * factor\n",
    "        return y\n",
    "\n",
    "\n",
    "# # Tranformer Encoder\n",
    "class EmbedderNeuronGroup_index(nn.Module):\n",
    "    def __init__(self, index_dict, d_model, seed=22, split_kernels_threshold=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_lst = nn.ModuleList()\n",
    "        self.index_dict = index_dict\n",
    "\n",
    "        self.split_kernels_threshold = split_kernels_threshold\n",
    "\n",
    "        for idx, layer in enumerate(index_dict[\"layer\"]):\n",
    "            i_dim = index_dict[\"kernel_size\"][idx] * index_dict[\"channels_in\"][idx] + 1\n",
    "            if (self.split_kernels_threshold != 0) and (\n",
    "                i_dim > self.split_kernels_threshold\n",
    "            ):\n",
    "                i_dim = self.split_kernels_threshold\n",
    "            self.layer_lst.append(nn.Linear(i_dim, d_model))\n",
    "            print(f\"layer {layer} - nn.Linear({i_dim},embed_dim)\")\n",
    "\n",
    "        self.get_kernel_slices()\n",
    "\n",
    "    def get_kernel_slices(\n",
    "        self,\n",
    "    ):\n",
    "        slice_lst = []\n",
    "        # loop over layers\n",
    "        for idx, layer in enumerate(self.index_dict[\"layer\"]):\n",
    "            # print(f\"### layer {layer} ###\")\n",
    "            kernel_slice_lst = []\n",
    "            for kernel_dx in range(self.index_dict[\"kernel_no\"][idx]):\n",
    "                kernel_start = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + kernel_dx\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                kernel_end = (\n",
    "                    kernel_start\n",
    "                    + self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                bias = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + self.index_dict[\"kernel_no\"][idx]\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                    + kernel_dx\n",
    "                )\n",
    "                index_kernel = list(range(kernel_start, kernel_end))\n",
    "                index_kernel.append(bias)\n",
    "\n",
    "                kernel_slice_lst.append(index_kernel)\n",
    "                print(index_kernel)\n",
    "            slice_lst.append(kernel_slice_lst)\n",
    "        self.slice_lst = slice_lst\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        counter = 0\n",
    "        for layer_embeddings in self.slice_lst:\n",
    "            counter += len(layer_embeddings)\n",
    "        return counter\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_lst = []\n",
    "        # loop over layers\n",
    "        for idx, kernel_slice_lst in enumerate(self.slice_lst):\n",
    "            # loop over kernels in layer\n",
    "            for kdx, kernel_index in enumerate(kernel_slice_lst):\n",
    "                print(index_kernel)\n",
    "                if (self.split_kernels_threshold != 0) and (\n",
    "                    len(kernel_index) > self.split_kernels_threshold\n",
    "                ):\n",
    "                    from math import ceil\n",
    "\n",
    "                    no_tokens = ceil(len(kernel_index) / self.split_kernels_threshold)\n",
    "                    for idx in range(no_tokens):\n",
    "                        idx_token_start = idx * self.split_kernels_threshold\n",
    "                        idx_token_end = idx_token_start + self.split_kernels_threshold\n",
    "                        kernel_tmp = kernel_index[idx_token_start:idx_token_end]\n",
    "                        if idx == no_tokens - 1:  # last\n",
    "                            x_tmp = torch.zeros(\n",
    "                                size=[x.shape[0], self.split_kernels_threshold]\n",
    "                            )  # pad\n",
    "                            x_tmp[:, : len(kernel_index)] = x[:, kernel_tmp]\n",
    "                        else:\n",
    "                            x_tmp = x[:, kernel_tmp]\n",
    "                        y_tmp = self.layer_lst[idx](x_tmp)\n",
    "                        y_lst.append(y_tmp)\n",
    "                else:\n",
    "                    y_tmp = self.layer_lst[idx](x[:, kernel_index])\n",
    "                    y_lst.append(y_tmp)\n",
    "        y = torch.stack(y_lst, dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DebedderNeuronGroup_index(nn.Module):\n",
    "    def __init__(self, index_dict, d_model, seed=22, layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_lst = nn.ModuleList()\n",
    "        self.index_dict = index_dict\n",
    "\n",
    "        for idx, layer in enumerate(index_dict[\"layer\"]):\n",
    "            i_dim = index_dict[\"kernel_size\"][idx] * index_dict[\"channels_in\"][idx] + 1\n",
    "            # get layers\n",
    "            if layers == 1:\n",
    "                self.layer_lst.append(nn.Linear(d_model, i_dim))\n",
    "            else:\n",
    "                from model_definitions.def_net import MLP\n",
    "\n",
    "                layertmp = MLP(\n",
    "                    i_dim=d_model,\n",
    "                    h_dim=[d_model for _ in range(layers - 2)],\n",
    "                    o_dim=i_dim,\n",
    "                    nlin=\"leakyrelu\",\n",
    "                    dropout=dropout,\n",
    "                    init_type=\"kaiming_normal\",\n",
    "                    use_bias=True,\n",
    "                )\n",
    "                self.layer_lst.append(layertmp)\n",
    "\n",
    "            # print(f\"layer {layer} - nn.Linear({i_dim},embed_dim)\")\n",
    "\n",
    "        self.get_kernel_slices()\n",
    "\n",
    "    def get_kernel_slices(\n",
    "        self,\n",
    "    ):\n",
    "        slice_lst = []\n",
    "        # loop over layers\n",
    "        for idx, layer in enumerate(self.index_dict[\"layer\"]):\n",
    "            # print(f\"### layer {layer} ###\")\n",
    "            kernel_slice_lst = []\n",
    "            for kernel_dx in range(self.index_dict[\"kernel_no\"][idx]):\n",
    "                kernel_start = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + kernel_dx\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                kernel_end = (\n",
    "                    kernel_start\n",
    "                    + self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                )\n",
    "                bias = (\n",
    "                    self.index_dict[\"idx_start\"][idx]\n",
    "                    + self.index_dict[\"kernel_no\"][idx]\n",
    "                    * self.index_dict[\"kernel_size\"][idx]\n",
    "                    * self.index_dict[\"channels_in\"][idx]\n",
    "                    + kernel_dx\n",
    "                )\n",
    "                index_kernel = list(range(kernel_start, kernel_end))\n",
    "                index_kernel.append(bias)\n",
    "\n",
    "                kernel_slice_lst.append(index_kernel)\n",
    "                # print(index_kernel)\n",
    "            slice_lst.append(kernel_slice_lst)\n",
    "        self.slice_lst = slice_lst\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        counter = 0\n",
    "        for layer_embeddings in self.slice_lst:\n",
    "            counter += len(layer_embeddings)\n",
    "        return counter\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        # get last value of last layer last kernel last index - zero based -> +1\n",
    "        i_dim = self.slice_lst[-1][-1][-1] + 1\n",
    "        y = torch.zeros((x.shape[0], i_dim), dtype=dtype).to(device)\n",
    "\n",
    "        # loop over layers\n",
    "        embed_dx = 0\n",
    "        for idx, kernel_slice_lst in enumerate(self.slice_lst):\n",
    "            # loop over kernels in layer\n",
    "            for kdx, kernel_index in enumerate(kernel_slice_lst):\n",
    "                # print(index_kernel)\n",
    "                # get values for this embedding\n",
    "                y_tmp = self.layer_lst[idx](x[:, embed_dx])\n",
    "                # match data types for mixed precision\n",
    "                if not y_tmp.dtype == y.dtype:\n",
    "                    y = y.to(y_tmp.dtype)\n",
    "                # put values in right places\n",
    "                y[:, kernel_index] = y_tmp\n",
    "                # raise counter\n",
    "                embed_dx += 1\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class EmbedderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, seed=22):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neuron_l1 = nn.Linear(16, d_model)\n",
    "        self.neuron_l2 = nn.Linear(5, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        # Hardcoded position for easy-fast integration\n",
    "        l = []\n",
    "        # l1\n",
    "        for ndx in range(5):\n",
    "            idx_start = ndx * 16\n",
    "            idx_end = idx_start + 16\n",
    "            l.append(self.neuron_l1(v[:, idx_start:idx_end]))\n",
    "        # l2\n",
    "        for ndx in range(4):\n",
    "            idx_start = 5 * 16 + ndx * 5\n",
    "            idx_end = idx_start + 5\n",
    "            l.append(self.neuron_l2(v[:, idx_start:idx_end]))\n",
    "\n",
    "        final = torch.stack(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n",
    "\n",
    "class EncoderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, N, heads, max_seq_len, dropout, d_ff):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = EmbedderNeuronGroup(d_model)\n",
    "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
    "        print(dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout, d_ff), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        scores = []\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x, sc = self.layers[i](x, mask)\n",
    "            scores.append(sc)\n",
    "        #print(\"scores variable shape is:\",scores[0][0].shape,\" norm variable shape is:\",self.norm(x).shape)\n",
    "        #print(scores[0][0])\n",
    "        return self.norm(x), scores\n",
    "\n",
    "\n",
    "class DebedderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neuron_l1 = nn.Linear(d_model, 16)\n",
    "        self.neuron_l2 = nn.Linear(d_model, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        l = []\n",
    "        for ndx in range(5):\n",
    "            l.append(self.neuron_l1(v[:, ndx]))\n",
    "        for ndx in range(5, 9):\n",
    "            l.append(self.neuron_l2(v[:, ndx]))\n",
    "\n",
    "        final = torch.cat(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n",
    "\n",
    "# # Custom Tranformer Decoder\n",
    "\n",
    "\n",
    "class Neck2Seq(nn.Module):\n",
    "    def __init__(self, d_model, neck):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neuron11 = nn.Linear(neck, d_model)\n",
    "        self.neuron12 = nn.Linear(neck, d_model)\n",
    "        self.neuron13 = nn.Linear(neck, d_model)\n",
    "        self.neuron14 = nn.Linear(neck, d_model)\n",
    "        self.neuron15 = nn.Linear(neck, d_model)\n",
    "        self.neuron21 = nn.Linear(neck, d_model)\n",
    "        self.neuron22 = nn.Linear(neck, d_model)\n",
    "        self.neuron23 = nn.Linear(neck, d_model)\n",
    "        self.neuron24 = nn.Linear(neck, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        # print(\"V shape: \", v.shape)\n",
    "        l = []\n",
    "        l.append(self.neuron11(v))\n",
    "        l.append(self.neuron12(v))\n",
    "        l.append(self.neuron13(v))\n",
    "        l.append(self.neuron14(v))\n",
    "        l.append(self.neuron15(v))\n",
    "        l.append(self.neuron21(v))\n",
    "        l.append(self.neuron22(v))\n",
    "        l.append(self.neuron23(v))\n",
    "        l.append(self.neuron24(v))\n",
    "        final = torch.stack(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n",
    "\n",
    "class Seq2Vec(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neuron11 = nn.Linear(d_model, 16)\n",
    "        self.neuron12 = nn.Linear(d_model, 16)\n",
    "        self.neuron13 = nn.Linear(d_model, 16)\n",
    "        self.neuron14 = nn.Linear(d_model, 16)\n",
    "        self.neuron15 = nn.Linear(d_model, 16)\n",
    "        self.neuron21 = nn.Linear(d_model, 5)\n",
    "        self.neuron22 = nn.Linear(d_model, 5)\n",
    "        self.neuron23 = nn.Linear(d_model, 5)\n",
    "        self.neuron24 = nn.Linear(d_model, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multiLinear(x)\n",
    "\n",
    "    def multiLinear(self, v):\n",
    "        l = []\n",
    "        l.append(self.neuron11(v[:, 0]))\n",
    "        l.append(self.neuron12(v[:, 1]))\n",
    "        l.append(self.neuron13(v[:, 2]))\n",
    "        l.append(self.neuron14(v[:, 3]))\n",
    "        l.append(self.neuron15(v[:, 4]))\n",
    "        l.append(self.neuron21(v[:, 5]))\n",
    "        l.append(self.neuron22(v[:, 6]))\n",
    "        l.append(self.neuron23(v[:, 7]))\n",
    "        l.append(self.neuron24(v[:, 8]))\n",
    "        final = torch.cat(l, dim=1)\n",
    "\n",
    "        # print(final.shape)\n",
    "        return final\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a115fd95-cad9-426e-954c-8d38b57198db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNeuronGroup(nn.Module):\n",
    "    def __init__(self, d_model, N, heads, max_seq_len, dropout, d_ff, neck):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Neck2Seq(d_model, neck)\n",
    "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
    "        print(dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout, d_ff), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "        self.lay = Seq2Vec(d_model=d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        scores = []\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x, sc = self.layers[i](x, mask)\n",
    "            scores.append(sc)\n",
    "        return self.lay(self.norm(x)), scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a4ed67-5a65-4ba0-abb1-4368154dd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AutoEncoder\n",
    "\n",
    "\n",
    "class TransformerAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_seq_len=9,\n",
    "        N=1,\n",
    "        heads=1,\n",
    "        d_model=100,\n",
    "        d_ff=100,\n",
    "        neck=20,\n",
    "        dropout=0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.enc = EncoderNeuronGroup(d_model, N, heads, max_seq_len, dropout, d_ff)\n",
    "        self.dec = DecoderNeuronGroup(\n",
    "            d_model, N, heads, max_seq_len, dropout, d_ff, neck\n",
    "        )\n",
    "\n",
    "        # Addition Approach\n",
    "        print(\"Addition Approach!\")\n",
    "        self.vec2neck = nn.Linear(d_ff, neck)\n",
    "\n",
    "        # Stacking Approach\n",
    "        #print(\"Stack Approach!\")\n",
    "        #self.vec2neck = nn.Linear(d_ff * max_seq_len, neck)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Xavier Uniform Initialitzation\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        # First Approach\n",
    "        out, scEnc = self.enc(inp)\n",
    "        print(out[0].shape)\n",
    "\n",
    "        # Addition\n",
    "        neck = self.tanh(self.vec2neck(torch.sum(out, dim=1, keepdim=False)))\n",
    "\n",
    "        # Stacking\n",
    "        #out = out.view(out.shape[0], out.shape[1] * out.shape[2])\n",
    "        #neck = self.tanh(self.vec2neck(out))\n",
    "\n",
    "        out, scDec = self.dec(neck)\n",
    "\n",
    "        return out, neck, scEnc, scDec\n",
    "\n",
    "    def count_parameters(self, model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    def numParams(self):\n",
    "        encNumParams = self.count_parameters(self.enc)\n",
    "        neckNumParams = self.count_parameters(self.vec2neck)\n",
    "        decNumParams = self.count_parameters(self.dec)\n",
    "        modelParams = self.count_parameters(self)\n",
    "\n",
    "        return (\n",
    "            \"EncParams: {}, NeckParams: {}, DecParams: {}, || ModelParams: {} \".format(\n",
    "                encNumParams, neckNumParams, decNumParams, modelParams\n",
    "            )\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9924d1ec-b55b-484a-a8b3-ba3a1a6068d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i,data in enumerate(trainloader):\n",
    "        counter += 1\n",
    "        inp1,inp2,tgt = data\n",
    "        inp1 = inp1.to(device)\n",
    "        inp2 = inp2.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(inp1)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, tgt)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        #_, preds = torch.max(outputs.data, 1)\n",
    "        #train_running_correct += (preds == labels).sum().item()\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    #epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss#, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            counter += 1\n",
    "            inp1,inp2,tgt = data\n",
    "            inp1 = inp1.to(device)\n",
    "            inp2 = inp2.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            # forward pass\n",
    "            outputs = model(inp1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, tgt)\n",
    "            valid_running_loss += loss.item()\n",
    "            # calculate the accuracy\n",
    "            #_, preds = torch.max(outputs.data, 1)\n",
    "            #valid_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    #epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss#, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dc5e99-fb54-499c-8800-5d93eb7bd289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.1\n",
      "Addition Approach!\n",
      "EncParams: 455048, NeckParams: 2020, DecParams: 481748, || ModelParams: 938816 \n",
      "torch.Size([9, 100])\n",
      "Output Shape:  torch.Size([2464, 100])\n"
     ]
    }
   ],
   "source": [
    "#AAA\n",
    "mod = TransformerAE(max_seq_len=2464, \n",
    "                    N=1, \n",
    "                    heads=1, \n",
    "                    d_model=100, \n",
    "                    d_ff=100,\n",
    "                    neck=20, \n",
    "                    dropout=0.1)\n",
    "\n",
    "print(mod.numParams())\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "vec = torch.rand(2464,100)\n",
    "\n",
    "\n",
    "mod=mod.to(device)\n",
    "vec=vec.to(device)\n",
    "out = mod(vec)\n",
    "print(\"Output Shape: \", out[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2859bcd4-c534-4a5f-a511-105ba2f215cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8f1582-de12-44cf-a47e-26fa6eb8c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerAE                                 [2464, 100]               --\n",
       "├─EncoderNeuronGroup: 1-1                     [2464, 9, 100]            --\n",
       "│    └─EmbedderNeuronGroup: 2-1               [2464, 9, 100]            --\n",
       "│    │    └─Linear: 3-1                       [2464, 100]               1,700\n",
       "│    │    └─Linear: 3-2                       [2464, 100]               (recursive)\n",
       "│    │    └─Linear: 3-3                       [2464, 100]               (recursive)\n",
       "│    │    └─Linear: 3-4                       [2464, 100]               (recursive)\n",
       "│    │    └─Linear: 3-5                       [2464, 100]               (recursive)\n",
       "│    │    └─Linear: 3-6                       [2464, 100]               600\n",
       "│    │    └─Linear: 3-7                       [2464, 100]               (recursive)\n",
       "│    │    └─Linear: 3-8                       [2464, 100]               (recursive)\n",
       "│    │    └─Linear: 3-9                       [2464, 100]               (recursive)\n",
       "│    └─PositionalEncoder: 2-2                 [2464, 9, 100]            --\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─EncoderLayer: 3-10                [2464, 9, 100]            452,548\n",
       "│    └─Norm: 2-4                              [2464, 9, 100]            200\n",
       "├─Linear: 1-2                                 [2464, 20]                2,020\n",
       "├─Tanh: 1-3                                   [2464, 20]                --\n",
       "├─DecoderNeuronGroup: 1-4                     [2464, 100]               --\n",
       "│    └─Neck2Seq: 2-5                          [2464, 9, 100]            --\n",
       "│    │    └─Linear: 3-11                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-12                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-13                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-14                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-15                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-16                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-17                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-18                      [2464, 100]               2,100\n",
       "│    │    └─Linear: 3-19                      [2464, 100]               2,100\n",
       "│    └─PositionalEncoder: 2-6                 [2464, 9, 100]            --\n",
       "│    └─ModuleList: 2-7                        --                        --\n",
       "│    │    └─EncoderLayer: 3-20                [2464, 9, 100]            452,548\n",
       "│    └─Norm: 2-8                              [2464, 9, 100]            200\n",
       "│    └─Seq2Vec: 2-9                           [2464, 100]               --\n",
       "│    │    └─Linear: 3-21                      [2464, 16]                1,616\n",
       "│    │    └─Linear: 3-22                      [2464, 16]                1,616\n",
       "│    │    └─Linear: 3-23                      [2464, 16]                1,616\n",
       "│    │    └─Linear: 3-24                      [2464, 16]                1,616\n",
       "│    │    └─Linear: 3-25                      [2464, 16]                1,616\n",
       "│    │    └─Linear: 3-26                      [2464, 5]                 505\n",
       "│    │    └─Linear: 3-27                      [2464, 5]                 505\n",
       "│    │    └─Linear: 3-28                      [2464, 5]                 505\n",
       "│    │    └─Linear: 3-29                      [2464, 5]                 505\n",
       "===============================================================================================\n",
       "Total params: 938,816\n",
       "Trainable params: 938,816\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.33\n",
       "===============================================================================================\n",
       "Input size (MB): 0.99\n",
       "Forward/backward pass size (MB): 1048.36\n",
       "Params size (MB): 3.76\n",
       "Estimated Total Size (MB): 1053.10\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 16\n",
    "summary(mod, input_size=(2464,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07ce498-4967-4da4-954a-3e4257f11c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "d=dict()\n",
    "S_couples=set()\n",
    "with open(r'/home/crns/Documents/GitHub/Federated-Continual-learning-/PFE/Code/NeurIPS_2022-Generative_Hyper_Representations/examples/Results/Leak/Weights_tensors_order Leak.json', 'r') as read_file:\n",
    "    d = json.loads(read_file.read())\n",
    "print(len(d))\n",
    "print(len(S_couples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb876143-c59c-463f-8e1f-6adf431f286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "L_activations=[\"leakyrelu\"]#,\"relu\"]#,,\"gelu\"\"tanh\",\"leakyrelu\",\"\n",
    "L_inits=[\"xavier_uniform\",\"uniform\",\"kaiming_normal\",\"xavier_normal\",\"normal\",\"kaiming_uniform\"]\n",
    "\n",
    "\n",
    "S=combinations(range(10), 5)\n",
    "All=list(range(10))\n",
    "ListExperiences=list()\n",
    "S_couples=set()\n",
    "\n",
    "\n",
    "for i in S :\n",
    "    L1=list(i)\n",
    "    L2=[k for k in All if k not in L1] \n",
    "    L1.extend(L2)\n",
    "    ListExperiences.append(L1)\n",
    "    \n",
    "    \n",
    "All=[\"{}\".format(x) for x in ListExperiences]\n",
    "for model in range(len(d)):\n",
    "    \n",
    "    search_for_comp=model\n",
    "\n",
    "    for comp in range(len(All)):\n",
    "        if d['{}'.format(search_for_comp)][:15] == (All[comp][:14]+\"]\") :\n",
    "            #print(\"Dataset 1:\",d['{}'.format(search_for_comp)][:15],\"index: \",search_for_comp)\n",
    "            #print(\"activation 1:\",d['{}'.format(search_for_comp)][17:].split(\"//\")[0])\n",
    "            #print(\"init 1:\",d['{}'.format(search_for_comp)][17:].split(\"//\")[1])\n",
    "            #print(\"----- -------------- -----\")\n",
    "            D2=All[comp][16:]\n",
    "    #print(\"[\"+D2)\n",
    "    for key,value in d.items() :\n",
    "        if value[:15]==\"[\"+D2:\n",
    "            P=product(range(6),range(6))\n",
    "            for activ,ini in P:\n",
    "                if (d['{}'.format(search_for_comp)][17:].split(\"//\")[1] == d[key][17:].split(\"//\")[1]) and (d['{}'.format(search_for_comp)][17:].split(\"//\")[0] == d[key][17:].split(\"//\")[0]):\n",
    "                    L=[]\n",
    "                    L.append(search_for_comp)\n",
    "                    #print(\"----- possible match -----\")\n",
    "                    #print(\"Dataset 2:\",d['{}'.format(str(int(key)+ini+activ))][:15],\"index: \",str(int(key)+ini+activ))\n",
    "                    #print(\"activation 2:\",d['{}'.format(str(int(key)+ini+activ))][17:].split(\"//\")[0])\n",
    "                    #print(\"init 2:\",d['{}'.format((str(int(key)+ini+activ)))][17:].split(\"//\")[1])\n",
    "                    L.append(int(key))\n",
    "                    \n",
    "                    S_couples.add(frozenset(L))\n",
    "len(S_couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f7006940-4a76-4f6d-91e9-19b394a33293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({389, 1127})\n"
     ]
    }
   ],
   "source": [
    "for x in S_couples:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba2e0757-28e2-4e52-a30c-10b46c573964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[0, 2, 3, 5, 9]//leakyrelu//kaiming_uniform',\n",
       " '[1, 4, 6, 7, 8]//leakyrelu//kaiming_uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['389'],d['1127']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f634f8a-4a14-40ae-8c52-2cf5800c923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Leak=torch.load(\"/home/crns/Documents/GitHub/Federated-Continual-learning-/PFE/Code/NeurIPS_2022-Generative_Hyper_Representations/examples/Results/Leak/model_weights Leak.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b415bb60-e3dc-4343-b0ef-8d5cc8788b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0794, -0.3352,  0.2180,  ..., -0.0210, -0.0284, -0.0220],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " torch.Size([2464]),\n",
       " tensor([ 0.5462, -0.1234,  0.3731,  ..., -0.0342, -0.0377, -0.0528],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " torch.Size([2464]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Leak[389],Leak[389].shape,Leak[1127],Leak[1127].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be8f5b40-d6ce-4d46-8de7-67cf999d7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Stream1=[]\n",
    "L_Stream2=[]\n",
    "for pair in S_couples:\n",
    "    c=0\n",
    "    for index in pair:\n",
    "        if c==0:\n",
    "            L_Stream1.append(Leak[index])\n",
    "            c=c+1\n",
    "        else :\n",
    "            L_Stream2.append(Leak[index])\n",
    "Stream1=torch.stack(L_Stream1)\n",
    "Stream2=torch.stack(L_Stream2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35c5c05b-ef8b-450b-86cb-eeefa65f65d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0794, -0.3352,  0.2180,  ..., -0.0210, -0.0284, -0.0220],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([ 0.5462, -0.1234,  0.3731,  ..., -0.0342, -0.0377, -0.0528],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " 756)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stream1[0],Stream2[0],len(Stream1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ca89180-f7d2-447b-84c4-a58ead72d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target=torch.load(\"/home/crns/Documents/GitHub/Federated-Continual-learning-/PFE/Code/NeurIPS_2022-Generative_Hyper_Representations/examples/Results/Leak/Target model_weights kaiming_normal.pt\", map_location=device)\n",
    "#seeds=[5,13,22,74,1234] the 5 target init seeds , inputs all initialized with 1234 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c84220cc-7508-4dcc-ad9a-1b0e7cd19685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2464]), torch.Size([2464]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target.shape,Target[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5818536c-6ac9-44d2-8bf5-78649716352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target=Target.repeat(151,1)\n",
    "Target=torch.cat([Target,Target[0].view(1,2464)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f33be452-04b5-4d11-8979-1d47c9a639a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([756, 2464]), torch.Size([756, 2464]), torch.Size([756, 2464]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stream2.shape,Stream1.shape,Target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48f0be65-555a-4068-97fb-cac8283aa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(Stream1,Stream2, Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d73768ce-d65c-4d96-9439-0ed7fff7035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsab_cat_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb844629-0f9a-43bb-8396-aa9d3d4e6b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2464]) torch.Size([1, 2464]) torch.Size([1, 2464])\n"
     ]
    }
   ],
   "source": [
    "for x1,x2,tg in dsab_cat_loader:\n",
    "    print(x1.shape,x2.shape,tg.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8c397-acc1-43bb-a049-f961f2e9f9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb069d-61a0-4baf-ad20-310d7d1d4232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d44681-6e0d-42f3-9118-999ce2cbd256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c59387-1ce0-4e78-9cc7-e13599268979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1981fbe-8c9a-4282-8510-f595fd0fb020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b39e9d3-e641-415b-961b-5b9be95a7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_dict={'channels_in': [3, 8, 6, 4, 20],\n",
    " 'idx_length': [608, 1206, 100, 740, 210],\n",
    " 'idx_start': [0, 608, 1814, 1914, 2654],\n",
    " 'kernel_no': [8, 6, 4, 20, 10],\n",
    " 'kernel_size': [25, 25, 4, 9, 1],\n",
    " 'layer': [[0, 'conv2d'],\n",
    "  [4, 'conv2d'],\n",
    "  [8, 'conv2d'],\n",
    "  [13, 'fc'],\n",
    "  [16, 'fc']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "761900c9-856a-4fa7-aabb-e64f848b0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "batchSize = 1000\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(mod.parameters(), lr=1e-3, weight_decay=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "232de652-539e-482e-90fa-638e8a737589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "torch.Size([9, 100])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7748/3901523011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsab_cat_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7748/3821047917.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_running_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# calculate the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3282\u001b[0m             \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m         )\n\u001b[0;32m-> 3284\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m         warnings.warn(\n\u001b[1;32m   3286\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train(mod, dsab_cat_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1f9e7-d443-4cbe-8da2-cfe73775215e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706bd04-2c29-4870-a9c7-0ce1a8bff1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35dfc8-24f5-4914-86a0-59ffd561b1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310a7ae-2de7-4bb0-a07f-345c6a71cd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d92aa2-debb-4d13-91bb-f925a591190c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb7eb1-3818-4882-825b-50ff67126e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbae0e-103c-4fba-8ceb-03e63fa11b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca05a5c-ebb5-410b-976c-6a431f427f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c08e87-4551-47f4-b500-6a63dd7b514f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af365551-78ec-4da4-8011-2914d2733617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c4fc6-58fc-4b80-8da3-e06297b83cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff95ac1-5f32-4ee0-abe3-22ddf1dc647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d1b16-7f0e-4de1-98e2-a7f2450cd430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72389e40-407e-42cd-a219-f038ff17ac6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37105eb2-39c6-4921-a22e-0a5fd53669d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c03258-3651-43cb-a264-0f3ae2904186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c29ad-9f71-49be-9a16-dc1fc389dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "vec=vec.cpu()\n",
    "\n",
    "f, axarr = plt.subplots(2, sharex=True,figsize=(20, 10))\n",
    "sns.heatmap(vec, linewidth=0.5,ax=axarr[0],cmap=\"cubehelix\")\n",
    "sns.heatmap(out[0].cpu().detach().numpy(), linewidth=0.5,ax=axarr[1],cmap=\"cubehelix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
