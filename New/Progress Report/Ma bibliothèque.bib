@online{2022EvgenyFeigin2024,
  title = {2022 Evgeny Feigin — Quiver representations and quiver varieties},
  date = {2024-09-20},
  url = {http://www.youtube.com/playlist?list=PLLGkFbxve670C-0Kle_O5IGCuuJ-lQ60l},
  urldate = {2024-09-20},
  abstract = {Partagez vos vidéos avec vos amis, vos proches et le monde entier},
  langid = {french},
  organization = {YouTube},
  keywords = {Representation Theory}
}

@online{230909442UseKantorovichRubinstein2025,
  title = {[2309.09442] {{On}} the {{Use}} of the {{Kantorovich-Rubinstein Distance}} for {{Dimensionality Reduction}}},
  date = {2025-11-12},
  url = {https://arxiv.org/abs/2309.09442},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\JEXIQNKS\2309.html}
}

@online{6RunningRIVET2025,
  title = {6. {{Running RIVET}} from the {{Console}} — {{RIVET}} 1.0 Documentation},
  date = {2025-11-12},
  url = {https://rivet.readthedocs.io/en/latest/rivetconsole.html},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\AK9MPA6V\rivetconsole.html}
}

@inproceedings{adlamNeuralTangentKernel2020,
  title = {The Neural Tangent Kernel in High Dimensions: {{Triple}} Descent and a Multi-Scale Theory of Generalization},
  shorttitle = {The Neural Tangent Kernel in High Dimensions},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Adlam, Ben and Pennington, Jeffrey},
  date = {2020},
  pages = {74--84},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v119/adlam20a.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\QKRACAQP\Adlam et Pennington - 2020 - The neural tangent kernel in high dimensions Triple descent and a multi-scale theory of generalizat.pdf}
}

@online{AIScientistsMay2025,
  title = {{{AI Scientists May Have Discovered LLMs}}' {{Light-Bulb Moment}} | {{Psychology Today}}},
  date = {2025-11-13},
  url = {https://www.psychologytoday.com/us/blog/the-future-brain/202508/ai-scientists-may-have-discovered-llms-light-bulb-moment},
  urldate = {2025-11-13},
  abstract = {A new study discovers a pivotal moment when AI starts comprehending what was read versus relying on the position of the words in a sentence.},
  langid = {american},
  file = {C:\Users\Aymen\Zotero\storage\PCAZ4HR2\ai-scientists-may-have-discovered-llms-light-bulb-moment.html}
}

@article{akaiExperimentalStabilityAnalysis2021,
  title = {Experimental Stability Analysis of Neural Networks in Classification Problems with Confidence Sets for Persistence Diagrams},
  author = {Akai, Naoki and Hirayama, Takatsugu and Murase, Hiroshi},
  date = {2021-11-01},
  journaltitle = {Neural Networks},
  volume = {143},
  pages = {42--51},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.05.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021001994},
  urldate = {2025-11-16},
  abstract = {We investigate classification performance of neural networks (NNs) based on topological insight in an attempt to guarantee stability of their inference. NNs which can accurately classify a dataset map it into a hidden space while disentangling intertwined data. NNs sometimes acquire forcible mapping to disentangle the data, and this forcible mapping generates outliers. The mapping around the outliers is unstable because the outputs change drastically. Hence, we define stable NNs to mean that they do not generate outliers. To investigate the possibility of the existence of outliers, we use persistent homology and a method to estimate the confidence set for persistence diagrams. The combined use enables us to test whether the focused geometry is topologically simple, that is, no outliers. In this work, we use the MNIST and CIFAR-10 datasets and investigate the relationship between the classification performance and the topological characteristics with several NNs. Investigation results with the MNIST dataset show that the test accuracy of all the networks is superior, exceeding 98\%, even though the transformed dataset is not topologically simple. Results with the CIFAR-10 dataset also show that the possibility of the existence of outliers is shown in the mapping by the accurate convolutional NNs. Therefore, we conclude that the presented investigation is necessary to guarantee that the NNs, in particular deep NNs, do not acquire unstable mapping for forcible classification.},
  keywords = {Neural networks,Persistent homology,Topological data analysis},
  file = {C:\Users\Aymen\Zotero\storage\D43JUDN5\S0893608021001994.html}
}

@inreference{AleksandrAleksandrovMathematician2025,
  title = {Aleksandr {{Aleksandrov}} (Mathematician)},
  booktitle = {Wikipedia},
  date = {2025-10-25T22:06:48Z},
  url = {https://en.wikipedia.org/w/index.php?title=Aleksandr_Aleksandrov_(mathematician)&oldid=1318770198},
  urldate = {2025-11-12},
  abstract = {Aleksandr Danilovich Aleksandrov (Russian: Алекса́ндр Дани́лович Алекса́ндров; 4 August 1912 – 27 July 1999) was a Soviet and Russian mathematician, physicist, philosopher and mountaineer.},
  langid = {english},
  annotation = {Page Version ID: 1318770198},
  file = {C:\Users\Aymen\Zotero\storage\2KBJJ4WZ\index.html}
}

@online{alvarez-melisGeometricDatasetDistances2020,
  title = {Geometric {{Dataset Distances}} via {{Optimal Transport}}},
  author = {Alvarez-Melis, David and Fusi, Nicolò},
  date = {2020-02-07},
  eprint = {2002.02923},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.02923},
  url = {http://arxiv.org/abs/2002.02923},
  urldate = {2024-09-20},
  abstract = {The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e.g., require training a model on each dataset). In this work we propose an alternative notion of distance between datasets that (i) is model-agnostic, (ii) does not involve training, (iii) can compare datasets even if their label sets are completely disjoint and (iv) has solid theoretical footing. This distance relies on optimal transport, which provides it with rich geometry awareness, interpretable correspondences and well-understood properties. Our results show that this novel distance provides meaningful comparison of datasets, and correlates well with transfer learning hardness across various experimental settings and datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Geometric Dataset Distances via optimal transport,Statistics - Machine Learning}
}

@article{amariInformationGeometry2021,
  title = {Information Geometry},
  author = {Amari, Shun-ichi},
  date = {2021-01-01},
  journaltitle = {Jpn. J. Math.},
  volume = {16},
  number = {1},
  pages = {1--48},
  issn = {1861-3624},
  doi = {10.1007/s11537-020-1920-5},
  url = {https://doi.org/10.1007/s11537-020-1920-5},
  urldate = {2025-11-13},
  abstract = {Information geometry has emerged from the study of the invariant structure in families of probability distributions. This invariance uniquely determines a second-order symmetric tensor g and third-order symmetric tensor T in a manifold of probability distributions. A pair of these tensors (g, T) defines a Riemannian metric and a pair of affine connections which together preserve the metric. Information geometry involves studying a Riemannian manifold having a pair of dual affine connections. Such a structure also arises from an asymmetric divergence function and affine differential geometry. A dually flat Riemannian manifold is particularly useful for various applications, because a generalized Pythagorean theorem and projection theorem hold. The Wasserstein distance gives another important geometry on probability distributions, which is non-invariant but responsible for the metric properties of a sample space. I attempt to construct information geometry of the entropy-regularized Wasserstein distance.},
  langid = {english},
  keywords = {53B12,canonical divergence,dual affine connection,information geometry,Pythagorean theorem,semiparametric statistics,Wasserstein geometry},
  file = {C:\Users\Aymen\Zotero\storage\NBFEHR2Y\Amari - 2021 - Information geometry.pdf}
}

@online{AppliedAlgebraicTopology2025,
  title = {Applied Algebraic Topology Network},
  date = {2025-11-12},
  url = {https://www.youtube.com/channel/UCYOcatH32zeOTnqjag0fNkw},
  urldate = {2025-11-12},
  abstract = {This is the YouTube channel for the Applied Algebraic Topology Research Network. Instead of watching recorded videos, you can join us online for the live presentations! For details on how to join the network and how to watch the talks live, please see the links to the AATRN homepage and to the AATRN seminar below. The directors of AATRN are Henry Adams at Colorado State University, Hana Dal Poz Kouřimská at IST Austria, Teresa Heiss at IST Austria, Sara Kališnik at ETH Zurich, Bastian Rieck at the Institute of AI for Health at Helmholtz Munich, and Elchanan Solomon at Duke University.},
  langid = {french},
  organization = {YouTube},
  file = {C:\Users\Aymen\Zotero\storage\3DKAV7HL\UCYOcatH32zeOTnqjag0fNkw.html}
}

@video{appliedalgebraictopologynetworkDavidLoiseaux102024,
  entrysubtype = {video},
  title = {David {{Loiseaux}} (10/16/24): {{Multiparameter Persistence}} for {{Machine Learning}}},
  shorttitle = {David {{Loiseaux}} (10/16/24)},
  editor = {{Applied Algebraic Topology Network}},
  editortype = {director},
  date = {2024-10-23},
  url = {https://www.youtube.com/watch?v=kEdPdpf5xoE},
  urldate = {2025-11-12},
  abstract = {The main tool of Topological Data Analysis is Persistent Homology, which captures the persistence of topological features as a filtration parameter changes---typically, a geometrical scale. For many applications however, it is beneficial to simultaneously vary multiple filtration parameters, such as, scales, the dataset's sampling density, or simply take into account other intrinsic properties of the data. To that end, a generalization of this construction, called Multiparameter Persistent Homology, enables the use of multiple filters at once. In this talk, I will present the recent advancements in multiparameter persistence aiming at making it practical for machine learning. To that end, I will introduce several descriptors; one is obtained by gluing one-dimensional slices, and takes the form of an interval decomposable module called an MMA decomposition, and the other is computed from the Möbius inversion of classical invariants called the signed barcode. Finally, I will discuss corresponding vectorization for these descriptors, and their differentiability properties.},
  annotation = {Directors: \_:n1348}
}

@online{apprehensive-wheel18DeepLearningFramework2022,
  type = {Reddit Post},
  title = {[{{D}}] {{Deep Learning Framework}} for {{C}}++.},
  author = {{Apprehensive-Wheel18}},
  date = {2022-06-13T06:23:48},
  url = {https://www.reddit.com/r/MachineLearning/comments/vb5lv6/d_deep_learning_framework_for_c/},
  urldate = {2025-11-13},
  organization = {r/MachineLearning},
  file = {C:\Users\Aymen\Zotero\storage\D2XP37G9\d_deep_learning_framework_for_c.html}
}

@online{armentaRepresentationTheoryNeural2020,
  title = {The {{Representation Theory}} of {{Neural Networks}}},
  author = {Armenta, Marco Antonio and Jodoin, Pierre-Marc},
  date = {2020-07-23},
  url = {https://arxiv.org/abs/2007.12213v2},
  urldate = {2024-10-03},
  abstract = {In this work, we show that neural networks can be represented via the mathematical theory of quiver representations. More specifically, we prove that a neural network is a quiver representation with activation functions, a mathematical object that we represent using a network quiver. Also, we show that network quivers gently adapt to common neural network concepts such as fully-connected layers, convolution operations, residual connections, batch normalization, pooling operations and even randomly wired neural networks. We show that this mathematical representation is by no means an approximation of what neural networks are as it exactly matches reality. This interpretation is algebraic and can be studied with algebraic methods. We also provide a quiver representation model to understand how a neural network creates representations from the data. We show that a neural network saves the data as quiver representations, and maps it to a geometrical space called the moduli space, which is given in terms of the underlying oriented graph of the network, i.e., its quiver. This results as a consequence of our defined objects and of understanding how the neural network computes a prediction in a combinatorial and algebraic way. Overall, representing neural networks through the quiver representation theory leads to 9 consequences and 4 inquiries for future research that we believe are of great interest to better understand what neural networks are and how they work.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {Manifold,Representation Theory}
}

@inreference{ArzelaAscoliTheorem2025,
  title = {Arzelà–{{Ascoli}} Theorem},
  booktitle = {Wikipedia},
  date = {2025-09-19T16:44:24Z},
  url = {https://en.wikipedia.org/w/index.php?title=Arzel%C3%A0%E2%80%93Ascoli_theorem&oldid=1312263423},
  urldate = {2025-11-12},
  abstract = {The Arzelà–Ascoli theorem is a fundamental result of mathematical analysis giving necessary and sufficient conditions to decide whether every sequence of a given family of real-valued continuous functions defined on a closed and bounded interval has a uniformly convergent subsequence.  The main condition is the equicontinuity of the family of functions. The theorem is the basis of many proofs in mathematics, including that of the Peano existence theorem in the theory of ordinary differential equations, Montel's theorem in complex analysis, and the Peter–Weyl theorem in harmonic analysis and various results concerning compactness of integral operators. The notion of equicontinuity was introduced in the late 19th century by the Italian mathematicians Cesare Arzelà and Giulio Ascoli. A weak form of the theorem was proven by Ascoli (1883–1884), who established the sufficient condition for compactness, and by Arzelà (1895), who established the necessary condition and gave the first clear presentation of the result.  A further generalization of the theorem was proven by Fréchet (1906), to sets of real-valued continuous functions with domain a compact metric space (Dunford \& Schwartz 1958, p. 382).  Modern formulations of the theorem allow for the domain to be compact Hausdorff and for the range to be an arbitrary metric space.  More general formulations of the theorem exist that give necessary and sufficient conditions for a family of functions from a compactly generated Hausdorff space into a uniform space to be compact in the compact-open topology; see Kelley (1991, page 234).},
  langid = {english},
  annotation = {Page Version ID: 1312263423},
  file = {C:\Users\Aymen\Zotero\storage\TNQV8SG3\index.html}
}

@online{AuslanderbehordeGoogleSearch2025,
  title = {({{Ausländerbehörde}}) - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=(Ausl%C3%A4nderbeh%C3%B6rde)&client=ms-android-samsung-ss&sourceid=chrome-mobile&ie=UTF-8&zx=1754129711620&no_sw_cr=1},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\C99SUDE6\search.html}
}

@online{AutoMLAutoMLMethods2025,
  title = {{{AutoML}} | {{AutoML}}: {{Methods}}, {{Systems}}, {{Challenges}} (First Book on {{AutoML}})},
  shorttitle = {{{AutoML}} | {{AutoML}}},
  date = {2025-11-13},
  url = {https://www.automl.org/book/},
  urldate = {2025-11-13},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\7376KWPX\book.html}
}

@online{AvalancheAI2025,
  title = {Avalanche {{AI}}},
  date = {2025-11-12},
  url = {https://www.avalanche-ai.com/},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\D852Z4WM\www.avalanche-ai.com.html}
}

@online{AvalancheEndtoEndLibrary,
  title = {Avalanche: An {{End-to-End Library}} for {{Continual Learning}} | {{Avalanche}}},
  shorttitle = {Avalanche},
  url = {https://avalanche.continualai.org},
  urldate = {2025-11-16},
  abstract = {Powered by ContinualAI},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\2YRLFEHF\avalanche.continualai.org.html}
}

@online{AvancementProjetAymentlili131gmailcom2024,
  title = {Avancement Projet - Aymentlili131@gmail.Com - {{Gmail}}},
  date = {2024-09-20},
  url = {https://mail.google.com/mail/u/0/#label/Forward/KtbxLrjCLnDrMznHXTWjfwJBLdzRbRsKfL?projector=1&messagePartId=0.5},
  urldate = {2024-09-20},
  keywords = {Explainable gradients,LRP}
}

@article{bahriStatisticalMechanicsDeep2020,
  title = {Statistical {{Mechanics}} of {{Deep Learning}}},
  author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
  date = {2020-03-10},
  journaltitle = {Annu. Rev. Condens. Matter Phys.},
  volume = {11},
  number = {1},
  pages = {501--528},
  issn = {1947-5454, 1947-5462},
  doi = {10.1146/annurev-conmatphys-031119-050745},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-conmatphys-031119-050745},
  urldate = {2025-11-13},
  abstract = {The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\UU239CQS\Bahri et al. - 2020 - Statistical Mechanics of Deep Learning.pdf}
}

@online{ballesterTopologicalDataAnalysis2024,
  title = {Topological {{Data Analysis}} for {{Neural Network Analysis}}: {{A Comprehensive Survey}}},
  shorttitle = {Topological {{Data Analysis}} for {{Neural Network Analysis}}},
  author = {Ballester, Rubén and Casacuberta, Carles and Escalera, Sergio},
  date = {2024-01-04},
  eprint = {2312.05840},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.05840},
  url = {http://arxiv.org/abs/2312.05840},
  urldate = {2025-11-12},
  abstract = {This survey provides a comprehensive exploration of applications of Topological Data Analysis (TDA) within neural network analysis. Using TDA tools such as persistent homology and Mapper, we delve into the intricate structures and behaviors of neural networks and their datasets. We discuss different strategies to obtain topological information from data and neural networks by means of TDA. Additionally, we review how topological information can be leveraged to analyze properties of neural networks, such as their generalization capacity or expressivity. We explore practical implications of deep learning, specifically focusing on areas like adversarial detection and model selection. Our survey organizes the examined works into four broad domains: 1. Characterization of neural network architectures; 2. Analysis of decision regions and boundaries; 3. Study of internal representations, activations, and parameters; 4. Exploration of training dynamics and loss functions. Within each category, we discuss several articles, offering background information to aid in understanding the various methodologies. We conclude with a synthesis of key insights gained from our study, accompanied by a discussion of challenges and potential advancements in the field.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  note = {Comment: 70 pages, 7 figures. 4 references added. Minor changes in the text. Part of generative models reestructured to improve generality and clarity of exposition
\par
Comment: 70 pages, 7 figures. 4 references added. Minor changes in the text. Part of generative models reestructured to improve generality and clarity of exposition},
  file = {C:\Users\Aymen\Zotero\storage\SXRNRJ67\Ballester et al. - 2024 - Topological Data Analysis for Neural Network Analysis A Comprehensive Survey.pdf}
}

@article{bapstUnveilingPredictivePower2020,
  title = {Unveiling the Predictive Power of Static Structure in Glassy Systems},
  author = {Bapst, Victor and Keck, Thomas and Grabska-Barwińska, A. and Donner, Craig and Cubuk, Ekin Dogus and Schoenholz, Samuel S. and Obika, Annette and Nelson, Alexander WR and Back, Trevor and Hassabis, Demis},
  date = {2020},
  journaltitle = {Nature physics},
  volume = {16},
  number = {4},
  pages = {448--454},
  publisher = {Nature Publishing Group UK London},
  url = {https://www.nature.com/articles/s41567-020-0842-8},
  urldate = {2025-11-13}
}

@online{barannikovRepresentationTopologyDivergence2022,
  title = {Representation {{Topology Divergence}}: {{A Method}} for {{Comparing Neural Network Representations}}},
  shorttitle = {Representation {{Topology Divergence}}},
  author = {Barannikov, Serguei and Trofimov, Ilya and Balabin, Nikita and Burnaev, Evgeny},
  date = {2022-06-28},
  eprint = {2201.00058},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.00058},
  url = {http://arxiv.org/abs/2201.00058},
  urldate = {2025-11-16},
  abstract = {Comparison of data representations is a complex multi-aspect problem that has not enjoyed a complete solution yet. We propose a method for comparing two data representations. We introduce the Representation Topology Divergence (RTD), measuring the dissimilarity in multi-scale topology between two point clouds of equal size with a one-to-one correspondence between points. The data point clouds are allowed to lie in different ambient spaces. The RTD is one of the few TDA-based practical methods applicable to real machine learning datasets. Experiments show that the proposed RTD agrees with the intuitive assessment of data representation similarity and is sensitive to its topological structure. We apply RTD to gain insights on neural networks representations in computer vision and NLP domains for various problems: training dynamics analysis, data distribution shift, transfer learning, ensemble learning, disentanglement assessment.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - K-Theory and Homology,Mathematics - Symplectic Geometry},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\97GI9MKQ\\Barannikov et al. - 2022 - Representation Topology Divergence A Method for Comparing Neural Network Representations.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\XXZ7CN3I\\2201.html}
}

@online{barannikovRepresentationTopologyDivergence2022a,
  title = {Representation {{Topology Divergence}}: {{A Method}} for {{Comparing Neural Network Representations}}},
  shorttitle = {Representation {{Topology Divergence}}},
  author = {Barannikov, Serguei and Trofimov, Ilya and Balabin, Nikita and Burnaev, Evgeny},
  date = {2022-06-28},
  eprint = {2201.00058},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.00058},
  url = {http://arxiv.org/abs/2201.00058},
  urldate = {2025-11-16},
  abstract = {Comparison of data representations is a complex multi-aspect problem that has not enjoyed a complete solution yet. We propose a method for comparing two data representations. We introduce the Representation Topology Divergence (RTD), measuring the dissimilarity in multi-scale topology between two point clouds of equal size with a one-to-one correspondence between points. The data point clouds are allowed to lie in different ambient spaces. The RTD is one of the few TDA-based practical methods applicable to real machine learning datasets. Experiments show that the proposed RTD agrees with the intuitive assessment of data representation similarity and is sensitive to its topological structure. We apply RTD to gain insights on neural networks representations in computer vision and NLP domains for various problems: training dynamics analysis, data distribution shift, transfer learning, ensemble learning, disentanglement assessment.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - K-Theory and Homology,Mathematics - Symplectic Geometry},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\DRLQBB7X\\Barannikov et al. - 2022 - Representation Topology Divergence A Method for Comparing Neural Network Representations.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\3HVFNB8B\\2201.html}
}

@online{bauerDistributedComputationPersistent2013,
  title = {Distributed Computation of Persistent Homology},
  author = {Bauer, Ulrich and Kerber, Michael and Reininghaus, Jan},
  date = {2013-10-03},
  eprint = {1310.0710},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1310.0710},
  url = {http://arxiv.org/abs/1310.0710},
  urldate = {2025-11-12},
  abstract = {Persistent homology is a popular and powerful tool for capturing topological features of data. Advances in algorithms for computing persistent homology have reduced the computation time drastically – as long as the algorithm does not exhaust the available memory. Following up on a recently presented parallel method for persistence computation on shared memory systems, we demonstrate that a simple adaption of the standard reduction algorithm leads to a variant for distributed systems. Our algorithmic design ensures that the data is distributed over the nodes without redundancy; this permits the computation of much larger instances than on a single machine. Moreover, we observe that the parallelism at least compensates for the overhead caused by communication between nodes, and often even speeds up the computation compared to sequential and even parallel shared memory algorithms. In our experiments, we were able to compute the persistent homology of filtrations with more than a billion (109) elements within seconds on a cluster with 32 nodes using less than 10GB of memory per node.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Distributed Parallel and Cluster Computing,Mathematics - Algebraic Topology},
  file = {C:\Users\Aymen\Zotero\storage\RWGH4DUG\Bauer et al. - 2013 - Distributed computation of persistent homology.pdf}
}

@online{BeginnersGuideTheory2025,
  title = {A Beginner's Guide to the Theory of Viscosity Solutions [2\&nbsp;Ed.]},
  date = {2025-11-12},
  url = {https://dokumen.pub/a-beginners-guide-to-the-theory-of-viscosity-solutions-2nbsped.html},
  urldate = {2025-11-12},
  abstract = {...},
  langid = {english},
  organization = {dokumen.pub},
  file = {C:\Users\Aymen\Zotero\storage\5FLE2BGH\a-beginners-guide-to-the-theory-of-viscosity-solutions-2nbsped.html}
}

@online{beltagyLongformerLongDocumentTransformer2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  date = {2020-12-02},
  eprint = {2004.05150},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.05150},
  url = {http://arxiv.org/abs/2004.05150},
  urldate = {2025-11-16},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Version 2 introduces the Longformer-Encoder-Decoder (LED) model},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\TDFE73BN\\Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\H76WEQWH\\2004.html}
}

@online{BenignOverfittingGoogle2025,
  title = {Benign Overfitting - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=benign+overfitting&oq=benign+overfitting&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIHCAEQABiABDIHCAIQABiABDIHCAMQABiABDIICAQQABgWGB4yCAgFEAAYFhgeMggIBhAAGBYYHjIICAcQABgWGB4yCAgIEAAYFhgeMggICRAAGBYYHjIICAoQABgWGB4yCAgLEAAYFhgeMgcIDBAhGI8C0gEINTEyNmowajeoAg-wAgHxBX_PQQ3-5bKG&client=ms-android-samsung-ss&sourceid=chrome-mobile&ie=UTF-8},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\MRRHM8LL\search.html}
}

@inreference{BernsteinMisesTheorem2024,
  title = {Bernstein–von {{Mises}} Theorem},
  booktitle = {Wikipedia},
  date = {2024-10-23T11:56:47Z},
  url = {https://en.wikipedia.org/w/index.php?title=Bernstein%E2%80%93von_Mises_theorem&oldid=1252893936},
  urldate = {2024-11-05},
  abstract = {In Bayesian inference, the Bernstein–von Mises theorem provides the basis for using Bayesian credible sets for confidence statements in parametric models. It states that under some conditions, a posterior distribution converges in total variation distance to a multivariate normal distribution centered at the maximum likelihood estimator                                                                                 θ                 \textasciicircum{}                                                               n                                     \{\textbackslash displaystyle \{\textbackslash widehat \{\textbackslash theta \}\}\_\{n\}\}     with covariance matrix given by                                     n                        −             1                                                     I                             (                    θ                        0                                        )                        −             1                                     \{\textbackslash displaystyle n\textasciicircum\{-1\}\{\textbackslash mathcal \{I\}\}(\textbackslash theta \_\{0\})\textasciicircum\{-1\}\}    , where                                    θ                        0                                     \{\textbackslash displaystyle \textbackslash theta \_\{0\}\}     is the true population parameter and                                                 I                             (                    θ                        0                             )                 \{\textbackslash displaystyle \{\textbackslash mathcal \{I\}\}(\textbackslash theta \_\{0\})\}     is the Fisher information matrix at the true population parameter value:                                   |                             |                  P         (         θ                    |                             x                        1                             ,         …                    x                        n                             )         −                                 N                             (                                                                 θ                 \textasciicircum{}                                                               n                             ,                    n                        −             1                                                     I                             (                    θ                        0                                        )                        −             1                             )                    |                                          |                                                  T               V                                                     →                                       P                                                   θ                                        0                                                                                             =         0                 \{\textbackslash displaystyle ||P(\textbackslash theta |x\_\{1\},\textbackslash dots x\_\{n\})-\{\textbackslash mathcal \{N\}\}(\{\textbackslash widehat \{\textbackslash theta \}\}\_\{n\},n\textasciicircum\{-1\}\{\textbackslash mathcal \{I\}\}(\textbackslash theta \_\{0\})\textasciicircum\{-1\})||\_\{\textbackslash mathrm \{TV\} \}\textbackslash xrightarrow \{P\_\{\textbackslash theta \_\{0\}\}\} =0\}    The Bernstein–von Mises theorem links Bayesian inference with frequentist inference. It assumes there is some true probabilistic process that generates the observations, as in frequentism, and then studies the quality of Bayesian methods of recovering that process, and making uncertainty statements about that process. In particular, it states that asymptotically, many Bayesian credible sets of a certain credibility level                         α                 \{\textbackslash displaystyle \textbackslash alpha \}     will act as confidence sets of confidence level                         α                 \{\textbackslash displaystyle \textbackslash alpha \}    , which allows for the interpretation of Bayesian credible sets.},
  langid = {english},
  annotation = {Page Version ID: 1252893936}
}

@online{BestDistributedSystems2025,
  title = {Best {{Distributed Systems Companies}} and {{Startups}} to {{Work}} for in 2026},
  date = {2025-11-13},
  url = {https://wellfound.com/startups/industry/distributed-systems-2},
  urldate = {2025-11-13},
  abstract = {Find the best Distributed Systems companies and startups currently hiring on Wellfound - See company jobs, overviews, benefits, funding info, employee reviews, and more.},
  langid = {english},
  organization = {Wellfound},
  file = {C:\Users\Aymen\Zotero\storage\L8SIBQIX\distributed-systems-2.html}
}

@online{bhumbraDeepLearningImproved2018,
  title = {Deep Learning Improved by Biological Activation Functions},
  author = {Bhumbra, Gardave S.},
  date = {2018-05-18},
  eprint = {1804.11237},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.11237},
  url = {http://arxiv.org/abs/1804.11237},
  urldate = {2024-09-20},
  abstract = {`Biologically inspired' activation functions, such as the logistic sigmoid, have been instrumental in the historical advancement of machine learning. However in the field of deep learning, they have been largely displaced by rectified linear units (ReLU) or similar functions, such as its exponential linear unit (ELU) variant, to mitigate the effects of vanishing gradients associated with error back-propagation. The logistic sigmoid however does not represent the true input-output relation in neuronal cells under physiological conditions. Here, bionodal root unit (BRU) activation functions are introduced, exhibiting input-output non-linearities that are substantially more biologically plausible since their functional form is based on known biophysical properties of neuronal cells. In order to evaluate the learning performance of BRU activations, deep networks are constructed with identical architectures except differing in their transfer functions (ReLU, ELU, and BRU). Multilayer perceptrons, stacked auto-encoders, and convolutional networks are used to test supervised and unsupervised learning based on the MNIST and CIFAR-10/100 datasets. Comparisons of learning performance, quantified using loss and error measurements, demonstrate that bionodal networks both train faster than their ReLU and ELU counterparts and result in the best generalised models even in the absence of formal regularisation. These results therefore suggest that revisiting the detailed properties of biological neurones and their circuitry might prove invaluable in the field of deep learning for the future.},
  pubstate = {prepublished},
  keywords = {BRU,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: 11 pages, 4 figures. 18/05/2018: 9 pages, 5 figures. Eq. 1 corrected. Changed name of biological activation functions. Weight initialisation simplified: experiments repeated, all figures changed, overall results unchanged, Methods shortened, Appendix removed. Added new Results section for new experiments on CIFAR 10/100 data. Extended arguments in Discussion}
}

@online{birdalIntrinsicDimensionPersistent2021,
  title = {Intrinsic {{Dimension}}, {{Persistent Homology}} and {{Generalization}} in {{Neural Networks}}},
  author = {Birdal, Tolga and Lou, Aaron and Guibas, Leonidas and Şimşekli, Umut},
  date = {2021-11-25},
  eprint = {2111.13171},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.13171},
  url = {http://arxiv.org/abs/2111.13171},
  urldate = {2025-11-16},
  abstract = {Disobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Recently, it has been shown that the trajectories of iterative optimization algorithms can possess fractal structures, and their generalization error can be formally linked to the complexity of such fractals. This complexity is measured by the fractal's intrinsic dimension, a quantity usually much smaller than the number of parameters in the network. Even though this perspective provides an explanation for why overparametrized networks would not overfit, computing the intrinsic dimension (e.g., for monitoring generalization during training) is a notoriously difficult task, where existing methods typically fail even in moderate ambient dimensions. In this study, we consider this problem from the lens of topological data analysis (TDA) and develop a generic computational tool that is built on rigorous mathematical foundations. By making a novel connection between learning theory and TDA, we first illustrate that the generalization error can be equivalently bounded in terms of a notion called the 'persistent homology dimension' (PHD), where, compared with prior work, our approach does not require any additional geometrical or statistical assumptions on the training dynamics. Then, by utilizing recently established theoretical results and TDA tools, we develop an efficient algorithm to estimate PHD in the scale of modern deep neural networks and further provide visualization tools to help understand generalization in deep learning. Our experiments show that the proposed approach can efficiently compute a network's intrinsic dimension in a variety of settings, which is predictive of the generalization error.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - General Topology,Statistics - Machine Learning},
  note = {Comment: Appears at NeurIPS 2021},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\NJQ35WKU\\Birdal et al. - 2021 - Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\VBH84ZJG\\2111.html}
}

@online{bodnerPyTorchOptimizersArent2024,
  title = {{{PyTorch Optimizers Aren}}’t {{Fast Enough}}. {{Try These Instead}}},
  author = {Bodner, Benjamin},
  date = {2024-10-14T17:39:36},
  url = {https://towardsdatascience.com/pytorch-optimizers-arent-fast-enough-try-these-instead-61a1350e3eac},
  urldate = {2024-11-05},
  abstract = {These 4 advanced optimizers will open your mind.},
  langid = {english},
  organization = {Medium},
  keywords = {Pytorch optimizers}
}

@article{botnanBottleneckStabilityRank2024,
  title = {On the Bottleneck Stability of Rank Decompositions of Multi-Parameter Persistence Modules},
  author = {Botnan, Magnus Bakke and Oppermann, Steffen and Oudot, Steve and Scoccola, Luis},
  date = {2024-08},
  journaltitle = {Advances in Mathematics},
  volume = {451},
  eprint = {2208.00300},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {109780},
  issn = {00018708},
  doi = {10.1016/j.aim.2024.109780},
  url = {http://arxiv.org/abs/2208.00300},
  urldate = {2025-11-12},
  abstract = {A significant part of modern topological data analysis is concerned with the design and study of algebraic invariants of poset representations—often referred to as persistence modules. One such invariant is the minimal rank decomposition, which encodes the ranks of all the structure morphisms of the persistence module by a single ordered pair of rectangle-decomposable modules, interpreted as a signed barcode. This signed barcode generalizes the concept of persistence barcode from one-parameter persistence to any number of parameters, raising the question of its bottleneck stability. We show in this paper that the minimal rank decomposition is not stable under the natural notion of signed bottleneck matching between signed barcodes. We remedy this by turning our focus to the rank exact decomposition, a related signed barcode induced by the minimal projective resolution of the module relative to the so-called rank exact structure, which we prove to be bottleneck stable under signed matchings. As part of our proof, we obtain two intermediate results of independent interest: we compute the global dimension of the rank exact structure on the category of finitely presentable multi-parameter persistence modules, and we prove a bottleneck stability result for hook-decomposable modules. We also give a bound for the size of the rank exact decomposition that is polynomial in the size of the usual minimal projective resolution, we prove a universality result for the dissimilarity function induced by the notion of signed matching, and we compute, in the two-parameter case, the global dimension of a different exact structure related to the upsets of the indexing poset. This set of results combines concepts from topological data analysis and from the representation theory of posets, and we believe is relevant to both areas.},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics - Representation Theory},
  note = {Comment: 33 pages, 4 figures; v2: add details, fix typos and minor issues, improve exposition, add conjecture 5.30; v3: minor improvements},
  file = {C:\Users\Aymen\Zotero\storage\CYYAC47Z\Botnan et al. - 2024 - On the bottleneck stability of rank decompositions of multi-parameter persistence modules.pdf}
}

@article{breaWeightspaceSymmetryDeep2019,
  title = {Weight-Space Symmetry in Deep Networks Gives Rise to Permutation Saddles, Connected by Equal-Loss Valleys across the Loss Landscape},
  author = {Brea, Johanni and Simsek, Berfin and Illing, Bernd and Gerstner, Wulfram},
  date = {2019-07-01},
  pages = {arXiv:1907.02911},
  doi = {10.48550/arXiv.1907.02911},
  abstract = {The permutation symmetry of neurons in each layer of a deep neural network gives rise not only to multiple equivalent global minima of the loss function, but also to first-order saddle points located on the path between the global minima. In a network of \$d-1\$ hidden layers with \$n\_k\$ neurons in layers \$k = 1, \textbackslash ldots, d\$, we construct smooth paths between equivalent global minima that lead through a `permutation point' where the input and output weight vectors of two neurons in the same hidden layer \$k\$ collide and interchange. We show that such permutation points are critical points with at least \$n\_\{k+1\}\$ vanishing eigenvalues of the Hessian matrix of second derivatives indicating a local plateau of the loss function. We find that a permutation point for the exchange of neurons \$i\$ and \$j\$ transits into a flat valley (or generally, an extended plateau of \$n\_\{k+1\}\$ flat dimensions) that enables all \$n\_k!\$ permutations of neurons in a given layer \$k\$ at the same loss value. Moreover, we introduce high-order permutation points by exploiting the recursive structure in neural network functions, and find that the number of \$K\textasciicircum\{\textbackslash text\{th\}\}\$-order permutation points is at least by a factor \$\textbackslash sum\_\{k=1\}\textasciicircum\{d-1\}\textbackslash frac\{1\}\{2!\textasciicircum K\}\{n\_k-K \textbackslash choose K\}\$ larger than the (already huge) number of equivalent global minima. In two tasks, we illustrate numerically that some of the permutation points correspond to first-order saddles (`permutation saddles'): first, in a toy network with a single hidden layer on a function approximation task and, second, in a multilayer network on the MNIST task. Our geometric approach yields a lower bound on the number of critical points generated by weight-space symmetries and provides a simple intuitive link between previous mathematical results and numerical observations.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{brenierPolarFactorizationMonotone1991,
  title = {Polar Factorization and Monotone Rearrangement of Vector‐valued Functions},
  author = {Brenier, Yann},
  date = {1991-06},
  journaltitle = {Comm Pure Appl Math},
  volume = {44},
  number = {4},
  pages = {375--417},
  issn = {0010-3640, 1097-0312},
  doi = {10.1002/cpa.3160440402},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.3160440402},
  urldate = {2025-11-12},
  abstract = {Given a probability space ( X , p ) and a bounded domain R in R d equipped with the Lebesgue measure 1 . I (normalized so that 10I = I ), it is shown (under additional technical assumptions on X and Q) that for every vector-valued function u E L p ( X ,p; R d )there is a unique “polar factorization” u = V\$.s, where \$ is a convex function defined on R and s is a measure-preservingmapping from ( X , p ) into ( Q , I . I), provided that u is nondegenerate, in the sense that p ( u - ’ ( E ) )= 0 for each Lebesgue negligible subset E of Rd.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\PQM6N3YD\Brenier - 1991 - Polar factorization and monotone rearrangement of vector‐valued functions.pdf}
}

@software{bwehlinBwehlinMasspcf2025,
  title = {Bwehlin/Masspcf},
  author = {{bwehlin}},
  date = {2025-07-13T19:46:41Z},
  origdate = {2023-09-24T16:19:43Z},
  url = {https://github.com/bwehlin/masspcf},
  urldate = {2025-11-12},
  abstract = {Massively Parallel Computations for Piecewise Constant Functions},
  annotation = {Programmers: \_:n1351}
}

@online{caiTrainingFreeGroupRelative2025,
  title = {Training-{{Free Group Relative Policy Optimization}}},
  author = {Cai, Yuzheng and Cai, Siqi and Shi, Yuchen and Xu, Zihan and Chen, Lichao and Qin, Yulei and Tan, Xiaoyu and Li, Gang and Li, Zongyi and Lin, Haojia and Mao, Yong and Li, Ke and Sun, Xing},
  date = {2025-10-10},
  eprint = {2510.08191},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.08191},
  url = {http://arxiv.org/abs/2510.08191},
  urldate = {2025-11-12},
  abstract = {Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\TKNHCDCQ\\Cai et al. - 2025 - Training-Free Group Relative Policy Optimization.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\BP68P6AG\\2510.html}
}

@article{carlssonTheoryMultidimensionalPersistence2009,
  title = {The {{Theory}} of {{Multidimensional Persistence}}},
  author = {Carlsson, Gunnar and Zomorodian, Afra},
  date = {2009-07},
  journaltitle = {Discrete Comput Geom},
  volume = {42},
  number = {1},
  pages = {71--93},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-009-9176-0},
  url = {http://link.springer.com/10.1007/s00454-009-9176-0},
  urldate = {2025-11-12},
  abstract = {Persistent homology captures the topology of a filtration—a one-parameter family of increasing spaces—in terms of a complete discrete invariant. This invariant is a multiset of intervals that denote the lifetimes of the topological entities within the filtration. In many applications of topology, we need to study a multifiltration: a family of spaces parameterized along multiple geometric dimensions. In this paper, we show that no similar complete discrete invariant exists for multidimensional persistence. Instead, we propose the rank invariant, a discrete invariant for the robust estimation of Betti numbers in a multifiltration, and prove its completeness in one dimension.},
  langid = {english},
  keywords = {Computational topology,Multidimensional analysis,Persistence,Persistent homology},
  file = {C:\Users\Aymen\Zotero\storage\7LQ3EK62\Carlsson et Zomorodian - 2009 - The Theory of Multidimensional Persistence.pdf}
}

@online{carlssonTopologicalApproachesDeep2018,
  title = {Topological {{Approaches}} to {{Deep Learning}}},
  author = {Carlsson, Gunnar and Gabrielsson, Rickard Brüel},
  date = {2018-11-02},
  eprint = {1811.01122},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.01122},
  url = {http://arxiv.org/abs/1811.01122},
  urldate = {2025-11-16},
  abstract = {We perform topological data analysis on the internal states of convolutional deep neural networks to develop an understanding of the computations that they perform. We apply this understanding to modify the computations so as to (a) speed up computations and (b) improve generalization from one data set of digits to another. One byproduct of the analysis is the production of a geometry on new sets of features on data sets of images, and use this observation to develop a methodology for constructing analogues of CNN's for many other geometries, including the graph structures constructed by topological data analysis.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  note = {Comment: 23 pages, 10 figures},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\QC9U74LL\\Carlsson et Gabrielsson - 2018 - Topological Approaches to Deep Learning.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\2FT7CAT8\\1811.html}
}

@article{carriereMultiparameterPersistenceImages,
  title = {Multiparameter {{Persistence Images}} for {{Topological Machine Learning}}},
  author = {Carrière, Mathieu and Blumberg, Andrew J},
  abstract = {In the last decade, there has been increasing interest in topological data analysis, a new methodology for using geometric structures in data for inference and learning. A central theme in the area is the idea of persistence, which in its most basic form studies how measures of shape change as a scale parameter varies. There are now a number of frameworks that support statistics and machine learning in this context. However, in many applications there are several different parameters one might wish to vary: for example, scale and density. In contrast to the one-parameter setting, techniques for applying statistics and machine learning in the setting of multiparameter persistence are not well understood due to the lack of a concise representation of the results.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\LZQYVWRN\Carrière et Blumberg - Multiparameter Persistence Images for Topological Machine Learning.pdf}
}

@incollection{caseyWhatPeopleDementia2016,
  title = {What {{People}} with {{Dementia Want}}: {{Designing MARIO}} an {{Acceptable Robot Companion}}},
  shorttitle = {What {{People}} with {{Dementia Want}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author = {Casey, Dympna and Felzmann, Heike and Pegman, Geoff and Kouroupetroglou, Christos and Murphy, Kathy and Koumpis, Adamantios and Whelan, Sally},
  editor = {Miesenberger, Klaus and Bühler, Christian and Penaz, Petr},
  date = {2016},
  volume = {9758},
  pages = {318--325},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-41264-1_44},
  url = {http://link.springer.com/10.1007/978-3-319-41264-1_44},
  urldate = {2025-11-13},
  isbn = {978-3-319-41263-4 978-3-319-41264-1},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\8PRDN3VC\Casey et al. - 2016 - What People with Dementia Want Designing MARIO an Acceptable Robot Companion.pdf}
}

@online{CategoriesAI2024,
  title = {Categories for AI},
  date = {2024-11-05},
  url = {http://www.youtube.com/playlist?list=PLSdFiFTAI4sQ0Rg4BIZcNnU-45I9DI-VB},
  urldate = {2024-11-05},
  abstract = {Please see: https://cats.for.ai/},
  langid = {french},
  organization = {YouTube},
  keywords = {Category theory}
}

@article{cesa-bianchiBilateralTradeRegret2023,
  title = {Bilateral {{Trade}}: {{A Regret Minimization Perspective}}},
  shorttitle = {Bilateral {{Trade}}},
  author = {Cesa-Bianchi, Nicolò and Cesari, Tommaso and Colomboni, Roberto and Fusco, Federico and Leonardi, Stefano},
  date = {2023-02-17},
  journaltitle = {Mathematics of OR},
  pages = {moor.2023.1351},
  issn = {0364-765X, 1526-5471},
  doi = {10.1287/moor.2023.1351},
  url = {http://pubsonline.informs.org/doi/10.1287/moor.2023.1351},
  urldate = {2024-11-05},
  abstract = {Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. In this paper, we cast the bilateral trade problem in a regret minimization framework over T rounds of seller/buyer interactions, with no prior knowledge on their private valuations. Our main contribution is a complete characterization of the regret regimes for fixed-price mechanisms with different feedback models and private valuations, using as a benchmark the best fixed price in hindsight. More precisely, we prove the following tight bounds on the regret: [Formula: see text] for full-feedback (i.e., direct revelation mechanisms). [Formula: see text] for realistic feedback (i.e., posted-price mechanisms) and independent seller/buyer valuations with bounded densities. [Formula: see text] for realistic feedback and seller/buyer valuations with bounded densities. [Formula: see text] for realistic feedback and independent seller/buyer valuations. [Formula: see text] for the adversarial setting.             Funding: This work was partially supported by the European Research Council Advanced [Grant 788893] AMDROMA “Algorithmic and Mechanism Design Research in Online Markets”, the Ministero dell’Istruzione, dell’Università e della Ricerca PRIN project ALGADIMAR “Algorithms, Games, and Digital Markets”, the AI Interdisciplinary Institute ANITI (funded by the French “Investing for the Future—PIA3” program under the [Grant agreement ANR-19-PI3A-0004], the project BOLD from the French national research agency (ANR), the EU Horizon 2020 ICT-48 research and innovation action ELISE (European Learning and Intelligent Systems Excellence, [Grant agreement 951847].},
  langid = {english}
}

@software{chamroukhiFchamroukhiHMMR_r2025,
  title = {Fchamroukhi/{{HMMR}}\_r},
  author = {Chamroukhi, Faïcel},
  date = {2025-10-20T11:36:18Z},
  origdate = {2019-01-17T09:23:54Z},
  url = {https://github.com/fchamroukhi/HMMR_r},
  urldate = {2025-11-12},
  abstract = {Hidden Markov Model Regression (HMMR) for Times Series Segmentation},
  annotation = {Programmers: \_:n1443}
}

@online{charlierPHomGeMPersistentHomology2019,
  title = {{{PHom-GeM}}: {{Persistent Homology}} for {{Generative Models}}},
  shorttitle = {{{PHom-GeM}}},
  author = {Charlier, Jeremy and State, Radu and Hilger, Jean},
  date = {2019-05-23},
  eprint = {1905.09894},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.09894},
  url = {http://arxiv.org/abs/1905.09894},
  urldate = {2025-11-16},
  abstract = {Generative neural network models, including Generative Adversarial Network (GAN) and Auto-Encoders (AE), are among the most popular neural network models to generate adversarial data. The GAN model is composed of a generator that produces synthetic data and of a discriminator that discriminates between the generator's output and the true data. AE consist of an encoder which maps the model distribution to a latent manifold and of a decoder which maps the latent manifold to a reconstructed distribution. However, generative models are known to provoke chaotically scattered reconstructed distribution during their training, and consequently, incomplete generated adversarial distributions. Current distance measures fail to address this problem because they are not able to acknowledge the shape of the data manifold, i.e. its topological features, and the scale at which the manifold should be analyzed. We propose Persistent Homology for Generative Models, PHom-GeM, a new methodology to assess and measure the distribution of a generative model. PHom-GeM minimizes an objective function between the true and the reconstructed distributions and uses persistent homology, the study of the topological features of a space at different spatial resolutions, to compare the nature of the true and the generated distributions. Our experiments underline the potential of persistent homology for Wasserstein GAN in comparison to Wasserstein AE and Variational AE. The experiments are conducted on a real-world data set particularly challenging for traditional distance measures and generative neural network models. PHom-GeM is the first methodology to propose a topological distance measure, the bottleneck distance, for generative models used to compare adversarial samples in the context of credit card transactions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\AGAMK4V3\\Charlier et al. - 2019 - PHom-GeM Persistent Homology for Generative Models.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\8SM7BMKF\\1905.html}
}

@online{chazalIntroductionTopologicalData2021,
  title = {An Introduction to {{Topological Data Analysis}}: Fundamental and Practical Aspects for Data Scientists},
  shorttitle = {An Introduction to {{Topological Data Analysis}}},
  author = {Chazal, Frédéric and Michel, Bertrand},
  date = {2021-02-26},
  eprint = {1710.04019},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1710.04019},
  url = {http://arxiv.org/abs/1710.04019},
  urldate = {2025-11-12},
  abstract = {Topological Data Analysis (tda) is a recent and fast growing field providing a set of new topological and geometric tools to infer relevant features for possibly complex data. This paper is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of tda for non experts.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\ZKV8H948\Chazal et Michel - 2021 - An introduction to Topological Data Analysis fundamental and practical aspects for data scientists.pdf}
}

@online{chenComprehensiveStudyDataset2023,
  title = {A {{Comprehensive Study}} on {{Dataset Distillation}}: {{Performance}}, {{Privacy}}, {{Robustness}} and {{Fairness}}},
  shorttitle = {A {{Comprehensive Study}} on {{Dataset Distillation}}},
  author = {Chen, Zongxiong and Geng, Jiahui and Zhu, Derui and Woisetschlaeger, Herbert and Li, Qing and Schimmler, Sonja and Mayer, Ruben and Rong, Chunming},
  date = {2023-05-27},
  eprint = {2305.03355},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.03355},
  url = {http://arxiv.org/abs/2305.03355},
  urldate = {2024-09-20},
  abstract = {The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,DD,survey}
}

@online{chenTopologicalRegularizerClassifiers2018,
  title = {A {{Topological Regularizer}} for {{Classifiers}} via {{Persistent Homology}}},
  author = {Chen, Chao and Ni, Xiuyan and Bai, Qinxun and Wang, Yusu},
  date = {2018-10-16},
  eprint = {1806.10714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.10714},
  url = {http://arxiv.org/abs/1806.10714},
  urldate = {2025-11-16},
  abstract = {Regularization plays a crucial role in supervised learning. Most existing methods enforce a global regularization in a structure agnostic manner. In this paper, we initiate a new direction and propose to enforce the structural simplicity of the classification boundary by regularizing over its topological complexity. In particular, our measurement of topological complexity incorporates the importance of topological features (e.g., connected components, handles, and so on) in a meaningful manner, and provides a direct control over spurious topological structures. We incorporate the new measurement as a topological penalty in training classifiers. We also propose an efficient algorithm to compute the gradient of such penalty. Our method provides a novel way to topologically simplify the global structure of the model, without having to sacrifice too much of the flexibility of the model. We demonstrate the effectiveness of our new topological regularizer on a range of synthetic and real-world datasets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\PXI2B25G\Chen et al. - 2018 - A Topological Regularizer for Classifiers via Persistent Homology.pdf}
}

@online{chindrisSimultaneousRobustSubspace2022,
  title = {Simultaneous Robust Subspace Recovery and Semi-Stability of Quiver Representations},
  author = {Chindris, Calin and Kline, Daniel},
  date = {2022-02-18},
  eprint = {2003.02962},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2003.02962},
  url = {http://arxiv.org/abs/2003.02962},
  urldate = {2024-09-20},
  abstract = {We consider the problem of simultaneously finding lower-dimensional subspace structures in a given \$m\$-tuple of possibly corrupted, high-dimensional data sets all of the same size. We refer to this problem as simultaneous robust subspace recovery (SRSR) and provide a quiver invariant theoretic approach to it. We show that SRSR is a particular case of the more general problem of effectively deciding whether a quiver representation is semi-stable (in the sense of Geometric Invariant Theory) and, in case it is not, finding a subrepresentation certifying in an optimal way that the representation is not semi-stable. In this paper, we show that SRSR and the more general quiver semi-stability problem can be solved effectively.},
  pubstate = {prepublished},
  keywords = {16G20 13A50 14L24,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Mathematics - Representation Theory,Quivers,Representation Theory}
}

@online{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  date = {2015-01-21},
  eprint = {1412.0233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.0233},
  urldate = {2024-09-17},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Loss surface,spinglass model}
}

@online{choromanskiRethinkingAttentionPerformers2022,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  date = {2022-11-19},
  eprint = {2009.14794},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.14794},
  url = {http://arxiv.org/abs/2009.14794},
  urldate = {2025-11-16},
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper + oral presentation at ICLR 2021. 38 pages. See https://github.com/google-research/google-research/tree/master/protein\_lm for protein language model code, and https://github.com/google-research/google-research/tree/master/performer for Performer code. See https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html for Google AI Blog},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\M2MU52X3\\Choromanski et al. - 2022 - Rethinking Attention with Performers.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\PL46ETLX\\2009.html}
}

@inproceedings{chowdhuryPathHomologiesDeep2019,
  title = {Path Homologies of Deep Feedforward Networks},
  booktitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  author = {Chowdhury, Samir and Gebhart, Thomas and Huntsman, Steve and Yutin, Matvey},
  date = {2019-12},
  eprint = {1910.07617},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {1077--1082},
  doi = {10.1109/ICMLA.2019.00181},
  url = {http://arxiv.org/abs/1910.07617},
  urldate = {2025-11-16},
  abstract = {We provide a characterization of two types of directed homology for fully-connected, feedforward neural network architectures. These exact characterizations of the directed homology structure of a neural network architecture are the first of their kind. We show that the directed flag homology of deep networks reduces to computing the simplicial homology of the underlying undirected graph, which is explicitly given by Euler characteristic computations. We also show that the path homology of these networks is non-trivial in higher dimensions and depends on the number and size of the layers within the network. These results provide a foundation for investigating homological differences between neural network architectures and their realized structure as implied by their parameters.},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  note = {Comment: To appear in the proceedings of IEEE ICMLA 2019},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\T5WKAXQS\\Chowdhury et al. - 2019 - Path homologies of deep feedforward networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\4GQR89MA\\1910.html}
}

@online{CollectiveProjectionMethods2025,
  title = {Collective Projection Methods to Solve {{PDEs}} - {{Google Search}}},
  date = {2025-11-12},
  url = {https://www.google.com/search?client=firefox-b-d&q=collective+projection+methods+to+solve+PDEs},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\WV2595XV\search.html}
}

@article{ComplexityNeuralNetwork2025,
  title = {On the {{Complexity}} of {{Neural Network Classifiers}}: {{A Comparison Between Shallow}} and {{Deep Architectures}} | {{Request PDF}}},
  shorttitle = {On the {{Complexity}} of {{Neural Network Classifiers}}},
  date = {2025-08-10},
  journaltitle = {ResearchGate},
  doi = {10.1109/TNNLS.2013.2293637},
  url = {https://www.researchgate.net/publication/263894641_On_the_Complexity_of_Neural_Network_Classifiers_A_Comparison_Between_Shallow_and_Deep_Architectures},
  urldate = {2025-11-16},
  abstract = {Request PDF | On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures | Recently, researchers in the artificial neural network field have focused their attention on connectionist models composed by several hidden... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\FVQC3U3J\263894641_On_the_Complexity_of_Neural_Network_Classifiers_A_Comparison_Between_Shallow_and_Deep.html}
}

@online{ComputationalHomologyGoogle2025,
  title = {Computational Homology - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=computational+homology&oq=computational+homology&gs_lcrp=EgZjaHJvbWUqCggAEAAY4wIYgAQyCggAEAAY4wIYgAQyBwgBEC4YgAQyBwgCEAAYgAQyCAgDEAAYFhgeMggIBBAAGBYYHjIICAUQABgWGB4yDQgGEAAYhgMYgAQYigUyDQgHEAAYhgMYgAQYigUyDQgIEAAYhgMYgAQYigUyBwgJEAAY7wUyCggKEAAYgAQYogQyBwgLEAAY7wUyBwgMECEYjwLSAQkxMDEyMmowajeoAg-wAgHxBbXf_mzN_KYN&client=ms-android-samsung-ss&sourceid=chrome-mobile&ie=UTF-8#ebo=0},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\CUA4BVQA\search.html}
}

@inreference{ConvexConjugate2025,
  title = {Convex Conjugate},
  booktitle = {Wikipedia},
  date = {2025-10-15T21:49:59Z},
  url = {https://en.wikipedia.org/w/index.php?title=Convex_conjugate&oldid=1317023829},
  urldate = {2025-11-12},
  abstract = {In mathematics and mathematical optimization, the convex conjugate of a function is a generalization of the Legendre transformation which applies to non-convex functions. It is also known as Legendre–Fenchel transformation, Fenchel transformation, or Fenchel conjugate (after Adrien-Marie Legendre and Werner Fenchel).  The convex conjugate is widely used for constructing the dual problem in optimization theory, thus generalizing Lagrangian duality.},
  langid = {english},
  annotation = {Page Version ID: 1317023829},
  file = {C:\Users\Aymen\Zotero\storage\LS2YI3EC\index.html}
}

@online{ConvexOptimizationBoyd2025,
  title = {Convex {{Optimization}} – {{Boyd}} and {{Vandenberghe}}},
  date = {2025-11-13},
  url = {https://stanford.edu/~boyd/cvxbook/},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\T2BUEEZ2\cvxbook.html}
}

@online{corneanuComputingTestingError2020,
  title = {Computing the {{Testing Error}} without a {{Testing Set}}},
  author = {Corneanu, Ciprian and Madadi, Meysam and Escalera, Sergio and Martinez, Aleix},
  date = {2020-05-01},
  eprint = {2005.00450},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.00450},
  url = {http://arxiv.org/abs/2005.00450},
  urldate = {2025-11-16},
  abstract = {Deep Neural Networks (DNNs) have revolutionized computer vision. We now have DNNs that achieve top (performance) results in many problems, including object recognition, facial expression analysis, and semantic segmentation, to name but a few. The design of the DNNs that achieve top results is, however, non-trivial and mostly done by trail-and-error. That is, typically, researchers will derive many DNN architectures (i.e., topologies) and then test them on multiple datasets. However, there are no guarantees that the selected DNN will perform well in the real world. One can use a testing set to estimate the performance gap between the training and testing sets, but avoiding overfitting-to-the-testing-data is almost impossible. Using a sequestered testing dataset may address this problem, but this requires a constant update of the dataset, a very expensive venture. Here, we derive an algorithm to estimate the performance gap between training and testing that does not require any testing dataset. Specifically, we derive a number of persistent topology measures that identify when a DNN is learning to generalize to unseen samples. This allows us to compute the DNN's testing error on unseen samples, even when we do not have access to them. We provide extensive experimental validation on multiple networks and datasets to demonstrate the feasibility of the proposed approach.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\JZQT4I5T\\Corneanu et al. - 2020 - Computing the Testing Error without a Testing Set.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\9S582V9C\\2005.html}
}

@inproceedings{corneanuWhatDoesIt2019,
  title = {What {{Does It Mean}} to {{Learn}} in {{Deep Networks}}? {{And}}, {{How Does One Detect Adversarial Attacks}}?},
  shorttitle = {What {{Does It Mean}} to {{Learn}} in {{Deep Networks}}?},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Corneanu, Ciprian A. and Madadi, Meysam and Escalera, Sergio and Martinez, Aleix M.},
  date = {2019-06},
  pages = {4752--4761},
  publisher = {IEEE},
  location = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00489},
  url = {https://ieeexplore.ieee.org/document/8953424/},
  urldate = {2025-11-16},
  abstract = {The flexibility and high-accuracy of Deep Neural Networks (DNNs) has transformed computer vision. But, the fact that we do not know when a specific DNN will work and when it will fail has resulted in a lack of trust. A clear example is self-driving cars; people are uncomfortable sitting in a car driven by algorithms that may fail under some unknown, unpredictable conditions. Interpretability and explainability approaches attempt to address this by uncovering what a DNN models, i.e., what each node (cell) in the network represents and what images are most likely to activate it. This can be used to generate, for example, adversarial attacks. But these approaches do not generally allow us to determine where a DNN will succeed or fail and why. i.e., does this learned representation generalize to unseen samples? Here, we derive a novel approach to define what it means to learn in deep networks, and how to use this knowledge to detect adversarial attacks. We show how this defines the ability of a network to generalize to unseen testing samples and, most importantly, why this is the case.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-7281-3293-8},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\DI9UUAHR\Corneanu et al. - 2019 - What Does It Mean to Learn in Deep Networks And, How Does One Detect Adversarial Attacks.pdf}
}

@article{crawProjectiveToricVarieties2008,
  title = {Projective Toric Varieties as Fine Moduli Spaces of Quiver Representations},
  author = {Craw, Alastair and Smith, Gregory G.},
  date = {2008-12},
  journaltitle = {ajm},
  volume = {130},
  number = {6},
  eprint = {math/0608183},
  eprinttype = {arXiv},
  pages = {1509--1534},
  issn = {1080-6377},
  doi = {10.1353/ajm.0.0027},
  url = {http://arxiv.org/abs/math/0608183},
  urldate = {2024-09-20},
  abstract = {This paper proves that every projective toric variety is the fine moduli space for stable representations of an appropriate bound quiver. To accomplish this, we study the quiver \$Q\$ with relations \$R\$ corresponding to the finite-dimensional algebra \$\textbackslash bigl(\textbackslash bigoplus\_\{i=0\}\textasciicircum\{r\} L\_i \textbackslash bigr)\$ where \$\textbackslash mathcal\{L\} := (\textbackslash mathscr\{O\}\_X,L\_1, ...c, L\_r)\$ is a list of line bundles on a projective toric variety \$X\$. The quiver \$Q\$ defines a smooth projective toric variety, called the multilinear series \$|\textbackslash mathcal\{L\}|\$, and a map \$X \textbackslash to |\textbackslash mathcal\{L\}|\$. We provide necessary and sufficient conditions for the induced map to be a closed embedding. As a consequence, we obtain a new geometric quotient construction of projective toric varieties. Under slightly stronger hypotheses on \$\textbackslash mathcal\{L\}\$, the closed embedding identifies \$X\$ with the fine moduli space of stable representations for the bound quiver \$(Q,R)\$.},
  keywords = {Mathematics - Algebraic Geometry,Mathematics - Representation Theory,Quivers},
  note = {Comment: revised version: improved exposition, corrected typos and other minor changes}
}

@online{CS6170Spring2024,
  title = {CS 6170 Spring 2021 Lectures - YouTube},
  date = {2024-09-16},
  url = {https://www.youtube.com/},
  urldate = {2024-09-16},
  abstract = {Profitez des vidéos et de la musique que vous aimez, mettez en ligne des contenus originaux, et partagez-les avec vos amis, vos proches et le monde entier.},
  langid = {french},
  keywords = {Cech,Homology,Rips,Simplicial Complex,TDA}
}

@article{cubukIdentifyingStructuralFlow2015,
  title = {Identifying {{Structural Flow Defects}} in {{Disordered Solids Using Machine-Learning Methods}}},
  author = {Cubuk, E. D. and Schoenholz, S. S. and Rieser, J. M. and Malone, B. D. and Rottler, J. and Durian, D. J. and Kaxiras, E. and Liu, A. J.},
  date = {2015-03-09},
  journaltitle = {Phys. Rev. Lett.},
  volume = {114},
  number = {10},
  pages = {108001},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.114.108001},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.114.108001},
  urldate = {2025-11-13},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\QXDMSVNM\Cubuk et al. - 2015 - Identifying Structural Flow Defects in Disordered Solids Using Machine-Learning Methods.pdf}
}

@article{cubukStructurepropertyRelationshipsUniversal2017,
  title = {Structure-Property Relationships from Universal Signatures of Plasticity in Disordered Solids},
  author = {Cubuk, E. D. and Ivancic, R. J. S. and Schoenholz, S. S. and Strickland, D. J. and Basu, A. and Davidson, Z. S. and Fontaine, J. and Hor, J. L. and Huang, Y.-R. and Jiang, Y. and Keim, N. C. and Koshigan, K. D. and Lefever, J. A. and Liu, T. and Ma, X.-G. and Magagnosc, D. J. and Morrow, E. and Ortiz, C. P. and Rieser, J. M. and Shavit, A. and Still, T. and Xu, Y. and Zhang, Y. and Nordstrom, K. N. and Arratia, P. E. and Carpick, R. W. and Durian, D. J. and Fakhraai, Z. and Jerolmack, D. J. and Lee, Daeyeon and Li, Ju and Riggleman, R. and Turner, K. T. and Yodh, A. G. and Gianola, D. S. and Liu, Andrea J.},
  date = {2017-11-24},
  journaltitle = {Science},
  volume = {358},
  number = {6366},
  pages = {1033--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aai8830},
  url = {https://www.science.org/doi/10.1126/science.aai8830},
  urldate = {2025-11-13},
  abstract = {Behavioral universality across size scales                            Glassy materials are characterized by a lack of long-range order, whether at the atomic level or at much larger length scales. But to what extent is their commonality in the behavior retained at these different scales? Cubuk               et al.               used experiments and simulations to show universality across seven orders of magnitude in length. Particle rearrangements in such systems are mediated by defects that are on the order of a few particle diameters. These rearrangements correlate with the material's softness and yielding behavior.                                         Science               , this issue p.               1033                        ,              A range of particle-based and glassy systems show universal features of the onset of plasticity and a universal yield strain.           ,              When deformed beyond their elastic limits, crystalline solids flow plastically via particle rearrangements localized around structural defects. Disordered solids also flow, but without obvious structural defects. We link structure to plasticity in disordered solids via a microscopic structural quantity, “softness,” designed by machine learning to be maximally predictive of rearrangements. Experimental results and computations enabled us to measure the spatial correlations and strain response of softness, as well as two measures of plasticity: the size of rearrangements and the yield strain. All four quantities maintained remarkable commonality in their values for disordered packings of objects ranging from atoms to grains, spanning seven orders of magnitude in diameter and 13 orders of magnitude in elastic modulus. These commonalities link the spatial correlations and strain response of softness to rearrangement size and yield strain, respectively.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\3MCUXRU4\Cubuk et al. - 2017 - Structure-property relationships from universal signatures of plasticity in disordered solids.pdf}
}

@article{cuneoDeepLearningMethod2023,
  title = {A Deep Learning Method to Separate Fluorophores Based on Their Fluorescence Lifetime},
  author = {Cuneo, Lisa and Castello, Marco and Piazza, Simonluca and Nepita, Irene and Lanzano', Luca and Bianchini, Paolo and Vicidomini, Giuseppe and Diaspro, Alberto},
  date = {2023-02},
  journaltitle = {Biophysical Journal},
  volume = {122},
  number = {3},
  pages = {462a-463a},
  issn = {00063495},
  doi = {10.1016/j.bpj.2022.11.2483},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0006349522033999},
  urldate = {2024-11-05},
  langid = {english}
}

@inproceedings{daiManifoldMatchingDeep2021,
  title = {Manifold {{Matching}} via {{Deep Metric Learning}} for {{Generative Modeling}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Dai, Mengyu and Hang, Haibin},
  date = {2021-10},
  pages = {6567--6577},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00652},
  url = {https://ieeexplore.ieee.org/document/9709635/},
  urldate = {2024-09-24},
  abstract = {We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a highdimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and p-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks learn simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-6654-2812-5},
  langid = {english}
}

@online{daiManifoldMatchingDeep2021a,
  title = {Manifold {{Matching}} via {{Deep Metric Learning}} for {{Generative Modeling}}},
  author = {Dai, Mengyu and Hang, Haibin},
  date = {2021-08-26},
  eprint = {2106.10777},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.10777},
  url = {http://arxiv.org/abs/2106.10777},
  urldate = {2024-09-24},
  abstract = {We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and \$p\$-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ICCV 2021. Code available at https://github.com/dzld00/pytorch-manifold-matching.git}
}

@software{DairaiTransformersRecipe2024,
  title = {Dair-Ai/{{Transformers-Recipe}}},
  date = {2024-09-20T09:18:42Z},
  origdate = {2021-12-18T10:44:13Z},
  url = {https://github.com/dair-ai/Transformers-Recipe},
  urldate = {2024-09-20},
  abstract = {🧠 A study guide to learn about Transformers},
  organization = {DAIR.AI},
  keywords = {ai,deep-learning,machine-learning,natural-language-processing,nlp,Transformer Ressources}
}

@online{daoTransformersAreSSMs2024,
  title = {Transformers Are {{SSMs}}: {{Generalized Models}} and {{Efficient Algorithms Through Structured State Space Duality}}},
  shorttitle = {Transformers Are {{SSMs}}},
  author = {Dao, Tri and Gu, Albert},
  date = {2024-05-31},
  eprint = {2405.21060},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.21060},
  url = {http://arxiv.org/abs/2405.21060},
  urldate = {2025-11-16},
  abstract = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: ICML 2024},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\KEUV78UW\\Dao et Gu - 2024 - Transformers are SSMs Generalized Models and Efficient Algorithms Through Structured State Space Du.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\6L7VW7YE\\2405.html}
}

@online{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2572},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1406.2572},
  urldate = {2024-09-17},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  note = {Comment: The theoretical review and analysis in this article draw heavily from arXiv:1405.4604 [cs.LG]}
}

@online{deGriffinMixingGated2024,
  title = {Griffin: {{Mixing Gated Linear Recurrences}} with {{Local Attention}} for {{Efficient Language Models}}},
  shorttitle = {Griffin},
  author = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and Freitas, Nando De and Gulcehre, Caglar},
  date = {2024-02-29},
  eprint = {2402.19427},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.19427},
  url = {http://arxiv.org/abs/2402.19427},
  urldate = {2025-11-16},
  abstract = {Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 25 pages, 11 figures},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\LS2K8PXY\\De et al. - 2024 - Griffin Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\QZTYDM6R\\2402.html}
}

@online{denilPredictingParametersDeep2014,
  title = {Predicting {{Parameters}} in {{Deep Learning}}},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2014-10-27},
  eprint = {1306.0543},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1306.0543},
  url = {http://arxiv.org/abs/1306.0543},
  urldate = {2024-09-20},
  abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@online{DeterministicMatrixTheory2025,
  title = {Deterministic Matrix Theory - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=deterministic+matrix+theory&oq=deterministic++matrix+theory&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCTE2ODI0ajBqN6gCD7ACAfEFMuVuYvAhOnE&client=ms-android-samsung-ss&sourceid=chrome-mobile&ie=UTF-8},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\EIHBYTCT\search.html}
}

@online{deutschGeneratingNeuralNetworks2018,
  title = {Generating {{Neural Networks}} with {{Neural Networks}}},
  author = {Deutsch, Lior},
  date = {2018-04-06},
  eprint = {1801.01952},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1801.01952},
  url = {http://arxiv.org/abs/1801.01952},
  urldate = {2024-09-20},
  abstract = {Hypernetworks are neural networks that generate weights for another neural network. We formulate the hypernetwork training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We explain how this simple formulation generalizes variational inference. We use multi-layered perceptrons to form the mapping from the low dimensional input random vector to the high dimensional weight space, and demonstrate how to reduce the number of parameters in this mapping by parameter sharing. We perform experiments and show that the generated weights are diverse and lie on a non-trivial manifold.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Generating NN,Statistics - Machine Learning}
}

@article{diasproEmergingMuellerMatrix2023,
  title = {Emerging {{Mueller}} Matrix Microscopy Applications in Biophysics and Biomedicine},
  author = {Diaspro, Alberto and Bianchini, Paolo and Callegari, Fabio and Cuneo, Lisa and Marongiu, Riccardo and Le Gratiet, Aymeric and Mohebi, Ali and Scotto, M. and Sheppard, Colin J. R.},
  date = {2023-08},
  journaltitle = {Riv. Nuovo Cim.},
  volume = {46},
  number = {8},
  pages = {473--519},
  issn = {0393-697X, 1826-9850},
  doi = {10.1007/s40766-023-00046-5},
  url = {https://link.springer.com/10.1007/s40766-023-00046-5},
  urldate = {2024-11-05},
  abstract = {Abstract             Polarized and wide-field light microscopy has been studied for many years to develop accurate and information-rich images within a focused framework on biophysics and biomedicine. Technological advances and conceptual understanding have recently led to significant results in terms of applications. Simultaneously, developments in label-free methods are opening a new window on molecular imaging at a low dose of illumination. The ability to encode and decode polarized light pixel by pixel, coupled with the computational strength provided by artificial intelligence, is the running perspective of label-free optical microscopy. More specifically, the information-rich content Mueller matrix microscopy through its 16 elements offers multimodal imaging, an original data set to be integrated with other advanced optical methods. This dilates the spectrum of possible and potential applications. Here, we explore the recent advances in basic and applied research towards technological applications tailored for specific questions in biophysics and biomedicine.},
  langid = {english}
}

@online{diaz-rodriguezDontForgetThere2018,
  title = {Don't Forget, There Is More than Forgetting: New Metrics for {{Continual Learning}}},
  shorttitle = {Don't Forget, There Is More than Forgetting},
  author = {Díaz-Rodríguez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
  date = {2018-10-31},
  eprint = {1810.13166},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.13166},
  url = {http://arxiv.org/abs/1810.13166},
  urldate = {2024-09-20},
  abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
  pubstate = {prepublished},
  keywords = {68T05 cs.LG cs.AI cs.CV cs.NE stat.ML,CL,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@online{diaz-rodriguezDontForgetThere2018a,
  title = {Don't Forget, There Is More than Forgetting: New Metrics for {{Continual Learning}}},
  shorttitle = {Don't Forget, There Is More than Forgetting},
  author = {Díaz-Rodríguez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
  date = {2018-10-31},
  eprint = {1810.13166},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.13166},
  url = {http://arxiv.org/abs/1810.13166},
  urldate = {2025-11-16},
  abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\Aymen\Zotero\storage\DQ6MT67H\Díaz-Rodríguez et al. - 2018 - Don't forget, there is more than forgetting new metrics for Continual Learning.pdf}
}

@inreference{DirichletBoundaryCondition2025,
  title = {Dirichlet Boundary Condition},
  booktitle = {Wikipedia},
  date = {2025-10-24T21:27:16Z},
  url = {https://en.wikipedia.org/w/index.php?title=Dirichlet_boundary_condition&oldid=1318601261},
  urldate = {2025-11-12},
  abstract = {In mathematics, the Dirichlet boundary condition is imposed on an ordinary or partial differential equation, such that the values that the solution takes along the boundary of the domain are fixed. The question of finding solutions to such equations is known as the Dirichlet problem. In the sciences and engineering, a Dirichlet boundary condition may also be referred to as a fixed boundary condition or boundary condition of the first type. It is named after Peter Gustav Lejeune Dirichlet (1805–1859). In finite-element analysis, the essential or Dirichlet boundary condition is defined by weighted-integral form of a differential equation. The dependent unknown u in the same form as the weight function w appearing in the boundary expression is termed a primary variable, and its specification constitutes the essential or Dirichlet boundary condition.},
  langid = {english},
  annotation = {Page Version ID: 1318601261},
  file = {C:\Users\Aymen\Zotero\storage\SQI5HMPJ\index.html}
}

@article{dixonFourloopRemainderFunction2014,
  title = {The Four-Loop Remainder Function and Multi-{{Regge}} Behavior at {{NNLLA}} in Planar \$ \textbackslash mathcal\{{{N}}\} \$ = 4 Super-{{Yang-Mills}} Theory},
  author = {Dixon, Lance J. and Drummond, James M. and Duhr, Claude and Pennington, Jeffrey},
  date = {2014-06},
  journaltitle = {J. High Energ. Phys.},
  volume = {2014},
  number = {6},
  pages = {116},
  issn = {1029-8479},
  doi = {10.1007/JHEP06(2014)116},
  url = {http://link.springer.com/10.1007/JHEP06(2014)116},
  urldate = {2025-11-13},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\R529G6D8\Dixon et al. - 2014 - The four-loop remainder function and multi-Regge behavior at NNLLA in planar $ mathcal N $ = 4 sup.pdf}
}

@article{dixonHexagonFunctionsThreeloop2013,
  title = {Hexagon Functions and the Three-Loop Remainder Function},
  author = {Dixon, Lance J. and Drummond, James M. and Von Hippel, Matt and Pennington, Jeffrey},
  date = {2013-12},
  journaltitle = {J. High Energ. Phys.},
  volume = {2013},
  number = {12},
  pages = {49},
  issn = {1029-8479},
  doi = {10.1007/JHEP12(2013)049},
  url = {https://link.springer.com/10.1007/JHEP12(2013)049},
  urldate = {2025-11-13},
  abstract = {A               bstract                                         We present the three-loop remainder function, which describes the scattering of six gluons in the maximally-helicity-violating configuration in planar                                \$ \textbackslash mathcal\{N\} \$                              = 4 super-Yang-Mills theory, as a function of the three dual conformal cross ratios. The result can be expressed in terms of multiple Goncharov polylogarithms. We also employ a more restricted class of               hexagon functions               which have the correct branch cuts and certain other restrictions on their symbols. We classify all the hexagon functions through transcendental weight five, using the coproduct for their Hopf algebra iteratively, which amounts to a set of first-order differential equations. The three-loop remainder function is a particular weight-six hexagon function, whose symbol was determined previously. The differential equations can be integrated numerically for generic values of the cross ratios, or analytically in certain kinematic limits, including the near-collinear and multi-Regge limits. These limits allow us to impose constraints from the operator product expansion and multi-Regge factorization directly at the function level, and thereby to fix uniquely a set of Riemann               ζ               valued constants that could not be fixed at the level of the symbol. The near-collinear limits agree precisely with recent predictions by Basso, Sever and Vieira based on integrability. The multi-Regge limits agree with the factorization formula of Fadin and Lipatov, and determine three constants entering the impact factor at this order. We plot the three-loop remainder function for various slices of the Euclidean region of positive cross ratios, and compare it to the two-loop one. For large ranges of the cross ratios, the ratio of the three-loop to the two-loop remainder function is relatively constant, and close to −7.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\7L7GFXQW\Dixon et al. - 2013 - Hexagon functions and the three-loop remainder function.pdf}
}

@online{draxlerEssentiallyNoBarriers2019,
  title = {Essentially {{No Barriers}} in {{Neural Network Energy Landscape}}},
  author = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A.},
  date = {2019-02-22},
  eprint = {1803.00885},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1803.00885},
  url = {http://arxiv.org/abs/1803.00885},
  urldate = {2024-09-17},
  abstract = {Training neural networks involves finding minima of a high-dimensional non-convex loss function. Knowledge of the structure of this energy landscape is sparse. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that neural networks have enough capacity for structural changes, or that these changes are small between minima. Also, each minimum has at least one vanishing Hessian eigenvalue in addition to those resulting from trivial invariance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Loss Landscape,minima transition,Statistics - Machine Learning},
  note = {Comment: In Proceedings of 35th International Conference on Machine Learning (ICML 2018)}
}

@online{dupuisGeneralizationBoundsDatadependent2023,
  title = {Generalization {{Bounds}} with {{Data-dependent Fractal Dimensions}}},
  author = {Dupuis, Benjamin and Deligiannidis, George and Şimşekli, Umut},
  date = {2023-07-10},
  eprint = {2302.02766},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2302.02766},
  url = {http://arxiv.org/abs/2302.02766},
  urldate = {2025-11-16},
  abstract = {Providing generalization guarantees for modern neural networks has been a crucial task in statistical learning. Recently, several studies have attempted to analyze the generalization error in such settings by using tools from fractal geometry. While these works have successfully introduced new mathematical tools to apprehend generalization, they heavily rely on a Lipschitz continuity assumption, which in general does not hold for neural networks and might make the bounds vacuous. In this work, we address this issue and prove fractal geometry-based generalization bounds without requiring any Lipschitz assumption. To achieve this goal, we build up on a classical covering argument in learning theory and introduce a data-dependent fractal dimension. Despite introducing a significant amount of technical complications, this new notion lets us control the generalization error (over either fixed or random hypothesis spaces) along with certain mutual information (MI) terms. To provide a clearer interpretation to the newly introduced MI terms, as a next step, we introduce a notion of "geometric stability" and link our bounds to the prior art. Finally, we make a rigorous connection between the proposed data-dependent dimension and topological data analysis tools, which then enables us to compute the dimension in a numerically efficient way. We support our theory with experiments conducted on various settings.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\ZICCAWZW\\Dupuis et al. - 2023 - Generalization Bounds with Data-dependent Fractal Dimensions.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\3MRQKFDX\\2302.html}
}

@inreference{EigenvaluesEigenvectors2025,
  title = {Eigenvalues and Eigenvectors},
  booktitle = {Wikipedia},
  date = {2025-11-11T18:52:51Z},
  url = {https://en.wikipedia.org/w/index.php?title=Eigenvalues_and_eigenvectors&oldid=1321634633},
  urldate = {2025-11-13},
  abstract = {In linear algebra, an eigenvector ( EYE-gən-) or characteristic vector is a (nonzero) vector that has its direction unchanged (or reversed) by a given linear transformation. More precisely, an eigenvector                                    v                          \{\textbackslash displaystyle \textbackslash mathbf \{v\} \}     of a linear transformation                         T                 \{\textbackslash displaystyle T\}     is scaled by a constant factor                         λ                 \{\textbackslash displaystyle \textbackslash lambda \}     when the linear transformation is applied to it:                         T                    v                  =         λ                    v                          \{\textbackslash displaystyle T\textbackslash mathbf \{v\} =\textbackslash lambda \textbackslash mathbf \{v\} \}    . The corresponding eigenvalue, characteristic value, or characteristic root is the multiplying factor                         λ                 \{\textbackslash displaystyle \textbackslash lambda \}     (possibly a negative or complex number). Geometrically, vectors are multi-dimensional quantities with magnitude and direction, often pictured as arrows. A linear transformation rotates, stretches, or shears the vectors upon which it acts. A linear transformation's eigenvectors are those vectors that are only stretched or shrunk, with neither rotation nor shear. The corresponding eigenvalue is the factor by which an eigenvector is stretched or shrunk. If the eigenvalue is negative, the eigenvector's direction is reversed. The eigenvectors and eigenvalues of a linear transformation serve to characterize it, and so they play important roles in all areas where linear algebra is applied, from geology to quantum mechanics. In particular, it is often the case that a system is represented by a linear transformation whose outputs are fed as inputs to the same transformation (feedback).  In such an application, the largest eigenvalue is of particular importance, because it governs the long-term behavior of the system after many applications of the linear transformation, and the associated eigenvector is the steady state of the system.},
  langid = {english},
  annotation = {Page Version ID: 1321634633},
  file = {C:\Users\Aymen\Zotero\storage\KWFEWK6A\index.html}
}

@online{el-noubyScalablePretrainingLarge2024,
  title = {Scalable {{Pre-training}} of {{Large Autoregressive Image Models}}},
  author = {El-Nouby, Alaaeldin and Klein, Michal and Zhai, Shuangfei and Bautista, Miguel Angel and Toshev, Alexander and Shankar, Vaishaal and Susskind, Joshua M. and Joulin, Armand},
  date = {2024-01-16},
  eprint = {2401.08541},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.08541},
  url = {http://arxiv.org/abs/2401.08541},
  urldate = {2024-09-20},
  abstract = {This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2 billion images, that achieves 84.0\% on ImageNet-1k with a frozen trunk. Interestingly, even at this scale, we observe no sign of saturation in performance, suggesting that AIM potentially represents a new frontier for training large-scale vision models. The pre-training of AIM is similar to the pre-training of LLMs, and does not require any image-specific strategy to stabilize the training at scale.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Vision LLM},
  note = {Comment: https://github.com/apple/ml-aim}
}

@online{EnfpGoogleSearch2025,
  title = {Enfp - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=enfp&client=ms-android-samsung-ss&sca_esv=e04a1f5e3e413b25&sxsrf=AE3TifMDjXBNnsiob2B3uenJJZRA8RDZ_Q%3A1758959873619&ei=AZnXaPbAJcmvkdUP1-uqqAU&oq=enfp&gs_lp=EhNtb2JpbGUtZ3dzLXdpei1zZXJwIgRlbmZwMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgUQABiABDIFEAAYgAQyBRAAGIAESPgdUPALWL4ZcAF4AZABAJgB2QKgAYoHqgEHMC4yLjEuMbgBA8gBAPgBAZgCBKACmgbCAgoQABiwAxjWBBhHwgINEAAYgAQYsAMYQxiKBcICChAjGIAEGCcYigXCAgQQIxgnwgILEC4YgAQYxwEYrwGYAwCIBgGQBgySBwcxLjEuMS4xoAehGrIHBzAuMS4xLjG4B4AGwgcFMy0yLjLIB0Y&sclient=mobile-gws-wiz-serp},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\SR9D6AL5\search.html}
}

@inreference{Equicontinuity2025,
  title = {Equicontinuity},
  booktitle = {Wikipedia},
  date = {2025-09-19T16:42:36Z},
  url = {https://en.wikipedia.org/w/index.php?title=Equicontinuity&oldid=1312263178},
  urldate = {2025-11-12},
  abstract = {In mathematical analysis, a family of functions is equicontinuous if all the functions are continuous and they have equal variation over a given neighbourhood, in a precise sense described herein.  In particular, the concept applies to countable families, and thus sequences of functions. Equicontinuity appears in the formulation of Ascoli's theorem, which states that a subset of C(X), the space of continuous functions on a compact Hausdorff space X, is compact if and only if it is closed, pointwise bounded and equicontinuous.  As a corollary, a sequence in C(X) is uniformly convergent if and only if it is equicontinuous and converges pointwise to a function (not necessarily continuous a-priori).  In particular, the limit of an equicontinuous pointwise convergent sequence of continuous functions fn on either a metric space or a locally compact space is continuous. If, in addition, fn are holomorphic, then the limit is also holomorphic. The uniform boundedness principle states that a pointwise bounded family of continuous linear operators between Banach spaces is equicontinuous.},
  langid = {english},
  annotation = {Page Version ID: 1312263178},
  file = {C:\Users\Aymen\Zotero\storage\Q8VTVHE7\index.html}
}

@article{faberPredictionErrorsMolecular2017,
  title = {Prediction {{Errors}} of {{Molecular Machine Learning Models Lower}} than {{Hybrid DFT Error}}},
  author = {Faber, Felix A. and Hutchison, Luke and Huang, Bing and Gilmer, Justin and Schoenholz, Samuel S. and Dahl, George E. and Vinyals, Oriol and Kearnes, Steven and Riley, Patrick F. and Von Lilienfeld, O. Anatole},
  date = {2017-11-14},
  journaltitle = {J. Chem. Theory Comput.},
  volume = {13},
  number = {11},
  pages = {5255--5264},
  issn = {1549-9618, 1549-9626},
  doi = {10.1021/acs.jctc.7b00577},
  url = {https://pubs.acs.org/doi/10.1021/acs.jctc.7b00577},
  urldate = {2025-11-13},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\UFQ57PU4\Faber et al. - 2017 - Prediction Errors of Molecular Machine Learning Models Lower than Hybrid DFT Error.pdf}
}

@online{FederatedLearningApplied2024,
  title = {Federated Learning | Applied Deep Learning},
  date = {2024-09-20},
  url = {http://www.youtube.com/playlist?list=PLoEMreTa9CNmPnHffTFkPpbCJWpzpm4ln},
  urldate = {2024-09-20},
  abstract = {Course Materials: https://github.com/maziarraissi/Applied-Deep-Learning},
  langid = {french},
  organization = {YouTube},
  keywords = {Federated Learning}
}

@online{fedusSwitchTransformersScaling2022,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  date = {2022-06-16},
  eprint = {2101.03961},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2101.03961},
  url = {http://arxiv.org/abs/2101.03961},
  urldate = {2025-11-16},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: JMLR},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\QMU9EH74\\Fedus et al. - 2022 - Switch Transformers Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\LRGBX2HE\\2101.html}
}

@online{FiniteElementMethods2025,
  title = {Finite Element Methods to Solve {{PDEs}} - {{Google Search}}},
  date = {2025-11-12},
  url = {https://www.google.com/search?client=firefox-b-d&q=finite+element+methods+to+solve+PDEs},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\3MSN885G\search.html}
}

@online{FisherinformationmatrixMnist_fimpyMaster2024,
  title = {Fisher-Information-Matrix/Mnist\_fim.Py at Master · Tudor-Berariu/Fisher-Information-Matrix},
  date = {2024-09-20},
  url = {https://github.com/tudor-berariu/fisher-information-matrix/blob/master/mnist_fim.py},
  urldate = {2024-09-20},
  abstract = {PyTorch implementation of FIM and empirical FIM. Contribute to tudor-berariu/fisher-information-matrix development by creating an account on GitHub.},
  langid = {english},
  organization = {GitHub},
  keywords = {FIM}
}

@article{fortGoldilocksZoneBetter2018,
  title = {The {{Goldilocks}} Zone: {{Towards}} Better Understanding of Neural Network Loss Landscapes},
  author = {Fort, Stanislav and Scherlis, Adam},
  date = {2018-07-01},
  pages = {arXiv:1807.02581},
  doi = {10.48550/arXiv.1807.02581},
  abstract = {We explore the loss landscape of fully-connected and convolutional neural networks using random, low-dimensional hyperplanes and hyperspheres. Evaluating the Hessian, \$H\$, of the loss function on these hypersurfaces, we observe 1) an unusual excess of the number of positive eigenvalues of \$H\$, and 2) a large value of \$\textbackslash mathrm\{Tr\}(H) / ||H||\$ at a well defined range of configuration space radii, corresponding to a thick, hollow, spherical shell we refer to as the \textbackslash textit\{Goldilocks zone\}. We observe this effect for fully-connected neural networks over a range of network widths and depths on MNIST and CIFAR-10 datasets with the \$\textbackslash mathrm\{ReLU\}\$ and \$\textbackslash tanh\$ non-linearities, and a similar effect for convolutional networks. Using our observations, we demonstrate a close connection between the Goldilocks zone, measures of local convexity/prevalence of positive curvature, and the suitability of a network initialization. We show that the high and stable accuracy reached when optimizing on random, low-dimensional hypersurfaces is directly related to the overlap between the hypersurface and the Goldilocks zone, and as a corollary demonstrate that the notion of intrinsic dimension is initialization-dependent. We note that common initialization techniques initialize neural networks in this particular region of unusually high convexity/prevalence of positive curvature, and offer a geometric intuition for their success. Furthermore, we demonstrate that initializing a neural network at a number of points and selecting for high measures of local convexity such as \$\textbackslash mathrm\{Tr\}(H) / ||H||\$, number of positive eigenvalues of \$H\$, or low initial loss, leads to statistically significantly faster training on MNIST. Based on our observations, we hypothesize that the Goldilocks zone contains an unusually high density of suitable initialization configurations.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and,Evolutionary Computing,Statistics - Machine Learning},
  note = {8 pages, 15 figures. Accepted for publication at the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19). A subset of the paper accepted at Modern Trends in Nonconvex Optimization for Machine Learning workshop at the 35th International Conference on Machine Learning (ICML 2018), and BayLearn 2018}
}

@article{framlingContextualImportanceUtility2024,
  title = {Contextual {{Importance}} and {{Utility}} in {{Python}}: {{New Functionality}} and {{Insights}} with the Py-Ciu {{Package}}},
  author = {Främling, Kary},
  date = {2024-08-01},
  pages = {arXiv:2408.09957},
  doi = {10.48550/arXiv.2408.09957},
  abstract = {The availability of easy-to-use and reliable software implementations is important for allowing researchers in academia and industry to test, assess and take into use eXplainable AI (XAI) methods. This paper describes the \textbackslash texttt\{py-ciu\} Python implementation of the Contextual Importance and Utility (CIU) model-agnostic, post-hoc explanation method and illustrates capabilities of CIU that go beyond the current state-of-the-art that could be useful for XAI practitioners in general.},
  keywords = {Computer Science - Artificial Intelligence,I.2.4},
  note = {In Proceedings of XAI 2024 Workshop of 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024), Jeju, South Corea}
}

@online{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2019-03-04},
  eprint = {1803.03635},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.03635},
  url = {http://arxiv.org/abs/1803.03635},
  urldate = {2024-09-20},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Sparsification},
  note = {Comment: ICLR camera ready}
}

@online{freemanTopologyGeometryHalfRectified2017,
  title = {Topology and {{Geometry}} of {{Half-Rectified Network Optimization}}},
  author = {Freeman, C. Daniel and Bruna, Joan},
  date = {2017-06-01},
  eprint = {1611.01540},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.01540},
  urldate = {2024-09-17},
  abstract = {The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Loss landscape,Loss surface,Statistics - Machine Learning,TDA},
  note = {Comment: 22 Pages (10 main + Appendices), 4 Figures, 1 Table, Published as a conference paper at ICLR 2017
\par
Comment: 22 Pages (10 main + Appendices), 4 Figures, 1 Table, Published as a conference paper at ICLR 2017}
}

@incollection{frenchCatastrophicForgettingConnectionist2006,
  title = {Catastrophic {{Forgetting}} in {{Connectionist Networks}}},
  booktitle = {Trends in {{Cognitive Sciences}} - {{TRENDS COGN SCI}}},
  author = {French, Robert},
  date = {2006-01-15},
  volume = {3},
  doi = {10.1002/0470018860.s00096},
  abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget ‘catastrophically’. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
  isbn = {978-0-470-01886-6},
  keywords = {CL}
}

@article{fukumizuLocalMinimaPlateaus2000,
  title = {Local Minima and Plateaus in Hierarchical Structures of Multilayer Perceptrons},
  author = {Fukumizu, K. and Amari, S.},
  date = {2000-04-01},
  journaltitle = {Neural Networks},
  volume = {13},
  number = {3},
  pages = {317--327},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(00)00009-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608000000095},
  urldate = {2024-09-17},
  abstract = {Local minima and plateaus pose a serious problem in learning of neural networks. We investigate the hierarchical geometric structure of the parameter space of three-layer perceptrons in order to show the existence of local minima and plateaus. It is proved that a critical point of the model with H−1 hidden units always gives many critical points of the model with H hidden units. These critical points consist of many lines in the parameter space, which can cause plateaus in learning of neural networks. Based on this result, we prove that a point in the critical lines corresponding to the global minimum of the smaller model can be a local minimum or a saddle point of the larger model. We give a necessary and sufficient condition for this, and show that this kind of local minima exist as a line segment if any. The results are universal in the sense that they do not require special properties of the target, loss functions and activation functions, but only use the hierarchical structure of the model.},
  keywords = {Error surface,Local minima,Loss Landscape,Multilayer perceptron,Plateau}
}

@online{gabellaTopologyLearningArtificial2019,
  title = {Topology of {{Learning}} in {{Artificial Neural Networks}}},
  author = {Gabella, Maxime},
  date = {2019-02-21},
  doi = {10.1109/TNNLS.2020.3015790},
  url = {https://arxiv.org/abs/1902.08160v4},
  urldate = {2024-09-20},
  abstract = {Understanding how neural networks learn remains one of the central challenges in machine learning research. From random at the start of training, the weights of a neural network evolve in such a way as to be able to perform a variety of tasks, like classifying images. Here we study the emergence of structure in the weights by applying methods from topological data analysis. We train simple feedforward neural networks on the MNIST dataset and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along two-dimensional surfaces. We show that natural coordinates on these learning surfaces correspond to important factors of variation.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {TDA,Weight generation}
}

@article{gabellaTopologyLearningFeedforward2021,
  title = {Topology of {{Learning}} in {{Feedforward Neural Networks}}},
  author = {Gabella, Maxime},
  date = {2021-08},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {8},
  pages = {3588--3592},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3015790},
  url = {https://ieeexplore.ieee.org/document/9174770},
  urldate = {2025-11-16},
  abstract = {Understanding how neural networks learn remains one of the central challenges in machine learning research. From random at the start of training, the weights of a neural network evolve in such a way as to be able to perform a variety of tasks, such as classifying images. Here, we study the emergence of structure in the weights by applying methods from topological data analysis. We train simple feedforward neural networks on the MNIST data set and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along 2-D surfaces. We show that natural coordinates on these learning surfaces correspond to important factors of variation.},
  keywords = {Bifurcation,Biological neural networks,feedforward neural networks,learning (artificial intelligence),machine learning,Network topology,neural networks,Neurons,Principal component analysis,Shape,Three-dimensional displays,topology,Training},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\C98A3RRP\\Gabella - 2021 - Topology of Learning in Feedforward Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\SGPAB97J\\9174770.html}
}

@online{gabrielssonExpositionInterpretationTopology2019,
  title = {Exposition and {{Interpretation}} of the {{Topology}} of {{Neural Networks}}},
  author = {Gabrielsson, Rickard Brüel and Carlsson, Gunnar},
  date = {2019-10-18},
  eprint = {1810.03234},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.03234},
  url = {http://arxiv.org/abs/1810.03234},
  urldate = {2024-09-20},
  abstract = {Convolutional neural networks (CNN's) are powerful and widely used tools. However, their interpretability is far from ideal. One such shortcoming is the difficulty of deducing a network's ability to generalize to unseen data. We use topological data analysis to show that the information encoded in the weights of a CNN can be organized in terms of a topological data model and demonstrate how such information can be interpreted and utilized. We show that the weights of convolutional layers at depths from 1 through 13 learn simple global structures. We also demonstrate the change of the simple structures over the course of training. In particular, we define and analyze the spaces of spatial filters of convolutional layers and show the recurrence, among all networks, depths, and during training, of a simple circle consisting of rotating edges, as well as a less recurring unanticipated complex circle that combines lines, edges, and non-linear patterns. We also demonstrate that topological structure correlates with a network's ability to generalize to unseen data and that topological information can be used to improve a network's performance. We train over a thousand CNN's on MNIST, CIFAR-10, SVHN, and ImageNet.},
  pubstate = {prepublished},
  keywords = {Carlsson,Computer Science - Computer Vision and Pattern Recognition,TDA}
}

@online{gabrielssonExpositionInterpretationTopology2019a,
  title = {Exposition and {{Interpretation}} of the {{Topology}} of {{Neural Networks}}},
  author = {Gabrielsson, Rickard Brüel and Carlsson, Gunnar},
  date = {2019-10-18},
  eprint = {1810.03234},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.03234},
  url = {http://arxiv.org/abs/1810.03234},
  urldate = {2025-11-16},
  abstract = {Convolutional neural networks (CNN's) are powerful and widely used tools. However, their interpretability is far from ideal. One such shortcoming is the difficulty of deducing a network's ability to generalize to unseen data. We use topological data analysis to show that the information encoded in the weights of a CNN can be organized in terms of a topological data model and demonstrate how such information can be interpreted and utilized. We show that the weights of convolutional layers at depths from 1 through 13 learn simple global structures. We also demonstrate the change of the simple structures over the course of training. In particular, we define and analyze the spaces of spatial filters of convolutional layers and show the recurrence, among all networks, depths, and during training, of a simple circle consisting of rotating edges, as well as a less recurring unanticipated complex circle that combines lines, edges, and non-linear patterns. We also demonstrate that topological structure correlates with a network's ability to generalize to unseen data and that topological information can be used to improve a network's performance. We train over a thousand CNN's on MNIST, CIFAR-10, SVHN, and ImageNet.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\KURGFRCP\\Gabrielsson et Carlsson - 2019 - Exposition and Interpretation of the Topology of Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\NCHJ27II\\1810.html}
}

@article{garipovLossSurfacesMode2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2018-02-01},
  pages = {arXiv:1802.10026},
  doi = {10.48550/arXiv.1802.10026},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
  keywords = {Computer Science - Artificial,Computer Science - Machine Learning,Intelligence,Statistics - Machine Learning},
  note = {Appears at Advances in Neural Information Processing Systems (NIPS), 2018}
}

@online{garipovLossSurfacesMode2018a,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2018-10-31},
  eprint = {1802.10026},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1802.10026},
  url = {http://arxiv.org/abs/1802.10026},
  urldate = {2025-11-13},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Appears at Advances in Neural Information Processing Systems (NIPS), 2018},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\VPG4REXN\\Garipov et al. - 2018 - Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\R29C42F6\\1802.html}
}

@online{gebhartCharacterizingShapeActivation2019,
  title = {Characterizing the {{Shape}} of {{Activation Space}} in {{Deep Neural Networks}}},
  author = {Gebhart, Thomas and Schrater, Paul and Hylton, Alan},
  date = {2019-05-30},
  eprint = {1901.09496},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1901.09496},
  url = {http://arxiv.org/abs/1901.09496},
  urldate = {2025-11-16},
  abstract = {The representations learned by deep neural networks are difficult to interpret in part due to their large parameter space and the complexities introduced by their multi-layer structure. We introduce a method for computing persistent homology over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input. This topological perspective provides unique insights into the distributed representations encoded by neural networks in terms of the shape of their activation structures. We demonstrate the value of this approach by showing an alternative explanation for the existence of adversarial examples. By studying the topology of network activations across multiple architectures and datasets, we find that adversarial perturbations do not add activations that target the semantic structure of the adversarial class as previously hypothesized. Rather, adversarial examples are explainable as alterations to the dominant activation structures induced by the original image, suggesting the class representations learned by deep networks are problematically sparse on the input space.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\724TLYEB\\Gebhart et al. - 2019 - Characterizing the Shape of Activation Space in Deep Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\XX9IFZTZ\\1901.html}
}

@online{gilmerAdversarialSpheres2018,
  title = {Adversarial {{Spheres}}},
  author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
  date = {2018-09-11},
  eprint = {1801.02774},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.02774},
  url = {http://arxiv.org/abs/1801.02774},
  urldate = {2025-11-13},
  abstract = {State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size \$O(1/\textbackslash sqrt\{d\})\$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\Aymen\Zotero\storage\KFJ3Z9SA\Gilmer et al. - 2018 - Adversarial Spheres.pdf}
}

@inproceedings{gilmerNeuralMessagePassing2017,
  title = {Neural Message Passing for Quantum Chemistry},
  booktitle = {International Conference on Machine Learning},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017},
  pages = {1263--1272},
  publisher = {Pmlr},
  url = {https://proceedings.mlr.press/v70/gilmer17a},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\DPMT82DM\Gilmer et al. - 2017 - Neural message passing for quantum chemistry.pdf}
}

@online{girrbachAddressingCaveatsNeural2023,
  title = {Addressing Caveats of Neural Persistence with Deep Graph Persistence},
  author = {Girrbach, Leander and Christensen, Anders and Winther, Ole and Akata, Zeynep and Koepke, A. Sophia},
  date = {2023-11-20},
  eprint = {2307.10865},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.10865},
  url = {http://arxiv.org/abs/2307.10865},
  urldate = {2025-11-16},
  abstract = {Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Transactions on Machine Learning Research (TMLR), 2023},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\36XAPGJ5\\Girrbach et al. - 2023 - Addressing caveats of neural persistence with deep graph persistence.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\IYYK9NCD\\2307.html}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010-03-31},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2024-09-20},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {Loss Landscape}
}

@online{goibertAdversarialRobustnessPerspective2022,
  title = {An {{Adversarial Robustness Perspective}} on the {{Topology}} of {{Neural Networks}}},
  author = {Goibert, Morgane and Ricatte, Thomas and Dohmatob, Elvis},
  date = {2022-11-04},
  eprint = {2211.02675},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.02675},
  url = {http://arxiv.org/abs/2211.02675},
  urldate = {2025-11-16},
  abstract = {In this paper, we investigate the impact of neural networks (NNs) topology on adversarial robustness. Specifically, we study the graph produced when an input traverses all the layers of a NN, and show that such graphs are different for clean and adversarial inputs. We find that graphs from clean inputs are more centralized around highway edges, whereas those from adversaries are more diffuse, leveraging under-optimized edges. Through experiments on a variety of datasets and architectures, we show that these under-optimized edges are a source of adversarial vulnerability and that they can be used to detect adversarial inputs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\7EQUKI6K\\Goibert et al. - 2022 - An Adversarial Robustness Perspective on the Topology of Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\J6SL9QBW\\2211.html}
}

@article{grezmakInterpretableConvolutionalNeural2020,
  title = {Interpretable {{Convolutional Neural Network Through Layer-wise Relevance Propagation}} for {{Machine Fault Diagnosis}}},
  author = {Grezmak, John and Zhang, Jianjing and Wang, Peng and Loparo, Kenneth A. and Gao, Robert X.},
  date = {2020-03-15},
  journaltitle = {IEEE Sensors J.},
  volume = {20},
  number = {6},
  pages = {3172--3181},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2019.2958787},
  url = {https://ieeexplore.ieee.org/document/8930493/},
  urldate = {2024-09-20},
  abstract = {As a state-of-the-art pattern recognition technique, convolutional neural networks (CNNs) have been increasingly investigated for machine fault diagnosis, due to their ability in analyzing nonlinear and nonstationary high-dimensional data that are typically associated with the performance degradation process of machines. A key issue of interest is how the inputs to CNNs that contain fault-related patterns are learned by CNNs to recognize discriminatory information for fault diagnosis. Understanding this link will help establish connection to the physical meaning of the diagnosis, contributing to the broad acceptance of CNNs as a trustworthy complement to physics-based reasoning by human experts. Using Layer-wise Relevance Propagation (LRP) as an indicator, this paper investigates the performance of a CNN trained by time-frequency spectra images of vibration signals measured on an induction motor. The LRP provides pixel-level representation of which values in the input signal contribute the most to the diagnosis results, thereby providing an improved understanding of how the CNN learns to distinguish between fault types from these inputs. Results have shown that the patterns learned by CNNs in the time-frequency spectra images are intuitive and consistent with respect to network re-training. Comparison with using raw time series and discrete Fourier transform coefficients as inputs reveals that time-frequency images allow for more consistent pattern recognition by CNNs.},
  keywords = {LRP},
  note = {[TLDR] This paper investigates the performance of a CNN trained by time-frequency spectra images of vibration signals measured on an induction motor using Layer-wise Relevance Propagation (LRP) as an indicator, and reveals that time- frequency images allow for more consistent pattern recognition by CNNs.}
}

@software{GUDHITDAtutorial2025,
  title = {{{GUDHI}}/{{TDA-tutorial}}},
  date = {2025-11-08T14:58:35Z},
  origdate = {2019-05-07T19:49:13Z},
  url = {https://github.com/GUDHI/TDA-tutorial},
  urldate = {2025-11-12},
  abstract = {A set of jupyter notebooks for the practice of TDA with the python Gudhi library together with popular machine learning and data sciences libraries.},
  organization = {Geometry Understanding in Higher Dimensions}
}

@online{guMambaLinearTimeSequence2024,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  date = {2024-05-31},
  eprint = {2312.00752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.00752},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2025-11-16},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\$\textbackslash times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\35TL3AN3\\Gu et Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\NFAEAXEM\\2312.html}
}

@online{hallacToeplitzInverseCovarianceBased2018,
  title = {Toeplitz {{Inverse Covariance-Based Clustering}} of {{Multivariate Time Series Data}}},
  author = {Hallac, David and Vare, Sagar and Boyd, Stephen and Leskovec, Jure},
  date = {2018-05-15},
  eprint = {1706.03161},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03161},
  url = {http://arxiv.org/abs/1706.03161},
  urldate = {2025-11-16},
  abstract = {Subsequence clustering of multivariate time series is a useful tool for discovering repeated patterns in temporal data. Once these patterns have been discovered, seemingly complicated datasets can be interpreted as a temporal sequence of only a small number of states, or clusters. For example, raw sensor data from a fitness-tracking application can be expressed as a timeline of a select few actions (i.e., walking, sitting, running). However, discovering these patterns is challenging because it requires simultaneous segmentation and clustering of the time series. Furthermore, interpreting the resulting clusters is difficult, especially when the data is high-dimensional. Here we propose a new method of model-based clustering, which we call Toeplitz Inverse Covariance-based Clustering (TICC). Each cluster in the TICC method is defined by a correlation network, or Markov random field (MRF), characterizing the interdependencies between different observations in a typical subsequence of that cluster. Based on this graphical representation, TICC simultaneously segments and clusters the time series data. We solve the TICC problem through alternating minimization, using a variation of the expectation maximization (EM) algorithm. We derive closed-form solutions to efficiently solve the two resulting subproblems in a scalable way, through dynamic programming and the alternating direction method of multipliers (ADMM), respectively. We validate our approach by comparing TICC to several state-of-the-art baselines in a series of synthetic experiments, and we then demonstrate on an automobile sensor dataset how TICC can be used to learn interpretable clusters in real-world scenarios.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Mathematics - Optimization and Control},
  note = {Comment: This revised version fixes two small typos in the published version},
  file = {C:\Users\Aymen\Zotero\storage\V34IZSSS\Hallac et al. - 2018 - Toeplitz Inverse Covariance-Based Clustering of Multivariate Time Series Data.pdf}
}

@software{HarvardedgeCs249r_book2025,
  title = {Harvard-Edge/Cs249r\_book},
  date = {2025-11-12T18:44:18Z},
  origdate = {2023-09-06T19:31:06Z},
  url = {https://github.com/harvard-edge/cs249r_book},
  urldate = {2025-11-12},
  abstract = {Introduction to Machine Learning Systems},
  organization = {Harvard Edge Computing},
  keywords = {artificial-intelligence,cloud-ml,computer-systems,courseware,deep-learning,edge-machine-learning,embedded-ml,machine-learning,machine-learning-systems,mobile-ml,textbook,tinyml}
}

@inreference{HaversineFormula2024,
  title = {Haversine Formula},
  booktitle = {Wikipedia},
  date = {2024-07-18T13:56:35Z},
  url = {https://en.wikipedia.org/w/index.php?title=Haversine_formula&oldid=1235273703},
  urldate = {2024-09-20},
  abstract = {The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. Important in navigation, it is a special case of a more general formula in spherical trigonometry, the law of haversines, that relates the sides and angles of spherical triangles. The first table of haversines in English was published by James Andrew in 1805, but Florian Cajori credits an earlier use by José de Mendoza y Ríos in 1801. The term haversine was coined in 1835 by James Inman. These names follow from the fact that they are customarily written in terms of the haversine function, given by hav θ = sin2(⁠θ/2⁠). The formulas could equally be written in terms of any multiple of the haversine, such as the older versine function (twice the haversine). Prior to the advent of computers, the elimination of division and multiplication by factors of two proved convenient enough that tables of haversine values and logarithms were included in 19th- and early 20th-century navigation and trigonometric texts.  These days, the haversine form is also convenient in that it has no coefficient in front of the sin2 function.},
  langid = {english},
  annotation = {Page Version ID: 1235273703}
}

@article{heAutoMLSurveyStateoftheArt2019,
  title = {{{AutoML}}: {{A Survey}} of the {{State-of-the-Art}}},
  author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  date = {2019-08-01},
  pages = {arXiv:1908.00709},
  doi = {10.48550/arXiv.1908.00709},
  abstract = {Deep learning (DL) techniques have penetrated all aspects of our lives and brought us great convenience. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering the applications of DL to more areas. Automated machine learning (AutoML) becomes a promising solution to build a DL system without human assistance, and a growing number of researchers focus on AutoML. In this paper, we provide a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. First, we introduce AutoML methods according to the pipeline, covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS). We focus more on NAS, as it is currently very hot sub-topic of AutoML. We summarize the performance of the representative NAS algorithms on the CIFAR-10 and ImageNet datasets and further discuss several worthy studying directions of NAS methods: one/two-stage NAS, one-shot NAS, and joint hyperparameter and architecture optimization. Finally, we discuss some open problems of the existing AutoML methods for future research.},
  keywords = {Computer Science - Computer,Computer Science - Machine Learning,Statistics - Machine Learning,Vision and Pattern Recognition},
  note = {automated machine learning (AutoML), published in journal of Knowledge- Based Systems; Knowledge-Based Systems, Volume 212, 5 January 2021, 106622; doi:10.1016/j.knosys.2020.106622}
}

@inreference{HelmholtzDecomposition2025,
  title = {Helmholtz Decomposition},
  booktitle = {Wikipedia},
  date = {2025-11-02T04:46:12Z},
  url = {https://en.wikipedia.org/w/index.php?title=Helmholtz_decomposition&oldid=1320006029},
  urldate = {2025-11-12},
  abstract = {In physics and mathematics, the Helmholtz decomposition theorem or the fundamental theorem of vector calculus states that certain differentiable vector fields can be resolved into the sum of an irrotational (curl-free) vector field and a solenoidal (divergence-free) vector field. In physics, often only the decomposition of sufficiently smooth, rapidly decaying vector fields in three dimensions is discussed. It is named after Hermann von Helmholtz.},
  langid = {english},
  annotation = {Page Version ID: 1320006029},
  file = {C:\Users\Aymen\Zotero\storage\KIF5EDCI\index.html}
}

@inreference{HelmholtzEquation2025,
  title = {Helmholtz Equation},
  booktitle = {Wikipedia},
  date = {2025-11-10T17:24:47Z},
  url = {https://en.wikipedia.org/w/index.php?title=Helmholtz_equation&oldid=1321457143},
  urldate = {2025-11-12},
  abstract = {In mathematics, the Helmholtz equation is the eigenvalue problem for the Laplace operator. It corresponds to the elliptic partial differential equation:                                   ∇                        2                             f         =         −                    k                        2                             f         ,                 \{\textbackslash displaystyle \textbackslash nabla \textasciicircum\{2\}f=-k\textasciicircum\{2\}f,\}    where ∇2 is the Laplace operator, –k2 is the eigenvalue, and f is the (eigen)function. When the equation is applied to waves, k is known as the wave number. The Helmholtz equation has a variety of applications in physics and other sciences, including the wave equation, the diffusion equation, and the Schrödinger equation for a free particle. In optics, the Helmholtz equation is the wave equation for the electric field. The equation is named after Hermann von Helmholtz, who studied it in 1860.},
  langid = {english},
  annotation = {Page Version ID: 1321457143},
  file = {C:\Users\Aymen\Zotero\storage\ZPYJG8SM\index.html}
}

@online{hematiUnderstandingHessianAlignment2023,
  title = {Understanding {{Hessian Alignment}} for {{Domain Generalization}}},
  author = {Hemati, Sobhan and Zhang, Guojun and Estiri, Amir and Chen, Xi},
  date = {2023-08-22},
  eprint = {2308.11778},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2308.11778},
  url = {http://arxiv.org/abs/2308.11778},
  urldate = {2024-10-04},
  abstract = {Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier's head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \textbackslash url\{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Hessian Alignment,Statistics - Machine Learning},
  note = {Comment: ICCV 2023}
}

@inreference{HessenbergMatrix2025,
  title = {Hessenberg Matrix},
  booktitle = {Wikipedia},
  date = {2025-10-21T14:22:04Z},
  url = {https://en.wikipedia.org/w/index.php?title=Hessenberg_matrix&oldid=1318031368},
  urldate = {2025-11-12},
  abstract = {In linear algebra, a Hessenberg matrix is a special kind of square matrix, one that is "almost" triangular. To be exact, an upper Hessenberg matrix has zero entries below the first subdiagonal, and a lower Hessenberg matrix has zero entries above the first superdiagonal. They are named after Karl Hessenberg. A Hessenberg decomposition is a matrix decomposition of a matrix                         A                 \{\textbackslash displaystyle A\}     into a unitary matrix                         P                 \{\textbackslash displaystyle P\}     and a Hessenberg matrix                         H                 \{\textbackslash displaystyle H\}     such that                         P         H                    P                        ∗                             =         A                 \{\textbackslash displaystyle PHP\textasciicircum\{*\}=A\}     where                                    P                        ∗                                     \{\textbackslash displaystyle P\textasciicircum\{*\}\}     denotes the conjugate transpose.},
  langid = {english},
  annotation = {Page Version ID: 1318031368},
  file = {C:\Users\Aymen\Zotero\storage\CLREDL9R\index.html}
}

@software{hiranabeKenjihiranabeTheArtofLinearAlgebra2025,
  title = {Kenjihiranabe/{{The-Art-of-Linear-Algebra}}},
  author = {Hiranabe, Kenji},
  date = {2025-11-12T13:25:18Z},
  origdate = {2021-09-02T02:41:54Z},
  url = {https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra},
  urldate = {2025-11-12},
  abstract = {Graphic notes on Gilbert Strang's "Linear Algebra for Everyone"},
  annotation = {Programmers: \_:n1434}
}

@inreference{HomologicalMirrorSymmetry2023,
  title = {Homological Mirror Symmetry},
  booktitle = {Wikipedia},
  date = {2023-11-05T12:32:12Z},
  url = {https://en.wikipedia.org/w/index.php?title=Homological_mirror_symmetry&oldid=1183613115},
  urldate = {2024-11-05},
  abstract = {Homological mirror symmetry is a mathematical conjecture made by Maxim Kontsevich. It seeks a systematic mathematical explanation for a phenomenon called mirror symmetry first observed by physicists studying string theory.},
  langid = {english},
  annotation = {Page Version ID: 1183613115}
}

@software{huyenChiphuyenAiebook2025,
  title = {Chiphuyen/Aie-Book},
  author = {Huyen, Chip},
  date = {2025-11-13T15:56:43Z},
  origdate = {2024-12-03T19:43:36Z},
  url = {https://github.com/chiphuyen/aie-book},
  urldate = {2025-11-13},
  abstract = {[WIP] Resources for AI engineers. Also contains supporting materials for the book AI Engineering (Chip Huyen, 2025)},
  annotation = {Programmers: \_:n1676}
}

@online{HyperRepresentationsGenerativeModels2025,
  title = {Hyper-{{Representations}} as {{Generative Models}}: {{Sampling Unseen Neural Network Weights P63172}} | {{GTC}} 2024 | {{NVIDIA On-Demand}}},
  shorttitle = {Hyper-{{Representations}} as {{Generative Models}}},
  date = {2025-11-13},
  url = {https://www.nvidia.com/en-us/on-demand/session/gtc24-p63172/},
  urldate = {2025-11-13},
  abstract = {Our hypothesis is that neural network (NN) models (i.e., their weights and biases) evolve on unique trajectories in weight space during training},
  langid = {american},
  organization = {NVIDIA},
  file = {C:\Users\Aymen\Zotero\storage\96EMMNP5\gtc24-p63172.html}
}

@online{IllustratedTransformerJay2024,
  title = {The {{Illustrated Transformer}} – {{Jay Alammar}} – {{Visualizing}} Machine Learning One Concept at a Time.},
  date = {2024-09-20},
  url = {https://jalammar.github.io/illustrated-transformer/},
  urldate = {2024-09-20},
  keywords = {Transformer Ressources}
}

@article{imtiazhumayunUnderstandingLocalGeometry2024,
  title = {Understanding the {{Local Geometry}} of {{Generative Model Manifolds}}},
  author = {Imtiaz Humayun, Ahmed and Amara, Ibtihel and Schumann, Candice and Farnadi, Golnoosh and Rostamzadeh, Negar and Havaei, Mohammad},
  date = {2024-08-01},
  pages = {arXiv:2408.08307},
  doi = {10.48550/arXiv.2408.08307},
  abstract = {Deep generative models learn continuous representations of complex data manifolds using a finite number of samples during training. For a pre-trained generative model, the common way to evaluate the quality of the manifold representation learned, is by computing global metrics like Fréchet Inception Distance using a large number of generated and real samples. However, generative model performance is not uniform across the learned manifold, e.g., for \textbackslash textit\{foundation models\} like Stable Diffusion generation performance can vary significantly based on the conditioning or initial noise vector being denoised. In this paper we study the relationship between the \textbackslash textit\{local geometry of the learned manifold\} and downstream generation. Based on the theory of continuous piecewise-linear (CPWL) generators, we use three geometric descriptors - scaling (\$\textbackslash psi\$), rank (\$\textbackslash nu\$), and complexity (\$\textbackslash delta\$) - to characterize a pre-trained generative model manifold locally. We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are correlated with generation aesthetics, artifacts, uncertainty, and even memorization. Finally we demonstrate that training a \textbackslash textit\{reward model\} on the local geometry can allow controlling the likelihood of a generated sample under the learned distribution.},
  keywords = {Computer Science - Computer,Computer Science - Machine Learning,Vision and Pattern Recognition},
  note = {Pre-print. 11 pages main, 8 pages app., 28 figures}
}

@online{jacotNeuralTangentKernel2020,
  title = {Neural {{Tangent Kernel}}: {{Convergence}} and {{Generalization}} in {{Neural Networks}}},
  shorttitle = {Neural {{Tangent Kernel}}},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
  date = {2020-02-11},
  eprint = {1806.07572},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1806.07572},
  url = {http://arxiv.org/abs/1806.07572},
  urldate = {2025-11-13},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_θ\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_θ\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Probability,Statistics - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\JQYV4UY9\1806.html}
}

@online{jinConditionalLoRAParameter2024,
  title = {Conditional {{LoRA Parameter Generation}}},
  author = {Jin, Xiaolong and Wang, Kai and Tang, Dongwen and Zhao, Wangbo and Zhou, Yukun and Tang, Junshu and You, Yang},
  date = {2024-08-02},
  eprint = {2408.01415},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2408.01415},
  urldate = {2024-11-05},
  abstract = {Generative models have achieved remarkable success in image, video, and text domains. Inspired by this, researchers have explored utilizing generative models to generate neural network parameters. However, these efforts have been limited by the parameter size and the practicality of generating high-performance parameters. In this paper, we propose COND P-DIFF, a novel approach that demonstrates the feasibility of controllable high-performance parameter generation, particularly for LoRA (Low-Rank Adaptation) weights, during the fine-tuning process. Specifically, we employ an autoencoder to extract efficient latent representations for parameters. We then train a conditional latent diffusion model to synthesize high-performing model parameters from random noise based on specific task conditions. Experimental results in both computer vision and natural language processing domains consistently demonstrate that COND P-DIFF can generate high-performance parameters conditioned on the given task. Moreover, we observe that the parameter distribution generated by COND P-DIFF exhibits differences compared to the distribution obtained through normal optimization methods, indicating a certain level of generalization capability. Our work paves the way for further exploration of condition-driven parameter generation, offering a promising direction for task-specific adaptation of neural networks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,weight generation}
}

@online{jingImplicitRankMinimizingAutoencoder2020,
  title = {Implicit {{Rank-Minimizing Autoencoder}}},
  author = {Jing, Li and Zbontar, Jure and LeCun, Yann},
  date = {2020-10-14},
  eprint = {2010.00679},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2010.00679},
  url = {http://arxiv.org/abs/2010.00679},
  urldate = {2024-09-20},
  abstract = {An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{kandasamyNeuralArchitectureSearch2019,
  title = {Neural {{Architecture Search}} with {{Bayesian Optimisation}} and {{Optimal Transport}}},
  author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric},
  date = {2019-03-15},
  eprint = {1802.07191},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.07191},
  urldate = {2024-09-20},
  abstract = {Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,NAS,NASBOT,OT,Statistics - Machine Learning}
}

@online{KevinWebsterImperial2025,
  title = {Kevin {{Webster}} | {{About}} | {{Imperial College London}}},
  date = {2025-11-13},
  url = {https://profiles.imperial.ac.uk/kevin.webster},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\4AGLX6F5\kevin.html}
}

@online{khrulkovGeometryScoreMethod2018,
  title = {Geometry {{Score}}: {{A Method For Comparing Generative Adversarial Networks}}},
  shorttitle = {Geometry {{Score}}},
  author = {Khrulkov, Valentin and Oseledets, Ivan},
  date = {2018-06-09},
  eprint = {1802.02664},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.02664},
  url = {http://arxiv.org/abs/1802.02664},
  urldate = {2025-11-16},
  abstract = {One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICML 2018},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\B9UDPXRT\\Khrulkov et Oseledets - 2018 - Geometry Score A Method For Comparing Generative Adversarial Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\WD2Y3APB\\1802.html}
}

@online{kimHypeBoyGenerativeSelfSupervised2024,
  title = {{{HypeBoy}}: {{Generative Self-Supervised Representation Learning}} on {{Hypergraphs}}},
  shorttitle = {{{HypeBoy}}},
  author = {Kim, Sunwoo and Kang, Shinhwan and Bu, Fanchen and Lee, Soo Yong and Yoo, Jaemin and Shin, Kijung},
  date = {2024-03-31},
  eprint = {2404.00638},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.00638},
  url = {http://arxiv.org/abs/2404.00638},
  urldate = {2024-09-20},
  abstract = {Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SSL method, HypeBoy. HypeBoy learns effective general-purpose hypergraph representations, outperforming 16 baseline methods across 11 benchmark datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Graph,Hyper-representation},
  note = {Comment: Published as a conference paper at ICLR 2024}
}

@online{kimTopPRRobustSupport2024,
  title = {{{TopP}}\&{{R}}: {{Robust Support Estimation Approach}} for {{Evaluating Fidelity}} and {{Diversity}} in {{Generative Models}}},
  shorttitle = {{{TopP}}\&{{R}}},
  author = {Kim, Pum Jun and Jang, Yoojin and Kim, Jisu and Yoo, Jaejun},
  date = {2024-01-24},
  eprint = {2306.08013},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.08013},
  url = {http://arxiv.org/abs/2306.08013},
  urldate = {2025-11-16},
  abstract = {We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Frechet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed (Non-IID) perturbations, while accurately capturing the true trend of change in samples. To the best of our knowledge, this is the first evaluation metric focused on the robust estimation of the support and provides its statistical consistency under noise.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted to NeurIPS 2023},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\542KNF62\\Kim et al. - 2024 - TopP&R Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Model.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\UJIMN4BC\\2306.html}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  date = {2017-03-28},
  journaltitle = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {114},
  number = {13},
  eprint = {1612.00796},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {3521--3526},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1611835114},
  url = {http://arxiv.org/abs/1612.00796},
  urldate = {2024-10-07},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  date = {2020-02-18},
  eprint = {2001.04451},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2001.04451},
  url = {http://arxiv.org/abs/2001.04451},
  urldate = {2025-11-16},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\textasciicircum 2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICLR 2020},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\6QCSIII9\\Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\XDUHPJ65\\2001.html}
}

@article{klabundeReSiComprehensiveBenchmark2024,
  title = {{{ReSi}}: {{A Comprehensive Benchmark}} for {{Representational Similarity Measures}}},
  author = {Klabunde, Max and Wald, Tassilo and Schumacher, Tobias and Maier-Hein, Klaus and Strohmaier, Markus and Lemmerich, Florian},
  date = {2024-08-01},
  pages = {arXiv:2408.00531},
  doi = {10.48550/arXiv.2408.00531},
  abstract = {Measuring the similarity of different representations of neural architectures is a fundamental task and an open research challenge for the machine learning community. This paper presents the first comprehensive benchmark for evaluating representational similarity measures based on well-defined groundings of similarity. The representational similarity (ReSi) benchmark consists of (i) six carefully designed tests for similarity measures, (ii) 23 similarity measures, (iii) eleven neural network architectures, and (iv) six datasets, spanning over the graph, language, and vision domains. The benchmark opens up several important avenues of research on representational similarity that enable novel explorations and applications of neural architectures. We demonstrate the utility of the ReSi benchmark by conducting experiments on various neural network architectures, real world datasets and similarity measures. All components of the benchmark are publicly available and thereby facilitate systematic reproduction and production of research results. The benchmark is extensible, future research can build on and further expand it. We believe that the ReSi benchmark can serve as a sound platform catalyzing future research that aims to systematically evaluate existing and explore novel ways of comparing representations of neural architectures.},
  keywords = {Computer Science - Machine Learning},
  note = {Feedback welcome! Code and data at https://github.com/mklabunde/resi}
}

@online{knyazevAcceleratingTrainingNeuron2024,
  title = {Accelerating {{Training}} with {{Neuron Interaction}} and {{Nowcasting Networks}}},
  author = {Knyazev, Boris and Moudgil, Abhinav and Lajoie, Guillaume and Belilovsky, Eugene and Lacoste-Julien, Simon},
  date = {2024-10-03},
  eprint = {2409.04434},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2409.04434},
  urldate = {2024-11-05},
  abstract = {Neural network training can be accelerated when a learnable update rule is used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable update rules can be costly and unstable to train and use. Recently, Jang et al. (2023) proposed a simpler approach to accelerate training based on weight nowcaster networks (WNNs). In their approach, Adam is used for most of the optimization steps and periodically, only every few steps, a WNN nowcasts (predicts near future) parameters. We improve WNNs by proposing neuron interaction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leverages neuron connectivity and graph neural networks to more accurately nowcast parameters. We further show that in some networks, such as Transformers, modeling neuron connectivity accurately is challenging. We address this and other limitations, which allows NiNo to accelerate Adam training by up to 50\% in vision and language tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Training acceleration}
}

@online{knyazevCanWeScale2023,
  title = {Can {{We Scale Transformers}} to {{Predict Parameters}} of {{Diverse ImageNet Models}}?},
  author = {Knyazev, Boris and Hwang, Doha and Lacoste-Julien, Simon},
  date = {2023-05-31},
  eprint = {2303.04143},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2303.04143},
  url = {http://arxiv.org/abs/2303.04143},
  urldate = {2024-09-20},
  abstract = {Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICML 2023, camera ready (7 tables with extra results added), code and models are at https://github.com/SamsungSAILMontreal/ghn3
\par
Comment: ICML 2023, camera ready (7 tables with extra results added), code and models are at https://github.com/SamsungSAILMontreal/ghn3}
}

@article{kosmynaYourBrainChatGPT,
  title = {Your {{Brain}} on {{ChatGPT}}: {{Accumulation}} of {{Cognitive Debt}} When {{Using}} an {{AI Assistant}} for {{Essay Writing Task}}△},
  author = {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye Tong and Situ, Jessica},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\GJV7UMW9\Kosmyna et al. - Your Brain on ChatGPT Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing T.pdf}
}

@online{kristiansenNarrowingFocusLearned2024,
  title = {Narrowing the {{Focus}}: {{Learned Optimizers}} for {{Pretrained Models}}},
  shorttitle = {Narrowing the {{Focus}}},
  author = {Kristiansen, Gus and Sandler, Mark and Zhmoginov, Andrey and Miller, Nolan and Goyal, Anirudh and Lee, Jihwan and Vladymyrov, Max},
  date = {2024-08-21},
  eprint = {2408.09310},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.09310},
  url = {http://arxiv.org/abs/2408.09310},
  urldate = {2024-09-20},
  abstract = {In modern deep learning, the models are learned by applying gradient updates using an optimizer, which transforms the updates based on various statistics. Optimizers are often hand-designed and tuning their hyperparameters is a big part of the training process. Learned optimizers have shown some initial promise, but are generally unsuccessful as a general optimization mechanism applicable to every problem. In this work we explore a different direction: instead of learning general optimizers, we instead specialize them to a specific training environment. We propose a novel optimizer technique that learns a layer-specific linear combination of update directions provided by a set of base optimizers, effectively adapting its strategy to the specific model and dataset. When evaluated on image classification tasks, this specialized optimizer significantly outperforms both traditional off-the-shelf methods such as Adam, as well as existing general learned optimizers. Moreover, it demonstrates robust generalization with respect to model initialization, evaluating on unseen datasets, and training durations beyond its meta-training horizon.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning}
}

@online{lacombeTopologicalUncertaintyMonitoring2021,
  title = {Topological {{Uncertainty}}: {{Monitoring}} Trained Neural Networks through Persistence of Activation Graphs},
  shorttitle = {Topological {{Uncertainty}}},
  author = {Lacombe, Théo and Ike, Yuichi and Carriere, Mathieu and Chazal, Frédéric and Glisse, Marc and Umeda, Yuhei},
  date = {2021-05-07},
  eprint = {2105.04404},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2105.04404},
  url = {http://arxiv.org/abs/2105.04404},
  urldate = {2025-11-16},
  abstract = {Although neural networks are capable of reaching astonishing performances on a wide variety of contexts, properly training networks on complicated tasks requires expertise and can be expensive from a computational perspective. In industrial applications, data coming from an open-world setting might widely differ from the benchmark datasets on which a network was trained. Being able to monitor the presence of such variations without retraining the network is of crucial importance. In this article, we develop a method to monitor trained neural networks based on the topological properties of their activation graphs. To each new observation, we assign a Topological Uncertainty, a score that aims to assess the reliability of the predictions by investigating the whole network instead of its final layer only, as typically done by practitioners. Our approach entirely works at a post-training level and does not require any assumption on the network architecture, optimization scheme, nor the use of data augmentation or auxiliary datasets; and can be faithfully applied on a large range of network architectures and data types. We showcase experimentally the potential of Topological Uncertainty in the context of trained network selection, Out-Of-Distribution detection, and shift-detection, both on synthetic and real datasets of images and graphs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\U9VEJ5C9\\Lacombe et al. - 2021 - Topological Uncertainty Monitoring trained neural networks through persistence of activation graphs.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\MQ9SQR5U\\2105.html}
}

@online{lanLCALossChange2020,
  title = {{{LCA}}: {{Loss Change Allocation}} for {{Neural Network Training}}},
  shorttitle = {{{LCA}}},
  author = {Lan, Janice and Liu, Rosanne and Zhou, Hattie and Yosinski, Jason},
  date = {2020-03-03},
  eprint = {1909.01440},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1909.01440},
  url = {http://arxiv.org/abs/1909.01440},
  urldate = {2024-09-20},
  abstract = {Neural networks enjoy widespread use, but many aspects of their training, representation, and operation are poorly understood. In particular, our view into the training process is limited, with a single scalar loss being the most common viewport into this high-dimensional, dynamic process. We propose a new window into training called Loss Change Allocation (LCA), in which credit for changes to the network loss is conservatively partitioned to the parameters. This measurement is accomplished by decomposing the components of an approximate path integral along the training trajectory using a Runge-Kutta integrator. This rich view shows which parameters are responsible for decreasing or increasing the loss during training, or which parameters "help" or "hurt" the network's learning, respectively. LCA may be summed over training iterations and/or over neurons, channels, or layers for increasingly coarse views. This new measurement device produces several insights into training. (1) We find that barely over 50\% of parameters help during any given iteration. (2) Some entire layers hurt overall, moving on average against the training gradient, a phenomenon we hypothesize may be due to phase lag in an oscillatory training process. (3) Finally, increments in learning proceed in a synchronized manner across layers, often peaking on identical iterations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: NeurIPS 2019 camera ready version
\par
Comment: NeurIPS 2019 camera ready version}
}

@inreference{LaplacesEquation2025,
  title = {Laplace's Equation},
  booktitle = {Wikipedia},
  date = {2025-07-31T04:08:30Z},
  url = {https://en.wikipedia.org/w/index.php?title=Laplace%27s_equation&oldid=1303458496},
  urldate = {2025-11-12},
  abstract = {In mathematics and physics, Laplace's equation is a second-order partial differential equation named after Pierre-Simon Laplace, who first studied its properties in 1786. This is often written as                                   ∇                        2                                      f         =         0                 \{\textbackslash displaystyle \textbackslash nabla \textasciicircum\{2\}\textbackslash!f=0\}     or                         Δ         f         =         0         ,                 \{\textbackslash displaystyle \textbackslash Delta f=0,\}    where                         Δ         =         ∇         ⋅         ∇         =                    ∇                        2                                     \{\textbackslash displaystyle \textbackslash Delta =\textbackslash nabla \textbackslash cdot \textbackslash nabla =\textbackslash nabla \textasciicircum\{2\}\}     is the Laplace operator,                         ∇         ⋅                 \{\textbackslash displaystyle \textbackslash nabla \textbackslash cdot \}     is the divergence operator (also symbolized "div"),                         ∇                 \{\textbackslash displaystyle \textbackslash nabla \}     is the gradient operator (also symbolized "grad"), and                         f         (         x         ,         y         ,         z         )                 \{\textbackslash displaystyle f(x,y,z)\}     is a twice-differentiable real-valued function. The Laplace operator therefore maps a scalar function to another scalar function. If the right-hand side is specified as a given function,                         h         (         x         ,         y         ,         z         )                 \{\textbackslash displaystyle h(x,y,z)\}    , we have                        Δ         f         =         h                 \{\textbackslash displaystyle \textbackslash Delta f=h\}    This is called Poisson's equation, a generalization of Laplace's equation. Laplace's equation and Poisson's equation are the simplest examples of elliptic partial differential equations. Laplace's equation is also a special case of the Helmholtz equation. The general theory of solutions to Laplace's equation is known as potential theory. The twice continuously differentiable solutions of Laplace's equation are the harmonic functions, which are important in multiple branches of physics, notably electrostatics, gravitation, and fluid dynamics. In the study of heat conduction, the Laplace equation is the steady-state heat equation. In general, Laplace's equation describes situations of equilibrium, or those that do not depend explicitly on time.},
  langid = {english},
  annotation = {Page Version ID: 1303458496}
}

@inreference{LebesgueMeasure2025,
  title = {Lebesgue Measure},
  booktitle = {Wikipedia},
  date = {2025-11-04T17:46:44Z},
  url = {https://en.wikipedia.org/w/index.php?title=Lebesgue_measure&oldid=1320435834},
  urldate = {2025-11-12},
  abstract = {In measure theory, a branch of mathematics, the Lebesgue measure, named after French mathematician Henri Lebesgue, is the standard way of assigning a measure to subsets of higher dimensional Euclidean n-spaces. For lower dimensions                         n         =         1         ,         2         ,                    or                   3                 \{\textbackslash displaystyle n=1,2,\{\textbackslash text\{or \}\}3\}    , it coincides with the standard measure of length, area, or volume. In general, it is also called n-dimensional volume, n-volume, hypervolume, or simply volume. It is used throughout real analysis, in particular to define Lebesgue integration. Sets that can be assigned a Lebesgue measure are called Lebesgue-measurable; the measure of the Lebesgue-measurable set                         A                 \{\textbackslash displaystyle A\}     is here denoted by                         λ         (         A         )                 \{\textbackslash displaystyle \textbackslash lambda (A)\}    . Henri Lebesgue described this measure in the year 1901 which, a year after, was followed up by his description of the Lebesgue integral. Both were published as part of his dissertation Intégrale, Longueur, Aire in 1902.},
  langid = {english},
  annotation = {Page Version ID: 1320435834},
  file = {C:\Users\Aymen\Zotero\storage\AUZ9K5WD\index.html}
}

@inproceedings{lecunOptimalBrainDamage1989,
  title = {Optimal {{Brain Damage}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Denker, John and Solla, Sara},
  editor = {Touretzky, D.},
  date = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf}
}

@online{lee-thorpFNetMixingTokens2022,
  title = {{{FNet}}: {{Mixing Tokens}} with {{Fourier Transforms}}},
  shorttitle = {{{FNet}}},
  author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  date = {2022-05-26},
  eprint = {2105.03824},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.03824},
  url = {http://arxiv.org/abs/2105.03824},
  urldate = {2025-11-16},
  abstract = {We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97\% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80\% faster on GPUs and 70\% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the "efficient" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: To appear at NAACL 2022},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\E9JKY9ZI\\Lee-Thorp et al. - 2022 - FNet Mixing Tokens with Fourier Transforms.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\3NMHSXS7\\2105.html}
}

@online{leeDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2018-03-06},
  eprint = {1711.00165},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1711.00165},
  url = {http://arxiv.org/abs/1711.00165},
  urldate = {2025-11-13},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published version in ICLR 2018. 10 pages + appendix
\par
Comment: Published version in ICLR 2018. 10 pages + appendix},
  file = {C:\Users\Aymen\Zotero\storage\H2EPAHHM\Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf}
}

@article{leeFiniteInfiniteNeural2020,
  title = {Finite versus Infinite Neural Networks: An Empirical Study},
  shorttitle = {Finite versus Infinite Neural Networks},
  author = {Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  date = {2020},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {15156--15172},
  url = {https://proceedings.neurips.cc/paper/2020/hash/ad086f59924fffe0773f8d0ca22ea712-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\BT3JR36Y\Lee et al. - 2020 - Finite versus infinite neural networks an empirical study.pdf}
}

@online{leeGrokfastAcceleratedGrokking2024,
  title = {Grokfast: {{Accelerated Grokking}} by {{Amplifying Slow Gradients}}},
  shorttitle = {Grokfast},
  author = {Lee, Jaerin and Kang, Bong Gyun and Kim, Kihoon and Lee, Kyoung Mu},
  date = {2024-06-05},
  eprint = {2405.20233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.20233},
  url = {http://arxiv.org/abs/2405.20233},
  urldate = {2024-09-20},
  abstract = {One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than \$\textbackslash times 50\$ with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. Our code is available at https://github.com/ironjr/grokfast.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: 17 pages, 13 figures. Typo fixed. Project page: https://jaerinlee.com/research/grokfast}
}

@article{leeWideNeuralNetworks2019,
  title = {Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  date = {2019},
  journaltitle = {Advances in neural information processing systems},
  volume = {32},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/0d1a9651497a38d8b1c3871c84528bd4-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\K5WASJ3J\Lee et al. - 2019 - Wide neural networks of any depth evolve as linear models under gradient descent.pdf}
}

@inreference{LegendreFunction2024,
  title = {Legendre Function},
  booktitle = {Wikipedia},
  date = {2024-09-08T16:13:22Z},
  url = {https://en.wikipedia.org/w/index.php?title=Legendre_function&oldid=1244692335},
  urldate = {2025-11-12},
  abstract = {In physical science and mathematics, the Legendre functions Pλ, Qλ and associated Legendre functions Pμλ, Qμλ, and Legendre functions of the second kind, Qn, are all solutions of Legendre's differential equation. The Legendre polynomials and the associated Legendre polynomials are also solutions of the differential equation in special cases, which, by virtue of being polynomials, have a large number of additional properties, mathematical structure, and applications. For these polynomial solutions, see the separate Wikipedia articles.},
  langid = {english},
  annotation = {Page Version ID: 1244692335},
  file = {C:\Users\Aymen\Zotero\storage\5X37AS89\index.html}
}

@online{LevyFlightNeural2025,
  title = {Levy Flight Neural Networks - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=levy+flight+neural+networks&oq=levy+flight+neural+networks&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigAdIBCDk5MDJqMGo3qAIPsAIB8QVjc-DI6KXK-Q&client=ms-android-samsung-ss&sourceid=chrome-mobile&ie=UTF-8},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\RWVQ3EEI\search.html}
}

@inreference{LevyProkhorovMetric2025,
  title = {Lévy–{{Prokhorov}} Metric},
  booktitle = {Wikipedia},
  date = {2025-10-15T18:42:40Z},
  url = {https://en.wikipedia.org/w/index.php?title=L%C3%A9vy%E2%80%93Prokhorov_metric&oldid=1316997919},
  urldate = {2025-11-13},
  abstract = {In mathematics, the Lévy–Prokhorov metric (sometimes known just as the Prokhorov metric) is a metric (i.e., a definition of distance) on the collection of probability measures on a given metric space. It is named after the French mathematician Paul Lévy and the Soviet mathematician Yuri Vasilyevich Prokhorov; Prokhorov introduced it in 1956 as a generalization of the earlier Lévy metric.},
  langid = {english},
  annotation = {Page Version ID: 1316997919},
  file = {C:\Users\Aymen\Zotero\storage\QXHC8SVL\index.html}
}

@online{LevyProkhorovPytorch2025,
  title = {Levy {{Prokhorov}} Pytorch - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=levy+Prokhorov+pytorch&client=ms-android-samsung-ss&sca_esv=b34f54761b29060b&sxsrf=AE3TifN-Cn9besIJssA3ONKLsrohP0YN-Q%3A1759143525050&ei=ZWbaaJ7oAoSNkdUP2-_q0Q0&oq=levy+Prokhorov+pytorch&gs_lp=EhNtb2JpbGUtZ3dzLXdpei1zZXJwIhZsZXZ5IFByb2tob3JvdiBweXRvcmNoMgUQIRigAUjvJFDpB1j4HXABeAGQAQCYAeACoAGvC6oBBzAuNi4xLjG4AQPIAQD4AQGYAgmgApsMwgIKEAAYsAMY1gQYR8ICCxAAGIAEGJECGIoFwgIGEAAYFhgewgILEAAYgAQYhgMYigXCAggQABiABBiiBMICBRAAGO8FwgIHECEYoAEYCpgDAIgGAZAGCJIHBzEuNi4xLjGgB-gWsgcHMC42LjEuMbgHgAzCBwcyLTMuNS4xyAdm&sclient=mobile-gws-wiz-serp},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\EDMNHBES\search.html}
}

@online{liaoRandomMatrixTheory2025,
  title = {Random {{Matrix Theory}} for {{Deep Learning}}: {{Beyond Eigenvalues}} of {{Linear Models}}},
  shorttitle = {Random {{Matrix Theory}} for {{Deep Learning}}},
  author = {Liao, Zhenyu and Mahoney, Michael W.},
  date = {2025-06-17},
  eprint = {2506.13139},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2506.13139},
  url = {http://arxiv.org/abs/2506.13139},
  urldate = {2025-11-14},
  abstract = {Modern Machine Learning (ML) and Deep Neural Networks (DNNs) often operate on high-dimensional data and rely on overparameterized models, where classical low-dimensional intuitions break down. In particular, the proportional regime where the data dimension, sample size, and number of model parameters are all large and comparable, gives rise to novel and sometimes counterintuitive behaviors. This paper extends traditional Random Matrix Theory (RMT) beyond eigenvalue-based analysis of linear models to address the challenges posed by nonlinear ML models such as DNNs in this regime. We introduce the concept of High-dimensional Equivalent, which unifies and generalizes both Deterministic Equivalent and Linear Equivalent, to systematically address three technical challenges: high dimensionality, nonlinearity, and the need to analyze generic eigenspectral functionals. Leveraging this framework, we provide precise characterizations of the training and generalization performance of linear models, nonlinear shallow networks, and deep networks. Our results capture rich phenomena, including scaling laws, double descent, and nonlinear learning dynamics, offering a unified perspective on the theoretical understanding of deep learning in high dimensions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 30 pages, 6 figures
\par
Comment: 30 pages, 6 figures
\par
Comment: 30 pages, 6 figures},
  file = {C:\Users\Aymen\Zotero\storage\EMPWELUW\Liao et Mahoney - 2025 - Random Matrix Theory for Deep Learning Beyond Eigenvalues of Linear Models.pdf}
}

@online{lieberJambaHybridTransformerMamba2024,
  title = {Jamba: {{A Hybrid Transformer-Mamba Language Model}}},
  shorttitle = {Jamba},
  author = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and Abend, Omri and Alon, Raz and Asida, Tomer and Bergman, Amir and Glozman, Roman and Gokhman, Michael and Manevich, Avashalom and Ratner, Nir and Rozen, Noam and Shwartz, Erez and Zusman, Mor and Shoham, Yoav},
  date = {2024-07-03},
  eprint = {2403.19887},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.19887},
  url = {http://arxiv.org/abs/2403.19887},
  urldate = {2025-11-16},
  abstract = {We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Webpage: https://www.ai21.com/jamba},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\TK9WLC3I\\Lieber et al. - 2024 - Jamba A Hybrid Transformer-Mamba Language Model.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\59UV5V7S\\2403.html}
}

@online{liFindingHomologyDecision2020,
  title = {Finding the {{Homology}} of {{Decision Boundaries}} with {{Active Learning}}},
  author = {Li, Weizhi and Dasarathy, Gautam and Ramamurthy, Karthikeyan Natesan and Berisha, Visar},
  date = {2020-11-19},
  eprint = {2011.09645},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.09645},
  url = {http://arxiv.org/abs/2011.09645},
  urldate = {2025-11-16},
  abstract = {Accurately and efficiently characterizing the decision boundary of classifiers is important for problems related to model selection and meta-learning. Inspired by topological data analysis, the characterization of decision boundaries using their homology has recently emerged as a general and powerful tool. In this paper, we propose an active learning algorithm to recover the homology of decision boundaries. Our algorithm sequentially and adaptively selects which samples it requires the labels of. We theoretically analyze the proposed framework and show that the query complexity of our active learning algorithm depends naturally on the intrinsic complexity of the underlying manifold. We demonstrate the effectiveness of our framework in selecting best-performing machine learning models for datasets just using their respective homological summaries. Experiments on several standard datasets show the sample complexity improvement in recovering the homology and demonstrate the practical utility of the framework for model selection. Source code for our algorithms and experimental results is available at https://github.com/wayne0908/Active-Learning-Homology.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\I92K8GLQ\\Li et al. - 2020 - Finding the Homology of Decision Boundaries with Active Learning.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\KAGMJXXL\\2011.html}
}

@software{liGuang000AwesomeDatasetDistillation2024,
  title = {Guang000/{{Awesome-Dataset-Distillation}}},
  author = {Li, Guang},
  date = {2024-09-16T08:25:05Z},
  origdate = {2022-06-09T00:32:32Z},
  url = {https://github.com/Guang000/Awesome-Dataset-Distillation},
  urldate = {2024-09-16},
  abstract = {A curated list of awesome papers on dataset distillation and related applications.},
  keywords = {awesome-list,DD,deep-learning,survey},
  annotation = {Programmers: \_:n200\\
Programmers: \_:n200}
}

@online{liLearningForgetting2017,
  title = {Learning without {{Forgetting}}},
  author = {Li, Zhizhong and Hoiem, Derek},
  date = {2017-02-14},
  eprint = {1606.09282},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.09282},
  urldate = {2024-09-20},
  abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Conference version appears in ECCV 2016; updated with journal version}
}

@software{linHibayesianAwesomeautomlpapers2025,
  title = {Hibayesian/Awesome-Automl-Papers},
  author = {Lin, Mark},
  date = {2025-11-09T19:36:09Z},
  origdate = {2017-11-20T08:19:19Z},
  url = {https://github.com/hibayesian/awesome-automl-papers},
  urldate = {2025-11-13},
  abstract = {A curated list of automated machine learning papers, articles, tutorials, slides and projects},
  keywords = {automated-feature-engineering,automl,hyperparameter-optimization,neural-architecture-search},
  annotation = {Programmers: \_:n1654}
}

@software{linHibayesianAwesomeautomlpapers2025a,
  title = {Hibayesian/Awesome-Automl-Papers},
  author = {Lin, Mark},
  date = {2025-11-15T23:47:33Z},
  origdate = {2017-11-20T08:19:19Z},
  url = {https://github.com/hibayesian/awesome-automl-papers},
  urldate = {2025-11-16},
  abstract = {A curated list of automated machine learning papers, articles, tutorials, slides and projects},
  keywords = {automated-feature-engineering,automl,hyperparameter-optimization,neural-architecture-search}
}

@online{linUnleashGraphNeural2024,
  title = {Unleash {{Graph Neural Networks}} from {{Heavy Tuning}}},
  author = {Lin, Lequan and Shi, Dai and Han, Andi and Wang, Zhiyong and Gao, Junbin},
  date = {2024-05-21},
  eprint = {2405.12521},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2405.12521},
  urldate = {2024-11-05},
  abstract = {Graph Neural Networks (GNNs) are deep-learning architectures designed for graph-type data, where understanding relationships among individual observations is crucial. However, achieving promising GNN performance, especially on unseen data, requires comprehensive hyperparameter tuning and meticulous training. Unfortunately, these processes come with high computational costs and significant human effort. Additionally, conventional searching algorithms such as grid search may result in overfitting on validation data, diminishing generalization accuracy. To tackle these challenges, we propose a graph conditional latent diffusion framework (GNN-Diff) to generate high-performing GNNs directly by learning from checkpoints saved during a light-tuning coarse search. Our method: (1) unleashes GNN training from heavy tuning and complex search space design; (2) produces GNN parameters that outperform those obtained through comprehensive grid search; and (3) establishes higher-quality generation for GNNs compared to diffusion frameworks designed for general neural networks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning}
}

@online{lipskySpectralSequenceParallelized2015,
  title = {A Spectral Sequence for Parallelized Persistence},
  author = {Lipsky, David and Skraba, Primoz and Vejdemo-Johansson, Mikael},
  date = {2015-03-19},
  eprint = {1112.1245},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1112.1245},
  url = {http://arxiv.org/abs/1112.1245},
  urldate = {2025-11-12},
  abstract = {We approach the problem of the computation of persistent homology for large datasets by a divide-and-conquer strategy. Dividing the total space into separate but overlapping components, we are able to limit the total memory residency for any part of the computation, while not degrading the overall complexity much. Locally computed persistence information is then merged from the components and their intersections using a spectral sequence generalizing the Mayer-Vietoris long exact sequence.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Distributed Parallel and Cluster Computing,Mathematics - Algebraic Topology},
  note = {Comment: 15 pages, 10 figures, submitted to the ACM Symposium on Computational Geometry},
  file = {C:\Users\Aymen\Zotero\storage\UK7TPUPL\Lipsky et al. - 2015 - A spectral sequence for parallelized persistence.pdf}
}

@online{liuKANKolmogorovArnoldNetworks2025,
  title = {{{KAN}}: {{Kolmogorov-Arnold Networks}}},
  shorttitle = {{{KAN}}},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
  date = {2025-02-09},
  eprint = {2404.19756},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.19756},
  url = {http://arxiv.org/abs/2404.19756},
  urldate = {2025-11-16},
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  note = {Comment: Accepted by International Conference on Learning Representations (ICLR) 2025 (conference version: https://openreview.net/forum?id=Ozo7qJ5vZi). Codes are available at https://github.com/KindXiaoming/pykan},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\R9GQ2AW7\\Liu et al. - 2025 - KAN Kolmogorov-Arnold Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\ZEYUBG3Z\\2404.html}
}

@article{liuMachineUnlearningGenerative2024,
  title = {Machine {{Unlearning}} in {{Generative AI}}: {{A Survey}}},
  author = {Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  date = {2024-07-01},
  pages = {arXiv:2407.20516},
  doi = {10.48550/arXiv.2407.20516},
  abstract = {Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.},
  keywords = {Computer Science - Artificial,Computer Science - Computation and Language,Computer Science - Machine Learning,Intelligence}
}

@online{liuOmnigrokGrokkingAlgorithmic2022,
  title = {Omnigrok: {{Grokking Beyond Algorithmic Data}}},
  shorttitle = {Omnigrok},
  author = {Liu, Ziming and Michaud, Eric J. and Tegmark, Max},
  date = {2022-10-03},
  url = {https://arxiv.org/abs/2210.01117v2},
  urldate = {2024-10-03},
  abstract = {Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {grokking}
}

@online{liuReLUNeuralNetworks2023,
  title = {{{ReLU Neural Networks}}, {{Polyhedral Decompositions}}, and {{Persistent Homolog}}},
  author = {Liu, Yajing and Cole, Christina M. and Peterson, Chris and Kirby, Michael},
  date = {2023-06-30},
  eprint = {2306.17418},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2306.17418},
  url = {http://arxiv.org/abs/2306.17418},
  urldate = {2025-11-16},
  abstract = {A ReLU neural network leads to a finite polyhedral decomposition of input space and a corresponding finite dual graph. We show that while this dual graph is a coarse quantization of input space, it is sufficiently robust that it can be combined with persistent homology to detect homological signals of manifolds in the input space from samples. This property holds for a variety of networks trained for a wide range of purposes that have nothing to do with this topological application. We found this feature to be surprising and interesting; we hope it will also be useful.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - Optimization and Control},
  note = {Comment: Accepted by Proceedings of the 2 nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at the 40 th In- ternational Conference on Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\IQYVJF9Q\\Liu et al. - 2023 - ReLU Neural Networks, Polyhedral Decompositions, and Persistent Homolog.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\IZCDKKL7\\2306.html}
}

@online{liuTopologicalStructureComplex2022,
  title = {Topological Structure of Complex Predictions},
  author = {Liu, Meng and Dey, Tamal K. and Gleich, David F.},
  date = {2022-10-20},
  eprint = {2207.14358},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.14358},
  url = {http://arxiv.org/abs/2207.14358},
  urldate = {2025-11-16},
  abstract = {Complex prediction models such as deep learning are the output from fitting machine learning, neural networks, or AI models to a set of training data. These are now standard tools in science. A key challenge with the current generation of models is that they are highly parameterized, which makes describing and interpreting the prediction strategies difficult. We use topological data analysis to transform these complex prediction models into pictures representing a topological view. The result is a map of the predictions that enables inspection. The methods scale up to large datasets across different domains and enable us to detect labeling errors in training data, understand generalization in image classification, and inspect predictions of likely pathogenic mutations in the BRCA1 gene.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Mathematics - Algebraic Topology},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\222WBZJP\\Liu et al. - 2022 - Topological structure of complex predictions.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\G2K6F26C\\2207.html}
}

@book{livanIntroductionRandomMatrices2018,
  title = {Introduction to {{Random Matrices}} - {{Theory}} and {{Practice}}},
  author = {Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
  date = {2018},
  volume = {26},
  eprint = {1712.07903},
  eprinttype = {arXiv},
  eprintclass = {math-ph},
  doi = {10.1007/978-3-319-70885-0},
  url = {http://arxiv.org/abs/1712.07903},
  urldate = {2025-11-13},
  abstract = {This is a book for absolute beginners. If you have heard about random matrix theory, commonly denoted RMT, but you do not know what that is, then welcome!, this is the place for you. Our aim is to provide a truly accessible introductory account of RMT for physicists and mathematicians at the beginning of their research career. We tried to write the sort of text we would have loved to read when we were beginning Ph.D. students ourselves. Our book is structured with light and short chapters, and the style is informal. The calculations we found most instructive are spelt out in full. Particular attention is paid to the numerical verification of most analytical results. Our book covers standard material - classical ensembles, orthogonal polynomial techniques, spectral densities and spacings - but also more advanced and modern topics - replica approach and free probability - that are not normally included in elementary accounts on RMT. This book is dedicated to the fond memory of Oriol Bohigas.},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics},
  file = {C:\Users\Aymen\Zotero\storage\GQ4SL93J\Livan et al. - 2018 - Introduction to Random Matrices - Theory and Practice.pdf}
}

@article{liVisualizingLossLandscape2017,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2017-12-01},
  pages = {arXiv:1712.09913},
  doi = {10.48550/arXiv.1712.09913},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  keywords = {Computer Science - Computer,Computer Science - Machine Learning,Statistics - Machine Learning,Vision and Pattern Recognition},
  note = {NIPS 2018 (extended version, 10.5 pages), code is available at https://github.com/tomgoldstein/loss-landscape}
}

@article{liVisualizingRethinkingMining2024,
  title = {Visualizing, {{Rethinking}}, and {{Mining}} the {{Loss Landscape}} of {{Deep Neural Networks}}},
  author = {Li, Xin-Chun and Li, Lan and Zhan, De-Chuan},
  date = {2024-05-01},
  pages = {arXiv:2405.12493},
  doi = {10.48550/arXiv.2405.12493},
  abstract = {The loss landscape of deep neural networks (DNNs) is commonly considered complex and wildly fluctuated. However, an interesting observation is that the loss surfaces plotted along Gaussian noise directions are almost v-basin ones with the perturbed model lying on the basin. This motivates us to rethink whether the 1D or 2D subspace could cover more complex local geometry structures, and how to mine the corresponding perturbation directions. This paper systematically and gradually categorizes the 1D curves from simple to complex, including v-basin, v-side, w-basin, w-peak, and vvv-basin curves. Notably, the latter two types are already hard to obtain via the intuitive construction of specific perturbation directions, and we need to propose proper mining algorithms to plot the corresponding 1D curves. Combining these 1D directions, various types of 2D surfaces are visualized such as the saddle surfaces and the bottom of a bottle of wine that are only shown by demo functions in previous works. Finally, we propose theoretical insights from the lens of the Hessian matrix to explain the observed several interesting phenomena.},
  keywords = {Computer Science - Machine Learning}
}

@online{loiseauxFrameworkFastStable2023,
  title = {A {{Framework}} for {{Fast}} and {{Stable Representations}} of {{Multiparameter Persistent Homology Decompositions}}},
  author = {Loiseaux, David and Carrière, Mathieu and Blumberg, Andrew J.},
  date = {2023-06-19},
  eprint = {2306.11170},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.11170},
  url = {http://arxiv.org/abs/2306.11170},
  urldate = {2025-11-16},
  abstract = {Topological data analysis (TDA) is an area of data science that focuses on using invariants from algebraic topology to provide multiscale shape descriptors for geometric data sets such as point clouds. One of the most important such descriptors is persistent homology, which encodes the change in shape as a filtration parameter changes; a typical parameter is the feature scale. For many data sets, it is useful to simultaneously vary multiple filtration parameters, for example feature scale and density. While the theoretical properties of single parameter persistent homology are well understood, less is known about the multiparameter case. In particular, a central question is the problem of representing multiparameter persistent homology by elements of a vector space for integration with standard machine learning algorithms. Existing approaches to this problem either ignore most of the multiparameter information to reduce to the one-parameter case or are heuristic and potentially unstable in the face of noise. In this article, we introduce a new general representation framework that leverages recent results on decompositions of multiparameter persistent homology. This framework is rich in information, fast to compute, and encompasses previous approaches. Moreover, we establish theoretical stability guarantees under this framework as well as efficient algorithms for practical computation, making this framework an applicable and versatile tool for analyzing geometric and point cloud data. We validate our stability results and algorithms with numerical experiments that demonstrate statistical convergence, prediction accuracy, and fast running times on several real data sets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  file = {C:\Users\Aymen\Zotero\storage\YH6EC8XJ\Loiseaux et al. - 2023 - A Framework for Fast and Stable Representations of Multiparameter Persistent Homology Decompositions.pdf}
}

@online{loiseauxMultiparameterModuleApproximation2022,
  title = {Multi-Parameter {{Module Approximation}}: An Efficient and Interpretable Invariant for Multi-Parameter Persistence Modules with Guarantees},
  shorttitle = {Multi-Parameter {{Module Approximation}}},
  author = {Loiseaux, David and Carrière, Mathieu and Blumberg, Andrew J.},
  date = {2022},
  doi = {10.48550/ARXIV.2206.02026},
  url = {https://arxiv.org/abs/2206.02026},
  urldate = {2025-11-12},
  abstract = {In this article, we introduce a new parameterized family of topological descriptors, taking the form of candidate decompositions, for multi-parameter persistence modules, and we identify a subfamily of these descriptors, that we call approximate decompositions, that are controllable approximations, in the sense that they preserve diagonal barcodes. Then, we introduce MMA (Multipersistence Module Approximation): an algorithm based on matching functions for computing instances of candidate decompositions with some precision parameter δ \&gt; 0. By design, MMA can handle an arbitrary number of filtrations, and has bounded complexity and running time. Moreover, we prove the robustess of MMA: when computed with so-called compatible matching functions, we show that MMA produces approximate decompositions (and we prove that such matching functions exist for n = 2 filtrations). Next, we restrict the focus on modules that can be decomposed into interval summands. In that case, compatible matching functions always exist, and we show that, for small enough δ, the approximate decompositions obtained with such compatible matching functions by MMA have an approximation error (in terms of the standard interleaving and bottleneck distances) that is bounded by δ, and that reaches zero for an even smaller, positive precision. Finally, we present empirical evidence validating that MMA has state-of-the-art performance and running time on several data sets.},
  pubstate = {prepublished},
  version = {4},
  keywords = {Algebraic Topology (math.AT),Computational Geometry (cs.CG),FOS: Computer and information sciences,FOS: Mathematics}
}

@article{loiseauxMultiparameterModuleApproximation2025,
  title = {Multi-Parameter {{Module Approximation}}: An Efficient and Interpretable Invariant for Multi-Parameter Persistence Modules with Guarantees},
  shorttitle = {Multi-Parameter {{Module Approximation}}},
  author = {Loiseaux, David and Carrière, Mathieu and Blumberg, Andrew J.},
  date = {2025-12},
  journaltitle = {J Appl. and Comput. Topology},
  volume = {9},
  number = {4},
  eprint = {2206.02026},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {26},
  issn = {2367-1726, 2367-1734},
  doi = {10.1007/s41468-025-00222-y},
  url = {http://arxiv.org/abs/2206.02026},
  urldate = {2025-11-16},
  abstract = {In this article, we introduce a new parameterized family of topological descriptors, taking the form of candidate decompositions, for multi-parameter persistence modules, and we identify a subfamily of these descriptors, that we call approximate decompositions, that are controllable approximations, in the sense that they preserve diagonal barcodes. Then, we introduce MMA (Multipersistence Module Approximation): an algorithm based on matching functions for computing instances of candidate decompositions with some precision parameter δ {$>$} 0. By design, MMA can handle an arbitrary number of filtrations, and has bounded complexity and running time. Moreover, we prove the robustess of MMA: when computed with so-called compatible matching functions, we show that MMA produces approximate decompositions (and we prove that such matching functions exist for n = 2 filtrations). Next, we restrict the focus on modules that can be decomposed into interval summands. In that case, compatible matching functions always exist, and we show that, for small enough δ, the approximate decompositions obtained with such compatible matching functions by MMA have an approximation error (in terms of the standard interleaving and bottleneck distances) that is bounded by δ, and that reaches zero for an even smaller, positive precision. Finally, we present empirical evidence validating that MMA has state-of-the-art performance and running time on several data sets.},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\C8CWL4UA\\Loiseaux et al. - 2025 - Multi-parameter Module Approximation an efficient and interpretable invariant for multi-parameter p.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\NEEMWWKK\\2206.html}
}

@article{loiseauxMultipersMultiparameterPersistence2024,
  title = {Multipers: {{Multiparameter Persistence}} for {{MachineLearning}}},
  shorttitle = {Multipers},
  author = {Loiseaux, David and Schreiber, Hannah},
  date = {2024-11-14},
  journaltitle = {JOSS},
  volume = {9},
  number = {103},
  pages = {6773},
  issn = {2475-9066},
  doi = {10.21105/joss.06773},
  url = {https://joss.theoj.org/papers/10.21105/joss.06773},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\AVDEQ326\Loiseaux et Schreiber - 2024 - multipers Multiparameter Persistence for MachineLearning.pdf}
}

@article{loiseauxMultipersMultiparameterPersistence2024a,
  title = {Multipers: {{Multiparameter Persistence}} for {{Machine Learning}}},
  shorttitle = {Multipers},
  author = {Loiseaux, David and Schreiber, Hannah},
  date = {2024-11-14},
  journaltitle = {Journal of Open Source Software},
  volume = {9},
  number = {103},
  pages = {6773},
  issn = {2475-9066},
  doi = {10.21105/joss.06773},
  url = {https://joss.theoj.org/papers/10.21105/joss.06773},
  urldate = {2025-11-16},
  abstract = {Loiseaux et al., (2024). multipers: Multiparameter Persistence for Machine Learning. Journal of Open Source Software, 9(103), 6773, https://doi.org/10.21105/joss.06773},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\MFHWAISR\Loiseaux et Schreiber - 2024 - multipers Multiparameter Persistence for Machine Learning.pdf}
}

@article{loiseauxStableVectorizationMultiparameter,
  title = {Stable {{Vectorization}} of {{Multiparameter Persistent Homology}} Using {{Signed Barcodes}} as {{Measures}}},
  author = {Loiseaux, David and Scoccola, Luis and Carrière, Mathieu and Botnan, Magnus B and Oudot, Steve},
  abstract = {Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case—where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest—and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes—a recent family of MPH descriptors—as signed measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. The resulting feature vectors are easy to define and to compute, and provably stable. While, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\97MRCVQC\Loiseaux et al. - Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures.pdf}
}

@online{loiseauxStableVectorizationMultiparameter2024,
  title = {Stable {{Vectorization}} of {{Multiparameter Persistent Homology}} Using {{Signed Barcodes}} as {{Measures}}},
  author = {Loiseaux, David and Scoccola, Luis and Carrière, Mathieu and Botnan, Magnus Bakke and Oudot, Steve},
  date = {2024-02-07},
  eprint = {2306.03801},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.03801},
  url = {http://arxiv.org/abs/2306.03801},
  urldate = {2025-11-16},
  abstract = {Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent family of MPH descriptors -- as signed measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. The resulting feature vectors are easy to define and to compute, and provably stable. While, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  note = {Comment: 26 pages, 4 figures, 9 tables; v2: final version in NeurIPS 2023},
  file = {C:\Users\Aymen\Zotero\storage\LI497G5C\Loiseaux et al. - 2024 - Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures.pdf}
}

@online{loiseauxStableVectorizationMultiparameter2024a,
  title = {Stable {{Vectorization}} of {{Multiparameter Persistent Homology}} Using {{Signed Barcodes}} as {{Measures}}},
  author = {Loiseaux, David and Scoccola, Luis and Carrière, Mathieu and Botnan, Magnus Bakke and Oudot, Steve},
  date = {2024-02-07},
  eprint = {2306.03801},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.03801},
  url = {http://arxiv.org/abs/2306.03801},
  urldate = {2025-11-16},
  abstract = {Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent family of MPH descriptors -- as signed measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. The resulting feature vectors are easy to define and to compute, and provably stable. While, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  note = {Comment: 26 pages, 4 figures, 9 tables; v2: final version in NeurIPS 2023},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\B9TUCW88\\Loiseaux et al. - 2024 - Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\4KVF3SDS\\2306.html}
}

@online{luoLargeScaleGenerativeDataFree2020,
  title = {Large-{{Scale Generative Data-Free Distillation}}},
  author = {Luo, Liangchen and Sandler, Mark and Lin, Zi and Zhmoginov, Andrey and Howard, Andrew},
  date = {2020-12-10},
  eprint = {2012.05578},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.05578},
  url = {http://arxiv.org/abs/2012.05578},
  urldate = {2024-09-20},
  abstract = {Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets. To this end, we propose a new method to train a generative image model by leveraging the intrinsic normalization layers' statistics of the trained teacher network. This enables us to build an ensemble of generators without training data that can efficiently produce substitute inputs for subsequent distillation. The proposed method pushes forward the data-free distillation performance on CIFAR-10 and CIFAR-100 to 95.02\% and 77.02\% respectively. Furthermore, we are able to scale it to ImageNet dataset, which to the best of our knowledge, has never been done using generative models in a data-free setting.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@online{MachineUnlearningRight2024,
  title = {Machine {{Unlearning}}: {{The Right}} to Be {{Forgotten}}},
  shorttitle = {Machine {{Unlearning}}},
  date = {2024-09-20},
  url = {https://kaggle.com/code/tamlhp/machine-unlearning-the-right-to-be-forgotten},
  urldate = {2024-09-20},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from 2023 Kaggle AI Report},
  langid = {english},
  keywords = {The right to be forgotten}
}

@inreference{MahalanobisDistance2024,
  title = {Mahalanobis Distance},
  booktitle = {Wikipedia},
  date = {2024-06-23T09:30:40Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mahalanobis_distance&oldid=1230540622},
  urldate = {2024-09-20},
  abstract = {The Mahalanobis distance is a measure of the distance between a point                         P                 \{\textbackslash displaystyle P\}     and a distribution                         D                 \{\textbackslash displaystyle D\}    , introduced by P. C. Mahalanobis in 1936. The mathematical details of Mahalanobis distance has appeared in the Journal of The Asiatic Society of Bengal. Mahalanobis's definition was prompted by the problem of identifying the similarities of skulls based on measurements (the earliest work related to similarities of skulls are from 1922 and another later work is from 1927). The sampling distribution of Mahalanobis distance has been obtained by Professor R.C. Bose, under the assumption of equal dispersion. It is a multivariate generalization of the square of the standard score                         z         =         (         x         −         μ         )                    /                  σ                 \{\textbackslash displaystyle z=(x-\textbackslash mu )/\textbackslash sigma \}    : how many standard deviations away                         P                 \{\textbackslash displaystyle P\}     is from the mean of                         D                 \{\textbackslash displaystyle D\}    . This distance is zero for                         P                 \{\textbackslash displaystyle P\}     at the mean of                         D                 \{\textbackslash displaystyle D\}     and grows as                         P                 \{\textbackslash displaystyle P\}     moves away from the mean along each principal component axis. If each of these axes is re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. The Mahalanobis distance is thus unitless, scale-invariant, and takes into account the correlations of the data set.},
  langid = {english},
  annotation = {Page Version ID: 1230540622}
}

@article{malviyaPredictingImpactModel2024,
  title = {Predicting the {{Impact}} of {{Model Expansion}} through the {{Minima Manifold}}: {{A Loss Landscape Perspective}}},
  author = {Malviya, Pranshu and Huang, Jerry and Fournier, Quentin and Chandar, Sarath},
  date = {2024-05-01},
  pages = {arXiv:2405.15895},
  doi = {10.48550/arXiv.2405.15895},
  abstract = {The optimal model for a given task is often challenging to determine, requiring training multiple models from scratch which becomes prohibitive as dataset and model sizes grow. A more efficient alternative is to reuse smaller pre-trained models by expanding them, however, this is not widely adopted as how this impacts training dynamics remains poorly understood. While prior works have introduced statistics to measure these effects, they remain flawed. To rectify this, we offer a new approach for understanding and quantifying the impact of expansion through the lens of the loss landscape, which has been shown to contain a manifold of linearly connected minima. Building on this new perspective, we propose a metric to study the impact of expansion by estimating the size of the manifold. Experimental results show a clear relationship between gains in performance and manifold size, enabling the comparison of candidate models and presenting a first step towards expanding models more reliably based on geometric properties of the loss landscape.},
  keywords = {Computer Science - Machine Learning}
}

@online{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  date = {2020-09-17},
  eprint = {1802.03426},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.03426},
  urldate = {2024-09-20},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that is applicable to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Reference implementation available at http://github.com/lmcinnes/umap}
}

@article{mcleishTransformersCanArithmetic2024,
  title = {Transformers {{Can Do Arithmetic}} with the {{Right Embeddings}}},
  author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  date = {2024-05-01},
  pages = {arXiv:2405.17399},
  doi = {10.48550/arXiv.2405.17399},
  abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
  keywords = {Computer Science - Artificial,Computer Science - Machine Learning,Intelligence}
}

@online{mcleishTransformersCanArithmetic2024a,
  title = {Transformers {{Can Do Arithmetic}} with the {{Right Embeddings}}},
  author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
  date = {2024-05-27},
  url = {https://arxiv.org/abs/2405.17399v1},
  urldate = {2024-09-20},
  abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99\% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {Transformer Embedding}
}

@online{mcmahanCommunicationEfficientLearningDeep2023,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and family=Arcas, given=Blaise Agüera, prefix=y, useprefix=false},
  date = {2023-01-26},
  eprint = {1602.05629},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.05629},
  url = {http://arxiv.org/abs/1602.05629},
  urldate = {2024-10-07},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,first,FL},
  note = {Comment: [v4] Fixes a typo in the FedAvg pseudocode. [v3] Updates the large-scale LSTM experiments, along with other minor changes}
}

@online{meilaManifoldLearningWhat2023,
  title = {Manifold Learning: What, How, and Why},
  shorttitle = {Manifold Learning},
  author = {Meilă, Marina and Zhang, Hanyu},
  date = {2023-11-07},
  eprint = {2311.03757},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2311.03757},
  urldate = {2024-09-20},
  abstract = {Manifold learning (ML), known also as non-linear dimension reduction, is a set of methods to find the low dimensional structure of data. Dimension reduction for large, high dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high dimensional point clouds, and allow one to visualize, denoise and interpret them. This survey presents the principles underlying ML, the representative methods, as well as their statistical foundations from a practicing statistician’s perspective. It describes the trade-offs, and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{meng$mathcalG$SGDOptimizingReLU2021,
  title = {\$\textbackslash mathcal\{{{G}}\}\$-{{SGD}}: {{Optimizing ReLU Neural Networks}} in Its {{Positively Scale-Invariant Space}}},
  shorttitle = {\$\textbackslash mathcal\{{{G}}\}\$-{{SGD}}},
  author = {Meng, Qi and Zheng, Shuxin and Zhang, Huishuai and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
  date = {2021-03-23},
  eprint = {1802.03713},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.03713},
  urldate = {2024-09-20},
  abstract = {It has been widely observed that many activation functions and pooling methods of neural network models have (positive-) rescaling-invariant property, including ReLU, PReLU, max-pooling, and average pooling, which makes fully-connected neural networks (FNNs) and convolutional neural networks (CNNs) invariant to (positive) rescaling operation across layers. This may cause unneglectable problems with their optimization: (1) different NN models could be equivalent, but their gradients can be very different from each other; (2) it can be proven that the loss functions may have many spurious critical points in the redundant weight space. To tackle these problems, in this paper, we first characterize the rescalinginvariant properties of NN models using equivalent classes and prove that the dimension of the equivalent class space is significantly smaller than the dimension of the original weight space. Then we represent the loss function in the compact equivalent class space and develop novel algorithms that conduct optimization of the NN models directly in the equivalent class space. We call these algorithms Equivalent Class Optimization (abbreviated as EC-Opt) algorithms. Moreover, we design efficient tricks to compute the gradients in the equivalent class, which almost have no extra computational complexity as compared to standard back-propagation (BP). We conducted experimental study to demonstrate the effectiveness of our proposed new optimization algorithms. In particular, we show that by using the idea of EC-Opt, we can significantly improve the accuracy of the learned model (for both FNN and CNN), as compared to using conventional stochastic gradient descent algorithms.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Equivalent spacemodulo,Statistics - Machine Learning}
}

@article{menzeMultimodalBrainTumor2015,
  title = {The {{Multimodal Brain Tumor Image Segmentation Benchmark}} ({{BRATS}})},
  author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-Andre and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herve and Demiralp, Cagatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jose Antonio and Meier, Raphael and Pereira, Sergio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M. S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
  date = {2015-10},
  journaltitle = {IEEE Trans. Med. Imaging},
  volume = {34},
  number = {10},
  pages = {1993--2024},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2014.2377694},
  url = {http://ieeexplore.ieee.org/document/6975210/},
  urldate = {2025-11-13},
  abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients—manually annotated by up to four raters—and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74\%–85\%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\8ZK93MWR\Menze et al. - 2015 - The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS).pdf}
}

@article{merchantScalingDeepLearning2023,
  title = {Scaling Deep Learning for Materials Discovery},
  author = {Merchant, Amil and Batzner, Simon and Schoenholz, Samuel S. and Aykol, Muratahan and Cheon, Gowoon and Cubuk, Ekin Dogus},
  date = {2023},
  journaltitle = {Nature},
  volume = {624},
  number = {7990},
  pages = {80--85},
  publisher = {Nature Publishing Group UK London},
  url = {https://www.nature.com/articles/s41586-023-06735-9},
  urldate = {2025-11-13}
}

@video{MertonJumpDiffusion2024,
  entrysubtype = {video},
  title = {Merton Jump Diffusion Model},
  date = {2024-09-20},
  url = {https://www.youtube.com/watch?v=URNN2_YPGVI},
  urldate = {2024-09-20},
  abstract = {Derives formula for the price of a European call option under the Merton's Jump Diffusion model.},
  langid = {french},
  keywords = {Time-series}
}

@online{metzGradientsAreNot2022,
  title = {Gradients Are {{Not All You Need}}},
  author = {Metz, Luke and Freeman, C. Daniel and Schoenholz, Samuel S. and Kachman, Tal},
  date = {2022-01-24},
  eprint = {2111.05803},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.05803},
  url = {http://arxiv.org/abs/2111.05803},
  urldate = {2025-11-13},
  abstract = {Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\A6NP99AJ\\Metz et al. - 2022 - Gradients are Not All You Need.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\YTYAVL7N\\2111.html}
}

@article{milicevicAlgorithmsComputingOptimal2023,
  title = {Algorithms for Computing the Optimal {{Gersgorin-type}} Localizations},
  author = {Milicevic, S. and Kostic, V.R.},
  date = {2023},
  journaltitle = {Filomat},
  volume = {37},
  number = {30},
  pages = {10395--10413},
  issn = {0354-5180, 2406-0933},
  doi = {10.2298/FIL2330395M},
  url = {https://doiserbia.nb.rs/Article.aspx?ID=0354-51802330395M},
  urldate = {2024-11-05},
  abstract = {In this paper we provide novel algorithms for computing the minimal    Gersgorin set for the localizations of eigenvalues. Two strategies for    curve tracing are considered: predictor-corrector and triangular grid    approximation. We combine these two strategies with two characterizations    (explicit and implicit) of the Minimal Gersgorin set to obtain four new    numerical algorithms. We show that these algorithms significantly decrease    computational complexity, especially for matrices of large size, and compare    them on matrices that arise in practically important eigenvalue problems.},
  langid = {english}
}

@online{MobilityEdgeEigenvalue2025,
  title = {Mobility Edge Eigenvalue - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=mobility+edge+eigenvalue&client=ms-android-samsung-ss&sca_esv=af69388857e2a179&sxsrf=AE3TifO6wcYAk3EZ8w3xa-6k3_vGn346Uw%3A1757329376063&ei=4Le-aPfPA_itkdUPq4jysAQ&oq=mobility+edge+eigenvalue&gs_lp=EhNtb2JpbGUtZ3dzLXdpei1zZXJwIhhtb2JpbGl0eSBlZGdlIGVpZ2VudmFsdWUyBRAhGKABMgUQIRigATIFECEYnwVIvD1Q4A9YgThwAHgAkAEAmAG0AaAB4yKqAQQwLjMyuAEDyAEA-AEBmAIOoAKyEcICCBAAGIAEGLADwgIJEAAYsAMYCBgewgIOEAAYgAQYsAMYhgMYigXCAgsQABiABBiwAxiiBMICCBAAGLADGO8FwgIKECMYgAQYJxiKBcICBhAAGBYYHsICCxAAGIAEGIYDGIoFwgIFEAAYgATCAggQABgWGAoYHsICCBAAGIAEGKIEwgIFEAAY7wXCAgcQIRigARgKwgIGECEYFRgNmAMAiAYBkAYIkgcEMC4xNKAH58cBsgcEMC4xNLgHshHCBwkyLTYuNi4xLjHIB6gB&sclient=mobile-gws-wiz-serp},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\RMQ656R4\search.html}
}

@video{morganmcguireAuthorsAdaHessianOptimizer2020,
  entrysubtype = {video},
  title = {With the {{Authors}}: {{AdaHessian Optimizer}}},
  shorttitle = {With the {{Authors}}},
  editor = {{Morgan McGuire}},
  editortype = {director},
  date = {2020},
  url = {https://www.youtube.com/watch?v=S87ancnZ0MM},
  urldate = {2024-11-05},
  abstract = {"ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning" Paper presentation and discussion with the fast.ai community of AdaHessian, a new second-order optimizer with promising results:  "(i) achieves 1.80\%/1.45\% higher accuracy on ResNets20/32 on Cifar10, and 5.55\% higher accuracy on ImageNet as compared to ADAM (ii) outperforms ADAMW for transformers by 0.27/0.33 BLEU score on IWSLT14/WMT14 and 1.8/1.0 PPL on PTB/Wikitext-103; and  (iii) achieves 0.032\% better score than AdaGrad for DLRM on the Criteo Ad Kaggle dataset." Paper: https://arxiv.org/abs/2006.00719 Authors repo: https://github.com/amirgholami/adahes... Fastai native AdaHessian optmizer implementation: https://github.com/morganmcg1/ImageNe... Fastai forums discussion thread: https://forums.fast.ai/t/adahessian/7...},
  keywords = {Pytorch optimizers},
  annotation = {Directors: \_:n487\\
Directors: \_:n507}
}

@inproceedings{mullerTopologicalDynamicsFunctional2024,
  title = {Topological {{Dynamics}} of~{{Functional Neural Network Graphs During Reinforcement Learning}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Muller, Matthew and Kroon, Steve and Chalup, Stephan},
  editor = {Luo, Biao and Cheng, Long and Wu, Zheng-Guang and Li, Hongyi and Li, Chaojie},
  date = {2024},
  pages = {190--204},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-99-8138-0_16},
  abstract = {This study investigates the topological structures of neural network activation graphs, with a focus on detecting higher-order Betti numbers during reinforcement learning. The paper presents visualisations of the neurotopological dynamics of reinforcement learning agents both during and after training, which are useful for different dynamics analyses which we explore in this work. Two applications are considered: frame-by-frame analysis of agent neurotopology and tracking per-neuron presence in cavity boundaries over training steps. The experimental analysis suggests that higher-order Betti numbers found in a neural network’s functional graph can be associated with learning more complex behaviours.},
  isbn = {978-981-99-8138-0},
  langid = {english}
}

@online{MultiparameterPersistenceIntroduction2025,
  title = {Multiparameter Persistence Introduction — Multipers Documentation},
  date = {2025-11-12},
  url = {https://davidlapous.github.io/multipers/notebooks/multipers_intro.html#point-clouds},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\K597XY9I\multipers_intro.html}
}

@article{munchInvitationEulerCharacteristic2023,
  title = {An {{Invitation}} to the {{Euler Characteristic Transform}}},
  author = {Munch, Elizabeth},
  date = {2023-10-01},
  pages = {arXiv:2310.10395},
  doi = {10.48550/arXiv.2310.10395},
  abstract = {The Euler characteristic transform (ECT) is a simple to define yet powerful representation of shape. The idea is to encode an embedded shape using sub-level sets of a a function defined based on a given direction, and then returning the Euler characteristics of these sublevel sets. Because the ECT has been shown to be injective on the space of embedded simplicial complexes, it has been used for applications spanning a range of disciplines, including plant morphology and protein structural analysis. In this survey article, we present a comprehensive overview of the Euler characteristic transform, highlighting the main idea on a simple leaf example, and surveying its its key concepts, theoretical foundations, and available applications.},
  keywords = {Computer Science - Computational Geometry}
}

@online{mundtCLEVACompassContinualLearning2022,
  title = {{{CLEVA-Compass}}: {{A Continual Learning EValuation Assessment Compass}} to {{Promote Research Transparency}} and {{Comparability}}},
  shorttitle = {{{CLEVA-Compass}}},
  author = {Mundt, Martin and Lang, Steven and Delfosse, Quentin and Kersting, Kristian},
  date = {2022-02-01},
  eprint = {2110.03331},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.03331},
  url = {http://arxiv.org/abs/2110.03331},
  urldate = {2025-11-16},
  abstract = {What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation. The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions. In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass, CLEVA-Compass for short. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape. In addition to promoting compact specification in the spirit of recent replication trends, the CLEVA-Compass thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: International Conference on Learning Representations (ICLR) 2022},
  file = {C:\Users\Aymen\Zotero\storage\N7JX98FX\Mundt et al. - 2022 - CLEVA-Compass A Continual Learning EValuation Assessment Compass to Promote Research Transparency a.pdf}
}

@article{murtyGrokkingHierarchicalStructure2023,
  title = {Grokking of {{Hierarchical Structure}} in {{Vanilla Transformers}}},
  author = {Murty, Shikhar and Sharma, Pratyusha and Andreas, Jacob and Manning, Christopher D.},
  date = {2023-05-01},
  pages = {arXiv:2305.18741},
  doi = {10.48550/arXiv.2305.18741},
  abstract = {For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods -- far beyond the point when in-domain accuracy has saturated. We call this phenomenon \textbackslash emph\{structural grokking\}. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of \textbackslash citet\{murty2023projections\}. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure.},
  keywords = {Computer Science - Computation and Language},
  note = {ACL 2023}
}

@online{naitzatTopologyDeepNeural2020,
  title = {Topology of Deep Neural Networks},
  author = {Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
  date = {2020-04-13},
  eprint = {2004.06093},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.06093},
  url = {http://arxiv.org/abs/2004.06093},
  urldate = {2025-11-16},
  abstract = {We study how the topology of a data set \$M = M\_a \textbackslash cup M\_b \textbackslash subseteq \textbackslash mathbb\{R\}\textasciicircum d\$, representing two classes \$a\$ and \$b\$ in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., with perfect accuracy on training set and near-zero generalization error (\$\textbackslash approx 0.01\textbackslash\%\$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of \$M\$ we begin with, when passed through a well-trained neural network \$f : \textbackslash mathbb\{R\}\textasciicircum d \textbackslash to \textbackslash mathbb\{R\}\textasciicircum p\$, there is a vast reduction in the Betti numbers of both components \$M\_a\$ and \$M\_b\$; in fact they nearly always reduce to their lowest possible values: \$β\_k\textbackslash bigl(f(M\_i)\textbackslash bigr) = 0\$ for \$k \textbackslash ge 1\$ and \$β\_0\textbackslash bigl(f(M\_i)\textbackslash bigr) = 1\$, \$i =a, b\$. Furthermore, (2) the reduction in Betti numbers is significantly faster for ReLU activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  note = {Comment: 34 pages, 22 figures},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\EZ5FSYCM\\Naitzat et al. - 2020 - Topology of deep neural networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\IIC4LR4L\\2004.html}
}

@online{NeuralTangentKernel2025,
  title = {Neural Tangent Kernel: {{Revision}} History - {{Wikipedia}}},
  shorttitle = {Neural Tangent Kernel},
  date = {2025-11-13},
  url = {https://en.wikipedia.org/w/index.php?title=Neural_tangent_kernel&action=history},
  urldate = {2025-11-13},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\XX9MQH3U\index.html}
}

@online{NeuralTangentKernel2025a,
  title = {The {{Neural Tangent Kernel}}},
  date = {2025-11-13},
  url = {https://rbcborealis.com/research-blogs/the-neural-tangent-kernel/},
  urldate = {2025-11-13},
  abstract = {In this blog, we examine the linear behaviour in infinitely wide networks, and delve into the concept of Neural Tangent Kernel (NTK).},
  langid = {american},
  organization = {RBC Borealis},
  file = {C:\Users\Aymen\Zotero\storage\HKDK6HAH\the-neural-tangent-kernel.html}
}

@online{NeuralTangentKernels2025,
  title = {Neural {{Tangent Kernels}} — {{PyTorch Tutorials}} 2.9.0+cu128 Documentation},
  date = {2025-11-13},
  url = {https://docs.pytorch.org/tutorials/intermediate/neural_tangent_kernels.html},
  urldate = {2025-11-13},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\HGX6DIC5\neural_tangent_kernels.html}
}

@online{nguyenConnectedSublevelSets2019,
  title = {On {{Connected Sublevel Sets}} in {{Deep Learning}}},
  author = {Nguyen, Quynh},
  date = {2019-05-14},
  eprint = {1901.07417},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1901.07417},
  url = {http://arxiv.org/abs/1901.07417},
  urldate = {2025-11-16},
  abstract = {This paper shows that every sublevel set of the loss function of a class of deep over-parameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Accepted at ICML 2019. More discussions and visualizations are added},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\T2N6K7R4\\Nguyen - 2019 - On Connected Sublevel Sets in Deep Learning.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\Q6RLCXSG\\1901.html}
}

@online{nguyenSynthesizingPreferredInputs2016,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  date = {2016-05-30},
  url = {https://arxiv.org/abs/1605.09304v5},
  urldate = {2024-09-20},
  abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
  langid = {english},
  organization = {arXiv.org},
  keywords = {DD,perfect input}
}

@online{novakBayesianDeepConvolutional2020,
  title = {Bayesian {{Deep Convolutional Networks}} with {{Many Channels}} Are {{Gaussian Processes}}},
  author = {Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2020-08-24},
  eprint = {1810.05148},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1810.05148},
  url = {http://arxiv.org/abs/1810.05148},
  urldate = {2025-11-13},
  abstract = {There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible. Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance, beneficial in finite channel CNNs trained with stochastic gradient descent (SGD), is guaranteed to play no role in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2019},
  file = {C:\Users\Aymen\Zotero\storage\F5H5ZZPI\Novak et al. - 2020 - Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes.pdf}
}

@online{novakNeuralTangentsFast2019,
  title = {Neural {{Tangents}}: {{Fast}} and {{Easy Infinite Neural Networks}} in {{Python}}},
  shorttitle = {Neural {{Tangents}}},
  author = {Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander A. and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
  date = {2019-12-06},
  eprint = {1912.02803},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1912.02803},
  url = {http://arxiv.org/abs/1912.02803},
  urldate = {2025-11-13},
  abstract = {Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space. The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. Neural Tangents is available at www.github.com/google/neural-tangents. We also provide an accompanying interactive Colab notebook.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\6E8NDRR9\\Novak et al. - 2019 - Neural Tangents Fast and Easy Infinite Neural Networks in Python.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\XLR3SKB2\\1912.html}
}

@online{novakSensitivityGeneralizationNeural2018,
  title = {Sensitivity and {{Generalization}} in {{Neural Networks}}: An {{Empirical Study}}},
  shorttitle = {Sensitivity and {{Generalization}} in {{Neural Networks}}},
  author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2018-06-20},
  eprint = {1802.08760},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1802.08760},
  url = {http://arxiv.org/abs/1802.08760},
  urldate = {2025-11-13},
  abstract = {In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization \$-\$ such as full-batch training or using random labels \$-\$ correspond to lower robustness, while factors associated with good generalization \$-\$ such as data augmentation and ReLU non-linearities \$-\$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2018},
  file = {C:\Users\Aymen\Zotero\storage\7QLRWD4I\Novak et al. - 2018 - Sensitivity and Generalization in Neural Networks an Empirical Study.pdf}
}

@online{openaiGPT4oSystemCard2024,
  title = {{{GPT-4o System Card}}},
  author = {{OpenAI} and Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and Mądry, Aleksander and Baker-Whitcomb, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and McGrew, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and McLeavey, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and family=Lohmann, given=Fred, prefix=von, useprefix=false and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya and Kanitscheider, Ingmar and Gulrajani, Ishaan and Coxon, Jacob and Menick, Jacob and Pachocki, Jakub and Aung, James and Betker, James and Crooks, James and Lennon, James and Kiros, Jamie and Leike, Jan and Park, Jane and Kwon, Jason and Phang, Jason and Teplitz, Jason and Wei, Jason and Wolfe, Jason and Chen, Jay and Harris, Jeff and Varavva, Jenia and Lee, Jessica Gan and Shieh, Jessica and Lin, Ji and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Jang, Joanne and Candela, Joaquin Quinonero and Beutler, Joe and Landers, Joe and Parish, Joel and Heidecke, Johannes and Schulman, John and Lachman, Jonathan and McKay, Jonathan and Uesato, Jonathan and Ward, Jonathan and Kim, Jong Wook and Huizinga, Joost and Sitkin, Jordan and Kraaijeveld, Jos and Gross, Josh and Kaplan, Josh and Snyder, Josh and Achiam, Joshua and Jiao, Joy and Lee, Joyce and Zhuang, Juntang and Harriman, Justyn and Fricke, Kai and Hayashi, Kai and Singhal, Karan and Shi, Katy and Karthik, Kavin and Wood, Kayla and Rimbach, Kendra and Hsu, Kenny and Nguyen, Kenny and Gu-Lemberg, Keren and Button, Kevin and Liu, Kevin and Howe, Kiel and Muthukumar, Krithika and Luther, Kyle and Ahmad, Lama and Kai, Larry and Itow, Lauren and Workman, Lauren and Pathak, Leher and Chen, Leo and Jing, Li and Guy, Lia and Fedus, Liam and Zhou, Liang and Mamitsuka, Lien and Weng, Lilian and McCallum, Lindsay and Held, Lindsey and Ouyang, Long and Feuvrier, Louis and Zhang, Lu and Kondraciuk, Lukas and Kaiser, Lukasz and Hewitt, Luke and Metz, Luke and Doshi, Lyric and Aflak, Mada and Simens, Maddie and Boyd, Madelaine and Thompson, Madeleine and Dukhan, Marat and Chen, Mark and Gray, Mark and Hudnall, Mark and Zhang, Marvin and Aljubeh, Marwan and Litwin, Mateusz and Zeng, Matthew and Johnson, Max and Shetty, Maya and Gupta, Mayank and Shah, Meghan and Yatbaz, Mehmet and Yang, Meng Jia and Zhong, Mengchao and Glaese, Mia and Chen, Mianna and Janner, Michael and Lampe, Michael and Petrov, Michael and Wu, Michael and Wang, Michele and Fradin, Michelle and Pokrass, Michelle and Castro, Miguel and family=Castro, given=Miguel Oom Temudo, prefix=de, useprefix=false and Pavlov, Mikhail and Brundage, Miles and Wang, Miles and Khan, Minal and Murati, Mira and Bavarian, Mo and Lin, Molly and Yesildal, Murat and Soto, Nacho and Gimelshein, Natalia and Cone, Natalie and Staudacher, Natalie and Summers, Natalie and LaFontaine, Natan and Chowdhury, Neil and Ryder, Nick and Stathas, Nick and Turley, Nick and Tezak, Nik and Felix, Niko and Kudige, Nithanth and Keskar, Nitish and Deutsch, Noah and Bundick, Noel and Puckett, Nora and Nachum, Ofir and Okelola, Ola and Boiko, Oleg and Murk, Oleg and Jaffe, Oliver and Watkins, Olivia and Godement, Olivier and Campbell-Moore, Owen and Chao, Patrick and McMillan, Paul and Belov, Pavel and Su, Peng and Bak, Peter and Bakkum, Peter and Deng, Peter and Dolan, Peter and Hoeschele, Peter and Welinder, Peter and Tillet, Phil and Pronin, Philip and Tillet, Philippe and Dhariwal, Prafulla and Yuan, Qiming and Dias, Rachel and Lim, Rachel and Arora, Rahul and Troll, Rajan and Lin, Randall and Lopes, Rapha Gontijo and Puri, Raul and Miyara, Reah and Leike, Reimar and Gaubert, Renaud and Zamani, Reza and Wang, Ricky and Donnelly, Rob and Honsby, Rob and Smith, Rocky and Sahai, Rohan and Ramchandani, Rohit and Huet, Romain and Carmichael, Rory and Zellers, Rowan and Chen, Roy and Chen, Ruby and Nigmatullin, Ruslan and Cheu, Ryan and Jain, Saachi and Altman, Sam and Schoenholz, Sam and Toizer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Culver, Sara and Ethersmith, Scott and Gray, Scott and Grove, Sean and Metzger, Sean and Hermani, Shamez and Jain, Shantanu and Zhao, Shengjia and Wu, Sherwin and Jomoto, Shino and Wu, Shirong and {Shuaiqi} and {Xia} and Phene, Sonia and Papay, Spencer and Narayanan, Srinivas and Coffey, Steve and Lee, Steve and Hall, Stewart and Balaji, Suchir and Broda, Tal and Stramer, Tal and Xu, Tao and Gogineni, Tarun and Christianson, Taya and Sanders, Ted and Patwardhan, Tejal and Cunninghman, Thomas and Degry, Thomas and Dimson, Thomas and Raoux, Thomas and Shadwell, Thomas and Zheng, Tianhao and Underwood, Todd and Markov, Todor and Sherbakov, Toki and Rubin, Tom and Stasi, Tom and Kaftan, Tomer and Heywood, Tristan and Peterson, Troy and Walters, Tyce and Eloundou, Tyna and Qi, Valerie and Moeller, Veit and Monaco, Vinnie and Kuo, Vishal and Fomenko, Vlad and Chang, Wayne and Zheng, Weiyi and Zhou, Wenda and Manassra, Wesam and Sheu, Will and Zaremba, Wojciech and Patil, Yash and Qian, Yilei and Kim, Yongjik and Cheng, Youlong and Zhang, Yu and He, Yuchen and Zhang, Yuchen and Jin, Yujia and Dai, Yunxing and Malkov, Yury},
  date = {2024-10-29},
  eprint = {2410.21276},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.21276},
  url = {http://arxiv.org/abs/2410.21276},
  urldate = {2025-11-13},
  abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\textbackslash\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\Aymen\Zotero\storage\HUTWSRUS\OpenAI et al. - 2024 - GPT-4o System Card.pdf}
}

@online{OptimalTransportWeak2025,
  title = {Optimal {{Transport}} - {{Weak Solutions}} of {{Monge-Ampere}} - {{YouTube}}},
  date = {2025-11-12},
  url = {https://www.youtube.com/watch?v=mmpyxStCoqU&list=PLJ6garKOlK2qKVhRm6UwvcQ46wK-ciHbl&index=13},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\H2NTUWSD\watch.html}
}

@online{ordonez-apraezDynamicsHarmonicAnalysis2023,
  title = {Dynamics {{Harmonic Analysis}} of {{Robotic Systems}}: {{Application}} in {{Data-Driven Koopman Modelling}}},
  shorttitle = {Dynamics {{Harmonic Analysis}} of {{Robotic Systems}}},
  author = {Ordoñez-Apraez, Daniel and Kostic, Vladimir and Turrisi, Giulio and Novelli, Pietro and Mastalli, Carlos and Semini, Claudio and Pontil, Massimiliano},
  date = {2023},
  doi = {10.48550/ARXIV.2312.07457},
  url = {https://arxiv.org/abs/2312.07457},
  urldate = {2024-11-05},
  abstract = {We introduce the use of harmonic analysis to decompose the state space of symmetric robotic systems into orthogonal isotypic subspaces. These are lower-dimensional spaces that capture distinct, symmetric, and synergistic motions. For linear dynamics, we characterize how this decomposition leads to a subdivision of the dynamics into independent linear systems on each subspace, a property we term dynamics harmonic analysis (DHA). To exploit this property, we use Koopman operator theory to propose an equivariant deep-learning architecture that leverages the properties of DHA to learn a global linear model of the system dynamics. Our architecture, validated on synthetic systems and the dynamics of locomotion of a quadrupedal robot, exhibits enhanced generalization, sample efficiency, and interpretability, with fewer trainable parameters and computational costs.},
  pubstate = {prepublished},
  version = {3},
  keywords = {43-08,Artificial Intelligence (cs.AI),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Machine Learning (cs.LG),Robotics (cs.RO),Systems and Control (eess.SY)}
}

@online{ordonez-apraezMorphologicalSymmetriesRobotics2024,
  title = {Morphological {{Symmetries}} in {{Robotics}}},
  author = {Ordoñez-Apraez, Daniel and Turrisi, Giulio and Kostic, Vladimir and Martin, Mario and Agudo, Antonio and Moreno-Noguer, Francesc and Pontil, Massimiliano and Semini, Claudio and Mastalli, Carlos},
  date = {2024},
  doi = {10.48550/ARXIV.2402.15552},
  url = {https://arxiv.org/abs/2402.15552},
  urldate = {2024-11-05},
  abstract = {We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot's morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot's state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot's equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models through data augmentation, or by applying equivariant/invariant constraints on the model's architecture. In the context of analytical methods, we employ abstract harmonic analysis to decompose the robot's dynamics into a superposition of lower-dimensional, independent dynamics. We substantiate our claims with both synthetic and real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we introduce the repository MorphoSymm to facilitate the practical use of the theory and applications outlined in this work.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Robotics (cs.RO),Systems and Control (eess.SY)},
  note = {\subsection{Other}

18 pages, 11 figures}
}

@article{oudotStabilityMultigradedBetti2024,
  title = {On the Stability of Multigraded {{Betti}} Numbers and {{Hilbert}} Functions},
  author = {Oudot, Steve and Scoccola, Luis},
  date = {2024-03-31},
  journaltitle = {SIAM J. Appl. Algebra Geometry},
  volume = {8},
  number = {1},
  eprint = {2112.11901},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {54--88},
  issn = {2470-6566},
  doi = {10.1137/22M1489150},
  url = {http://arxiv.org/abs/2112.11901},
  urldate = {2025-11-12},
  abstract = {Multigraded Betti numbers are one of the simplest invariants of multiparameter persistence modules. This invariant is useful in theory—it completely determines the Hilbert function of the module and the isomorphism type of the free modules in its minimal free resolution—as well as in practice—it is easy to visualize and it is one of the main outputs of current multiparameter persistent homology software, such as RIVET. However, to the best of our knowledge, no stability result with respect to the interleaving distance has been established for this invariant so far, and this potential lack of stability limits its practical applications. We prove a stability result for multigraded Betti numbers, using an efficiently computable bottleneck-type dissimilarity function we introduce. Our notion of matching is inspired by recent work on signed barcodes, and allows matching bars of the same module in homological degrees of different parity, in addition to matchings bars of different modules in homological degrees of the same parity. Our stability result is a combination of Hilbert’s syzygy theorem, Bjerkevik’s bottleneck stability for free modules, and a novel stability result for projective resolutions. We also prove, in the two-parameter case, a 1-Wasserstein stability result for Hilbert functions with respect to the 1-presentation distance of Bjerkevik and Lesnick.},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics - Representation Theory},
  note = {Comment: 24 pages, 4 figures; v2: adds section on efficient computability of lower bounds, section on consequences of main results, no-go result (Prop. 4), generalization of Thm. 1 (Thm. 26), and improves exposition; v3: adds several details and improves exposition; v4: minor clarifications and use numbering as in published version},
  file = {C:\Users\Aymen\Zotero\storage\7E93CGDA\Oudot et Scoccola - 2024 - On the stability of multigraded Betti numbers and Hilbert functions.pdf}
}

@inproceedings{OverfittingMeasurementDeep,
  title = {Overfitting {{Measurement}} of {{Deep Neural Networks Using No Data}} | {{Request PDF}}},
  booktitle = {{{ResearchGate}}},
  doi = {10.1109/DSAA53316.2021.9564119},
  url = {https://www.researchgate.net/publication/363235732_Overfitting_Measurement_of_Deep_Neural_Networks_Using_No_Data},
  urldate = {2025-11-16},
  abstract = {Request PDF | On Oct 6, 2021, Satoru Watanabe and others published Overfitting Measurement of Deep Neural Networks Using No Data | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\XQW9IYPW\363235732_Overfitting_Measurement_of_Deep_Neural_Networks_Using_No_Data.html}
}

@online{PADMEPlatformAnalytics2025,
  title = {{{PADME}} | {{Platform}} for {{Analytics}} and {{Distributed Machine Learning}} for {{Enterprises}}},
  date = {2025-11-13},
  url = {https://padme-analytics.de/},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\ELUIBQ87\padme-analytics.de.html}
}

@online{PageNonTrouvee2025,
  title = {Page non trouvée},
  date = {2025-11-13},
  url = {http://www.mathprepa.fr/travaux-diriges-mathematiques-mpsi-pcsi/},
  urldate = {2025-11-13},
  langid = {french},
  organization = {Mathprepa},
  file = {C:\Users\Aymen\Zotero\storage\2UQJ8LRL\travaux-diriges-mathematiques-mpsi-pcsi.html}
}

@article{papamarkouPositionTopologicalDeep2024,
  title = {Position: {{Topological Deep Learning}} Is the {{New Frontier}} for {{Relational Learning}}},
  author = {Papamarkou, Theodore and Birdal, Tolga and Bronstein, Michael and Carlsson, Gunnar and Curry, Justin and Gao, Yue and Hajij, Mustafa and Kwitt, Roland and Liò, Pietro and Di Lorenzo, Paolo and Maroulas, Vasileios and Miolane, Nina and Nasrin, Farzana and Natesan Ramamurthy, Karthikeyan and Rieck, Bastian and Scardapane, Simone and Schaub, Michael T. and Veličković, Petar and Wang, Bei and Wang, Yusu and Wei, Guo-Wei and Zamzmi, Ghada},
  date = {2024-02-01},
  pages = {arXiv:2402.08871},
  doi = {10.48550/arXiv.2402.08871},
  abstract = {Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL is the new frontier for relational learning. TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024}
}

@online{papillonArchitecturesTopologicalDeep2024,
  title = {Architectures of {{Topological Deep Learning}}: {{A Survey}} of {{Message-Passing Topological Neural Networks}}},
  shorttitle = {Architectures of {{Topological Deep Learning}}},
  author = {Papillon, Mathilde and Sanborn, Sophia and Hajij, Mustafa and Miolane, Nina},
  date = {2024-02-21},
  eprint = {2304.10031},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.10031},
  url = {http://arxiv.org/abs/2304.10031},
  urldate = {2024-09-20},
  abstract = {The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature for relational systems has also led to a lack of unification in notation and language across message-passing Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying message-passing TNNs to new real-world problems. To address this issue, we provide an accessible introduction to TDL for relational systems, and compare the recently published message-passing TNNs using a unified mathematical and graphical notation. Through an intuitive and critical review of the emerging field of TDL, we extract valuable insights into current challenges and exciting opportunities for future development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Graphs,Simplices,TDA}
}

@online{PDFHiddenMarkov,
  title = {({{PDF}}) {{Hidden Markov Model Regression}}},
  url = {https://www.researchgate.net/publication/2791408_Hidden_Markov_Model_Regression},
  urldate = {2025-11-16},
  abstract = {PDF | Hidden Markov Model Regression (HMMR) is an extension of the Hidden Markov Model (HMM) to regression analysis. We assume that the parameters of... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  organization = {ResearchGate},
  file = {C:\Users\Aymen\Zotero\storage\QPGZSAVA\2791408_Hidden_Markov_Model_Regression.html}
}

@inproceedings{penningtonEmergenceSpectralUniversality2018,
  title = {The Emergence of Spectral Universality in Deep Networks},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  date = {2018},
  pages = {1924--1932},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v84/pennington18a.html},
  urldate = {2025-11-13},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\CUCBPP3G\\Pennington et al. - 2018 - The emergence of spectral universality in deep networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\RANYVCMJ\\Pennington et al. - 2018 - The emergence of spectral universality in deep networks.pdf}
}

@inproceedings{penningtonGeometryNeuralNetwork2017,
  title = {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
  booktitle = {International Conference on Machine Learning},
  author = {Pennington, Jeffrey and Bahri, Yasaman},
  date = {2017},
  pages = {2798--2806},
  publisher = {PMLR},
  url = {http://proceedings.mlr.press/v70/pennington17a.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\9B3FSILI\Pennington et Bahri - 2017 - Geometry of neural network loss surfaces via random matrix theory.pdf}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global}} Vectors for Word Representation},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  date = {2014},
  pages = {1532--1543},
  url = {https://aclanthology.org/D14-1162.pdf},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\8RL2LK8S\Pennington et al. - 2014 - Glove Global vectors for word representation.pdf}
}

@article{penningtonNonlinearRandomMatrix2017,
  title = {Nonlinear Random Matrix Theory for Deep Learning},
  author = {Pennington, Jeffrey and Worah, Pratik},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  url = {https://proceedings.neurips.cc/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\U4XRRJ58\Pennington et Worah - 2017 - Nonlinear random matrix theory for deep learning.pdf}
}

@inproceedings{penningtonProceedings2014Conference2014,
  title = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics Stroudsburg, PA},
  url = {https://scholar.google.com/scholar?cluster=2008811717314355285&hl=en&oi=scholarr},
  urldate = {2025-11-13}
}

@article{penningtonResurrectingSigmoidDeep2017,
  title = {Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry: Theory and Practice},
  shorttitle = {Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry},
  author = {Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  url = {https://proceedings.neurips.cc/paper/2017/hash/d9fc0cdb67638d50f411432d0d41d0ba-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\WFJPSUJW\Pennington et al. - 2017 - Resurrecting the sigmoid in deep learning through dynamical isometry theory and practice.pdf}
}

@online{PeopleAvalanche2025,
  title = {The {{People}} | {{Avalanche}}},
  date = {2025-11-13},
  url = {https://avalanche.continualai.org/about-us/the-team},
  urldate = {2025-11-13},
  abstract = {All the People that Made Avalanche Great},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\KGK4YSTL\the-team.html}
}

@article{petriTopologicalExpressivePower,
  title = {On {{The Topological Expressive Power}} of {{Neural Networks}}},
  author = {Petri, Giovanni and Leitão, António},
  abstract = {We propose a topological description of neural network expressive power. We adopt the topology of the space of decision boundaries realized by a neural architecture as a measure of its intrinsic expressive power. By sampling a large number of neural architectures with different sizes and design, we show how such measure of expressive power depends on the properties of the architectures, like depth, width and other related quantities.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\RTQGKSLE\Petri et Leitão - On The Topological Expressive Power of Neural Networks.pdf}
}

@article{peyreCourseNotesComputational,
  title = {Course Notes on {{Computational Optimal Transport}}},
  author = {Peyre, Gabriel},
  abstract = {These course notes present the foundational mathematics concepts relevant to computational optimal transport. They delve into various formulations and techniques, starting with Monge’s formulation, illustrated through applications to one-dimensional optimal transport and distributions involving Gaussians. The notes then explore Kantorovich’s formulation, a more general approach that addresses limitations in Monge’s method and applies to a wider range of problems. Additionally, the course covers the concept of the Wasserstein distance, a key metric in optimal transport theory that quantifies the cost of transporting one distribution to another. The notes also introduce entropic regularization, an important technique used to smooth optimization problems and make them more tractable computationally. This approach facilitates the numerical solution of optimal transport problems by adding an entropy term to the objective function, leading to faster and more stable algorithms. Finally, the dual formulation of optimal transport problems is presented. This perspective allows for deriving alternative algorithms and insights into the structure of optimal transport solutions. By exploring the dual problem, students can understand the relationships between different formulations and how they can be exploited to solve transport problems more efficiently. Overall, these course notes provide a comprehensive introduction to the mathematics of computational optimal transport, offering insights into both theoretical foundations and practical applications.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\89SE76IR\Peyre - Course notes on Computational Optimal Transport.pdf}
}

@online{PHomGeMPersistentHomology,
  title = {{{PHom-GeM}}: {{Persistent}} Homology for Generative Mod- Els ({{Charlier}} et al., 2019) - {{Google Search}}},
  url = {https://www.google.com/search?client=firefox-b-d&q=PHom-GeM%3A+Persistent+homology+for+generative+mod-+els+%28Charlier+et+al.%2C+2019%29},
  urldate = {2025-11-16},
  file = {C:\Users\Aymen\Zotero\storage\REUDBFVS\search.html}
}

@software{poggioAaronestradaIntAgACOfeature2023,
  title = {Aaronestrada/{{IntAgACOfeature}}},
  author = {Poggio, Aaron Estrada},
  date = {2023-11-14T04:08:35Z},
  origdate = {2017-01-09T21:49:47Z},
  url = {https://github.com/aaronestrada/IntAgACOfeature},
  urldate = {2024-09-20},
  abstract = {Unsupervised Feature Selection using Ant Colony Optimization - Prototype project for Intelligent Agents course},
  keywords = {UFSACO},
  annotation = {Programmers: \_:n869\\
Programmers: \_:n916}
}

@online{PotentialLossFunctions2024,
  title = {Potential Loss Functions. {{LWR}} on {{CNN}} or {{Transformer-OT-KL-}} Modified Cook's Distance - Replace Amputated Values},
  date = {2024-09-20},
  url = {https://chatgpt.com/share/ea42501f-b198-483e-b49e-073df4307184},
  urldate = {2024-09-20}
}

@online{powerGrokkingGeneralizationOverfitting2022,
  title = {Grokking: {{Generalization Beyond Overfitting}} on {{Small Algorithmic Datasets}}},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  date = {2022-01-06},
  url = {https://arxiv.org/abs/2201.02177v1},
  urldate = {2024-10-03},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {Grokking}
}

@inreference{ProjectionLinearAlgebra2025,
  title = {Projection (Linear Algebra)},
  booktitle = {Wikipedia},
  date = {2025-08-11T14:48:41Z},
  url = {https://en.wikipedia.org/w/index.php?title=Projection_(linear_algebra)&oldid=1305347791},
  urldate = {2025-11-12},
  abstract = {In linear algebra and functional analysis, a projection is a linear transformation                         P                 \{\textbackslash displaystyle P\}     from a vector space to itself (an endomorphism) such that                         P         ∘         P         =         P                 \{\textbackslash displaystyle P\textbackslash circ P=P\}    . That is, whenever                         P                 \{\textbackslash displaystyle P\}     is applied twice to any vector, it gives the same result as if it were applied once (i.e.                         P                 \{\textbackslash displaystyle P\}     is idempotent). It leaves its image unchanged. This definition of "projection" formalizes and generalizes the idea of graphical projection.  One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object.},
  langid = {english},
  annotation = {Page Version ID: 1305347791},
  file = {C:\Users\Aymen\Zotero\storage\3GYETEJP\index.html}
}

@online{purvineExperimentalObservationsTopology2022,
  title = {Experimental {{Observations}} of the {{Topology}} of {{Convolutional Neural Network Activations}}},
  author = {Purvine, Emilie and Brown, Davis and Jefferson, Brett and Joslyn, Cliff and Praggastis, Brenda and Rathore, Archit and Shapiro, Madelyn and Wang, Bei and Zhou, Youjia},
  date = {2022-12-01},
  eprint = {2212.00222},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.00222},
  url = {http://arxiv.org/abs/2212.00222},
  urldate = {2025-11-16},
  abstract = {Topological data analysis (TDA) is a branch of computational mathematics, bridging algebraic topology and data science, that provides compact, noise-robust representations of complex structures. Deep neural networks (DNNs) learn millions of parameters associated with a series of transformations defined by the model architecture, resulting in high-dimensional, difficult-to-interpret internal representations of input data. As DNNs become more ubiquitous across multiple sectors of our society, there is increasing recognition that mathematical methods are needed to aid analysts, researchers, and practitioners in understanding and interpreting how these models' internal representations relate to the final classification. In this paper, we apply cutting edge techniques from TDA with the goal of gaining insight into the interpretability of convolutional neural networks used for image classification. We use two common TDA approaches to explore several methods for modeling hidden-layer activations as high-dimensional point clouds, and provide experimental evidence that these point clouds capture valuable structural information about the model's process. First, we demonstrate that a distance metric based on persistent homology can be used to quantify meaningful differences between layers, and we discuss these distances in the broader context of existing representational similarity metrics for neural network interpretability. Second, we show that a mapper graph can provide semantic insight into how these models organize hierarchical class knowledge at each layer. These observations demonstrate that TDA is a useful tool to help deep learning practitioners unlock the hidden structures of their models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning},
  note = {Comment: Accepted at AAAI 2023. This version includes supplementary material},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\B4PIVWDE\\Purvine et al. - 2022 - Experimental Observations of the Topology of Convolutional Neural Network Activations.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\TZL9ZEU4\\2212.html}
}

@online{PyTorchAPIGeomLoss2025,
  title = {{{PyTorch API}} — {{GeomLoss}}},
  date = {2025-11-12},
  url = {https://www.kernel-operations.io/geomloss/api/pytorch-api.html},
  urldate = {2025-11-12},
  file = {C:\Users\Aymen\Zotero\storage\I5L77LL2\pytorch-api.html}
}

@online{PytorchsymeigGoogleSearch2025,
  title = {Pytorch.Symeig - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?q=pytorch.symeig&client=ms-android-samsung-ss&sca_esv=0f2da4589a5603d0&sxsrf=AE3TifO6c-xYPQh3k5Hy96FCL8YFLnvL6A%3A1756888338329&ei=Ev23aIzOE66IkdUP5OC_mAY&oq=pytorch.symeig&gs_lp=EhNtb2JpbGUtZ3dzLXdpei1zZXJwIg5weXRvcmNoLnN5bWVpZzIGEAAYDRgeMgYQABgNGB4yBhAAGA0YHjIGEAAYDRgeMgQQABgeMgYQABgNGB4yBhAAGA0YHjIIEAAYChgNGB5I-I8BUKYkWJqLAXACeAGQAQCYAbwCoAHVF6oBCDAuMTIuMy4xuAEDyAEA-AEBmAIRoAK1GKgCHsICBxAjGCcY6gLCAgoQIxjwBRgnGOoCwgIEECMYJ8ICChAjGIAEGCcYigXCAgsQABiABBiRAhiKBcICCxAuGIAEGNEDGMcBwgIFEC4YgATCAgUQABiABMICBxAAGIAEGArCAgoQLhiABBjUAhgKwgIHEC4YgAQYCsICBxAAGIAEGA3CAgcQIxiwAhgnwgIEEB4YCpgDPvEF0SOlnFTvHeeSBwcyLjkuNS4xoAf2XrIHBzAuOS41LjG4B9kXwgcKMi0zLjExLjIuMcgHgAI&sclient=mobile-gws-wiz-serp},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\Z8XIMCID\search.html}
}

@online{qiBetterGenerativeReplay2023,
  title = {Better {{Generative Replay}} for {{Continual Federated Learning}}},
  author = {Qi, Daiqing and Zhao, Handong and Li, Sheng},
  date = {2023-02-25},
  eprint = {2302.13001},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.13001},
  urldate = {2024-09-20},
  abstract = {Federated Learning (FL) aims to develop a centralized server that learns from distributed clients via communications without accessing the clients’ local data. However, existing works mainly focus on federated learning in a single task scenario. with static data. In this paper, we introduce the continual federated learning (CFL) problem, where clients incrementally learn new tasks and history data cannot be stored due to certain reasons, such as limited storage and data retention policy 1. Generative replay (GR) based methods are effective for continual learning without storing history data. However, we fail when trying to intuitively adapt GR models for this setting. By analyzing the behaviors of clients during training, we find the unstable training process caused by distributed training on non-IID data leads to a notable performance degradation. To address this problem, we propose our FedCIL model with two simple but effective solutions: 1. model consolidation and 2. consistency enforcement. Experimental results on multiple benchmark datasets demonstrate that our method significantly outperforms baselines.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {CL,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,GAN}
}

@online{qinCosFormerRethinkingSoftmax2022,
  title = {{{cosFormer}}: {{Rethinking Softmax}} in {{Attention}}},
  shorttitle = {{{cosFormer}}},
  author = {Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
  date = {2022-02-17},
  eprint = {2202.08791},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.08791},
  url = {http://arxiv.org/abs/2202.08791},
  urldate = {2025-11-16},
  abstract = {Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at https://github.com/OpenNLPLab/cosFormer.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted to ICLR2022. Yiran Zhong is the corresponding author. Zhen Qin, Weixuan Sun, Hui Deng contributed equally to this work},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\VXFCGMGV\\Qin et al. - 2022 - cosFormer Rethinking Softmax in Attention.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\6AEV2R6F\\2202.html}
}

@online{QuantitativePerformanceAssessment,
  title = {Quantitative Performance Assessment of {{CNN}} Units via Topological Entropy Calculation ({{Zhao}} and {{Zhang}}, 2022) - {{Google Search}}},
  url = {https://www.google.com/search?client=firefox-b-d&q=Quantitative+performance+assessment+of+CNN+units+via+topological+entropy+calculation+%28Zhao+and+Zhang%2C+2022%29},
  urldate = {2025-11-16},
  file = {C:\Users\Aymen\Zotero\storage\N3P7P22C\search.html}
}

@online{Qwen2025,
  title = {Qwen},
  date = {2025-11-13},
  url = {https://chat.qwen.ai/auth},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\UGWDQF27\auth.html}
}

@online{radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2024-09-20},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,GAN latent space manipulation,GAN transitions},
  note = {Comment: Under review as a conference paper at ICLR 2016
\par
Comment: Under review as a conference paper at ICLR 2016}
}

@online{ramamurthyTopologicalDataAnalysis2018,
  title = {Topological {{Data Analysis}} of {{Decision Boundaries}} with {{Application}} to {{Model Selection}}},
  author = {Ramamurthy, Karthikeyan Natesan and Varshney, Kush R. and Mody, Krishnan},
  date = {2018-05-25},
  eprint = {1805.09949},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1805.09949},
  url = {http://arxiv.org/abs/1805.09949},
  urldate = {2025-11-16},
  abstract = {We propose the labeled Čech complex, the plain labeled Vietoris-Rips complex, and the locally scaled labeled Vietoris-Rips complex to perform persistent homology inference of decision boundaries in classification tasks. We provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples. Our main objective is quantification of deep neural network complexity to enable matching of datasets to pre-trained models; we report results for experiments using MNIST, FashionMNIST, and CIFAR10.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Reproducible software available, 17 pages, 10 figures, 12 tables},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\4TNMVHNM\\Ramamurthy et al. - 2018 - Topological Data Analysis of Decision Boundaries with Application to Model Selection.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\NN6A2P9J\\1805.html}
}

@inreference{RandomMatrix2025,
  title = {Random Matrix},
  booktitle = {Wikipedia},
  date = {2025-10-08T00:14:37Z},
  url = {https://en.wikipedia.org/w/index.php?title=Random_matrix&oldid=1315669505},
  urldate = {2025-11-13},
  abstract = {In probability theory and mathematical physics, a random matrix is a matrix-valued random variable—that is, a matrix in which some or all of its entries are sampled randomly from a probability distribution. Random matrix theory (RMT) is the study of properties of random matrices, often as they become large. RMT provides techniques like mean-field theory, diagrammatic methods, the cavity method, or the replica method to compute quantities like traces, spectral densities, or scalar products between eigenvectors. Many physical phenomena, such as the spectrum of nuclei of heavy atoms, the thermal conductivity of a lattice, or the emergence of quantum chaos, can be modeled mathematically as problems concerning large, random matrices.},
  langid = {english},
  annotation = {Page Version ID: 1315669505},
  file = {C:\Users\Aymen\Zotero\storage\F7RB47KJ\index.html}
}

@online{rathoreTopoActVisuallyExploring2021,
  title = {{{TopoAct}}: {{Visually Exploring}} the {{Shape}} of {{Activations}} in {{Deep Learning}}},
  shorttitle = {{{TopoAct}}},
  author = {Rathore, Archit and Chalapathi, Nithin and Palande, Sourabh and Wang, Bei},
  date = {2021-04-12},
  eprint = {1912.06332},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.06332},
  urldate = {2024-09-20},
  abstract = {Deep neural networks such as GoogLeNet, ResNet, and BERT have achieved impressive performance in tasks such as image and text classification. To understand how such performance is achieved, we probe a trained deep neural network by studying neuron activations, i.e., combinations of neuron firings, at various layers of the network in response to a particular input. With a large number of inputs, we aim to obtain a global view of what neurons detect by studying their activations. In particular, we develop visualizations that show the shape of the activation space, the organizational principle behind neuron activations, and the relationships of these activations within a layer. Applying tools from topological data analysis, we present TopoAct, a visual exploration system to study topological summaries of activation vectors. We present exploration scenarios using TopoAct that provide valuable insights into learned representations of neural networks. We expect TopoAct to give a topological perspective that enriches the current toolbox of neural network analysis, and to provide a basis for network architecture diagnosis and data anomaly detection.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Graphics,Computer Science - Machine Learning}
}

@article{rathoreTopoBERTExploringTopology2023,
  title = {{{TopoBERT}}: {{Exploring}} the Topology of Fine-Tuned Word Representations},
  shorttitle = {{{TopoBERT}}},
  author = {Rathore, Archit and Zhou, Yichu and Srikumar, Vivek and Wang, Bei},
  date = {2023-07},
  journaltitle = {Information Visualization},
  volume = {22},
  number = {3},
  pages = {186--208},
  issn = {1473-8716, 1473-8724},
  doi = {10.1177/14738716231168671},
  url = {https://journals.sagepub.com/doi/10.1177/14738716231168671},
  urldate = {2025-11-16},
  abstract = {Transformer-based language models such as BERT and its variants have found widespread use in natural language processing (NLP). A common way of using these models is to fine-tune them to improve their performance on a specific task. However, it is currently unclear how the fine-tuning process affects the underlying structure of the word embeddings from these models. We present TopoBERT, a visual analytics system for interactively exploring the finetuning process of various transformer-based models — across multiple fine-tuning batch updates, subsequent layers of the model, and different NLP tasks — from a topological perspective. The system uses the mapper algorithm from topological data analysis (TDA) to generate a graph that approximates the shape of a model’s embedding space for an input dataset. TopoBERT enables its users (e.g., experts in NLP and linguistics) to (1) interactively explore the fine-tuning process across different model-task pairs, (2) visualize the shape of embedding spaces at multiple scales and layers, and (3) connect linguistic and contextual information about the input dataset with the topology of the embedding space. Using TopoBERT, we provide various use cases to exemplify its applications in exploring fine-tuned word embeddings. We further demonstrate the utility of TopoBERT, which enables users to generate insights about the fine-tuning process and provides support for empirical validation of these insights.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\9GBE32CS\Rathore et al. - 2023 - TopoBERT Exploring the topology of fine-tuned word representations.pdf}
}

@online{RepresentationTheory2024,
  title = {Representation theory},
  date = {2024-09-20},
  url = {http://www.youtube.com/playlist?list=PL8yHsr3EFj51AM_VmB0VygPIzcUCH_jUG},
  urldate = {2024-09-20},
  abstract = {This playlist is a collection of videos on complex representations of finite groups. It is part of an online graduate algebra course; for the earlier part of...},
  langid = {french},
  organization = {YouTube},
  keywords = {Representation Theory}
}

@inproceedings{rieckNeuralPersistenceComplexity2018,
  title = {Neural {{Persistence}}: {{A Complexity Measure}} for {{Deep Neural Networks Using Algebraic Topology}}},
  shorttitle = {Neural {{Persistence}}},
  author = {Rieck, Bastian and Togninalli, Matteo and Bock, Christian and Moor, Michael and Horn, Max and Gumbsch, Thomas and Borgwardt, Karsten},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=ByxkijC5FQ},
  urldate = {2025-11-16},
  abstract = {While many approaches to make neural networks more fathomable have been proposed, they are restricted to interrogating the network with input data. Measures for characterizing and monitoring structural properties, however, have not been developed. In this work, we propose neural persistence, a complexity measure for neural network architectures based on topological data analysis on weighted stratified graphs. To demonstrate the usefulness of our approach, we show that neural persistence reflects best practices developed in the deep learning community such as dropout and batch normalization. Moreover, we derive a neural persistence-based stopping criterion that shortens the training process while achieving comparable accuracies as early stopping based on validation loss.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\3HE5BH5E\Rieck et al. - 2018 - Neural Persistence A Complexity Measure for Deep Neural Networks Using Algebraic Topology.pdf}
}

@software{riekeJriekeAwesomemachinelearningstartupsberlin2025,
  title = {Jrieke/Awesome-Machine-Learning-Startups-Berlin},
  author = {Rieke, Johannes},
  date = {2025-10-29T16:54:21Z},
  origdate = {2020-07-18T03:36:52Z},
  url = {https://github.com/jrieke/awesome-machine-learning-startups-berlin},
  urldate = {2025-11-13},
  abstract = {🤖 A curated list of machine learning \& artificial intelligence startups in Berlin (Germany)},
  keywords = {artificial-intelligence,berlin,berlin-startup-jobs,deep-learning,germany,machine-learning,startup,startups},
  annotation = {Programmers: \_:n1665}
}

@article{rifaiContractiveAutoEncodersExplicit,
  title = {Contractive {{Auto-Encoders}}: {{Explicit Invariance During Feature Extraction}}},
  author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized autoencoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
  langid = {english},
  keywords = {CAE,satiration Jacobian correlation}
}

@online{rosascoDistilledReplayOvercoming2021,
  title = {Distilled {{Replay}}: {{Overcoming Forgetting}} through {{Synthetic Samples}}},
  shorttitle = {Distilled {{Replay}}},
  author = {Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  date = {2021-06-22},
  eprint = {2103.15851},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.15851},
  urldate = {2024-09-16},
  abstract = {Replay strategies are Continual Learning techniques which mitigate catastrophic forgetting by keeping a buffer of patterns from previous experiences, which are interleaved with new data during training. The amount of patterns stored in the buffer is a critical parameter which largely influences the final performance and the memory footprint of the approach. This work introduces Distilled Replay, a novel replay strategy for Continual Learning which is able to mitigate forgetting by keeping a very small buffer (1 pattern per class) of highly informative samples. Distilled Replay builds the buffer through a distillation process which compresses a large dataset into a tiny set of informative examples. We show the effectiveness of our Distilled Replay against popular replay-based strategies on four Continual Learning benchmarks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {CL,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,DD,MNIST}
}

@online{sagunEigenvaluesHessianDeep2017,
  title = {Eigenvalues of the {{Hessian}} in {{Deep Learning}}: {{Singularity}} and {{Beyond}}},
  shorttitle = {Eigenvalues of the {{Hessian}} in {{Deep Learning}}},
  author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
  date = {2017-10-05},
  eprint = {1611.07476},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.07476},
  urldate = {2024-09-17},
  abstract = {We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges that depend on the input data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: ICLR submission, 2016 - updated to match the openreview.net version}
}

@inproceedings{sandlerMetaLearningBidirectionalUpdate2021,
  title = {Meta-{{Learning Bidirectional Update Rules}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Sandler, Mark and Vladymyrov, Max and Zhmoginov, Andrey and Miller, Nolan and Madams, Tom and Jackson, Andrew and Arcas, Blaise Agüera Y.},
  date = {2021-07-01},
  pages = {9288--9300},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/sandler21a.html},
  urldate = {2024-09-20},
  abstract = {In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional "genome". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@online{sandlerTrainingTrajectoriesMinibatch2023,
  title = {Training Trajectories, Mini-Batch Losses and the Curious Role of the Learning Rate},
  author = {Sandler, Mark and Zhmoginov, Andrey and Vladymyrov, Max and Miller, Nolan},
  date = {2023-02-01},
  eprint = {2301.02312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.02312},
  url = {http://arxiv.org/abs/2301.02312},
  urldate = {2024-09-20},
  abstract = {Stochastic gradient descent plays a fundamental role in nearly all applications of deep learning. However its ability to converge to a global minimum remains shrouded in mystery. In this paper we propose to study the behavior of the loss function on fixed mini-batches along SGD trajectories. We show that the loss function on a fixed batch appears to be remarkably convex-like. In particular for ResNet the loss for any fixed mini-batch can be accurately modeled by a quadratic function and a very low loss value can be reached in just one step of gradient descent with sufficiently large learning rate. We propose a simple model that allows to analyze the relationship between the gradients of stochastic mini-batches and the full batch. Our analysis allows us to discover the equivalency between iterate aggregates and specific learning rate schedules. In particular, for Exponential Moving Average (EMA) and Stochastic Weight Averaging we show that our proposed model matches the observed training trajectories on ImageNet. Our theoretical model predicts that an even simpler averaging technique, averaging just two points a many steps apart, significantly improves accuracy compared to the baseline. We validated our findings on ImageNet and other datasets using ResNet architecture.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: 21 pages, 14 figures}
}

@online{sannaiReconstructionTrainingSamples2018,
  title = {Reconstruction of Training Samples from Loss Functions},
  author = {Sannai, Akiyoshi},
  date = {2018-05-18},
  eprint = {1805.07337},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.07337},
  urldate = {2024-09-20},
  abstract = {This paper presents a new mathematical framework to analyze the loss functions of deep neural networks with ReLU functions. Furthermore, as as application of this theory, we prove that the loss functions can reconstruct the inputs of the training samples up to scalar multiplication (as vectors) and can provide the number of layers and nodes of the deep neural network. Namely, if we have all input and output of a loss function (or equivalently all possible learning process), for all input of each training sample xi ∈ Rn, we can obtain vectors x′i ∈ Rn satisfying xi = cix′i for some ci 6= 0. To prove theorem, we introduce the notion of virtual polynomials, which are polynomials written as the output of a node in a deep neural network. Using virtual polynomials, we find an algebraic structure for the loss surfaces, called semi-algebraic sets. We analyze these loss surfaces from the algebro-geometric point of view. Factorization of polynomials is one of the most standard ideas in algebra. Hence, we express the factorization of the virtual polynomials in terms of their active paths. This framework can be applied to the leakage problem in the training of deep neural networks. The main theorem in this paper indicates that there are many risks associated with the training of deep neural networks. For example, if we have N (the dimension of weight space) + 1 nonsmooth points on the loss surface, which are sufficiently close to each other, we can obtain the input of training sample up to scalar multiplication. We also point out that the structures of the loss surfaces depend on the shape of the deep neural network and not on the training samples.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Reconstructions,Statistics - Machine Learning},
  note = {Comment: 11 pages, 3 figures
\par
Comment: 11 pages, 3 figures}
}

@online{schoenholzDeepInformationPropagation2017,
  title = {Deep {{Information Propagation}}},
  author = {Schoenholz, Samuel S. and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
  date = {2017-04-06},
  eprint = {1611.01232},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1611.01232},
  url = {http://arxiv.org/abs/1611.01232},
  urldate = {2025-11-13},
  abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\XSZDJPNP\Schoenholz et al. - 2017 - Deep Information Propagation.pdf}
}

@article{schoenholzJaxMdFramework2020,
  title = {Jax Md: A Framework for Differentiable Physics},
  shorttitle = {Jax Md},
  author = {Schoenholz, Samuel and Cubuk, Ekin Dogus},
  date = {2020},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {11428--11441},
  url = {https://proceedings.neurips.cc/paper/2020/hash/83d3d4b6c9579515e1679aca8cbc8033-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\5BHMKDC9\Schoenholz et Cubuk - 2020 - Jax md a framework for differentiable physics.pdf}
}

@article{schoenholzStructuralApproachRelaxation2016,
  title = {A Structural Approach to Relaxation in Glassy Liquids},
  author = {Schoenholz, Samuel S. and Cubuk, Ekin D. and Sussman, Daniel M. and Kaxiras, Efthimios and Liu, Andrea J.},
  date = {2016},
  journaltitle = {Nature Physics},
  volume = {12},
  number = {5},
  pages = {469--471},
  publisher = {Nature Publishing Group UK London},
  url = {https://www.nature.com/articles/nphys3644},
  urldate = {2025-11-13}
}

@online{schurholtHyperRepresentationsGenerativeModels2022,
  title = {Hyper-{{Representations}} as {{Generative Models}}: {{Sampling Unseen Neural Network Weights}}},
  shorttitle = {Hyper-{{Representations}} as {{Generative Models}}},
  author = {Schürholt, Konstantin and Knyazev, Boris and Giró-i-Nieto, Xavier and Borth, Damian},
  date = {2022-09-29},
  url = {https://arxiv.org/abs/2209.14733v1},
  urldate = {2024-10-03},
  abstract = {Learning representations of neural network weights given a model zoo is an emerging and challenging area with many potential applications from model inspection, to neural architecture search or knowledge distillation. Recently, an autoencoder trained on a model zoo was able to learn a hyper-representation, which captures intrinsic and extrinsic properties of the models in the zoo. In this work, we extend hyper-representations for generative use to sample new model weights. We propose layer-wise loss normalization which we demonstrate is key to generate high-performing models and several sampling methods based on the topology of hyper-representations. The models generated using our methods are diverse, performant and capable to outperform strong baselines as evaluated on several downstream tasks: initialization, ensemble sampling and transfer learning. Our results indicate the potential of knowledge aggregation from model zoos to new models via hyper-representations thereby paving the avenue for novel research directions.},
  langid = {english},
  organization = {arXiv.org},
  keywords = {SOTA,weight regression}
}

@online{schurholtHyperRepresentationsLearningPopulations2024,
  title = {Hyper-{{Representations}}: {{Learning}} from {{Populations}} of {{Neural Networks}}},
  shorttitle = {Hyper-{{Representations}}},
  author = {Schürholt, Konstantin},
  date = {2024-10-07},
  eprint = {2410.05107},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.05107},
  url = {http://arxiv.org/abs/2410.05107},
  urldate = {2025-11-16},
  abstract = {This thesis addresses the challenge of understanding Neural Networks through the lens of their most fundamental component: the weights, which encapsulate the learned information and determine the model behavior. At the core of this thesis is a fundamental question: Can we learn general, task-agnostic representations from populations of Neural Network models? The key contribution of this thesis to answer that question are hyper-representations, a self-supervised method to learn representations of NN weights. Work in this thesis finds that trained NN models indeed occupy meaningful structures in the weight space, that can be learned and used. Through extensive experiments, this thesis demonstrates that hyper-representations uncover model properties, such as their performance, state of training, or hyperparameters. Moreover, the identification of regions with specific properties in hyper-representation space allows to sample and generate model weights with targeted properties. This thesis demonstrates applications for fine-tuning, and transfer learning to great success. Lastly, it presents methods that allow hyper-representations to generalize beyond model sizes, architectures, and tasks. The practical implications of that are profound, as it opens the door to foundation models of Neural Networks, which aggregate and instantiate their knowledge across models and architectures. Ultimately, this thesis contributes to the deeper understanding of Neural Networks by investigating structures in their weights which leads to more interpretable, efficient, and adaptable models. By laying the groundwork for representation learning of NN weights, this research demonstrates the potential to change the way Neural Networks are developed, analyzed, and used.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: PhD Dissertation accepted at University of St. Gallen},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\A3VWEZDH\\Schürholt - 2024 - Hyper-Representations Learning from Populations of Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\LEEE4VZ8\\2410.html}
}

@online{schurholtHyperRepresentationsPreTrainingTransfer2022,
  title = {Hyper-{{Representations}} for {{Pre-Training}} and {{Transfer Learning}}},
  author = {Schürholt, Konstantin and Knyazev, Boris and Giró-i-Nieto, Xavier and Borth, Damian},
  date = {2022-07-22},
  eprint = {2207.10951},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.10951},
  url = {http://arxiv.org/abs/2207.10951},
  urldate = {2024-09-20},
  abstract = {Learning representations of neural network weights given a model zoo is an emerging and challenging area with many potential applications from model inspection, to neural architecture search or knowledge distillation. Recently, an autoencoder trained on a model zoo was able to learn a hyper-representation, which captures intrinsic and extrinsic properties of the models in the zoo. In this work, we extend hyper-representations for generative use to sample new model weights as pre-training. We propose layer-wise loss normalization which we demonstrate is key to generate high-performing models and a sampling method based on the empirical density of hyper-representations. The models generated using our methods are diverse, performant and capable to outperform conventional baselines for transfer learning. Our results indicate the potential of knowledge aggregation from model zoos to new models via hyper-representations thereby paving the avenue for novel research directions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Hyper-representations}
}

@inproceedings{schurholtSelfSupervisedRepresentationLearning2021,
  title = {Self-{{Supervised Representation Learning}} on {{Neural Network Weights}} for {{Model Characteristic Prediction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schürholt, Konstantin and Kostadinov, Dimche and Borth, Damian},
  date = {2021},
  volume = {34},
  pages = {16481--16493},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/89562dccfeb1d0394b9ae7e09544dc70-Abstract.html},
  urldate = {2024-09-20},
  abstract = {Self-Supervised Learning (SSL) has been shown to learn useful and information-preserving representations. Neural Networks (NNs) are widely applied, yet their weight space is still not fully understood. Therefore, we propose to use SSL to learn hyper-representations of the weights of populations of NNs. To that end, we introduce domain specific data augmentations and an adapted attention architecture.  Our empirical evaluation demonstrates that self-supervised representation learning in this domain is able to recover diverse NN model characteristics. Further, we show that the proposed learned representations outperform prior work for predicting hyper-parameters, test accuracy, and generalization gap as well as transfer to out-of-distribution settings.},
  keywords = {Weight generation}
}

@online{scoccolaDifferentiabilityOptimizationMultiparameter2024,
  title = {Differentiability and {{Optimization}} of {{Multiparameter Persistent Homology}}},
  author = {Scoccola, Luis and Setlur, Siddharth and Loiseaux, David and Carrière, Mathieu and Oudot, Steve},
  date = {2024-08-30},
  eprint = {2406.07224},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.07224},
  url = {http://arxiv.org/abs/2406.07224},
  urldate = {2025-11-16},
  abstract = {Real-valued functions on geometric data -- such as node attributes on a graph -- can be optimized using descriptors from persistent homology, allowing the user to incorporate topological terms in the loss function. When optimizing a single real-valued function (the one-parameter setting), there is a canonical choice of descriptor for persistent homology: the barcode. The operation mapping a real-valued function to its barcode is differentiable almost everywhere, and the convergence of gradient descent for losses using barcodes is relatively well understood. When optimizing a vector-valued function (the multiparameter setting), there is no unique choice of descriptor for multiparameter persistent homology, and many distinct descriptors have been proposed. This calls for the development of a general framework for differentiability and optimization that applies to a wide range of multiparameter homological descriptors. In this article, we develop such a framework and show that it encompasses well-known descriptors of different flavors, such as signed barcodes and the multiparameter persistence landscape. We complement the theory with numerical experiments supporting the idea that optimizing multiparameter homological descriptors can lead to improved performances compared to optimizing one-parameter descriptors, even when using the simplest and most efficiently computable multiparameter descriptors.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics - Optimization and Control},
  note = {Comment: 13 pages + 13 page appendix, 8 figures. v2: update references and fix typos},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\TVMHPSL5\\Scoccola et al. - 2024 - Differentiability and Optimization of Multiparameter Persistent Homology.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\BWGBUGGI\\2406.html}
}

@online{scoccolaDifferentiabilityOptimizationMultiparameter2024a,
  title = {Differentiability and {{Optimization}} of {{Multiparameter Persistent Homology}}},
  author = {Scoccola, Luis and Setlur, Siddharth and Loiseaux, David and Carrière, Mathieu and Oudot, Steve},
  date = {2024-08-30},
  eprint = {2406.07224},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.07224},
  url = {http://arxiv.org/abs/2406.07224},
  urldate = {2025-11-16},
  abstract = {Real-valued functions on geometric data -- such as node attributes on a graph -- can be optimized using descriptors from persistent homology, allowing the user to incorporate topological terms in the loss function. When optimizing a single real-valued function (the one-parameter setting), there is a canonical choice of descriptor for persistent homology: the barcode. The operation mapping a real-valued function to its barcode is differentiable almost everywhere, and the convergence of gradient descent for losses using barcodes is relatively well understood. When optimizing a vector-valued function (the multiparameter setting), there is no unique choice of descriptor for multiparameter persistent homology, and many distinct descriptors have been proposed. This calls for the development of a general framework for differentiability and optimization that applies to a wide range of multiparameter homological descriptors. In this article, we develop such a framework and show that it encompasses well-known descriptors of different flavors, such as signed barcodes and the multiparameter persistence landscape. We complement the theory with numerical experiments supporting the idea that optimizing multiparameter homological descriptors can lead to improved performances compared to optimizing one-parameter descriptors, even when using the simplest and most efficiently computable multiparameter descriptors.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics - Optimization and Control},
  note = {Comment: 13 pages + 13 page appendix, 8 figures. v2: update references and fix typos},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\88D7RM3G\\Scoccola et al. - 2024 - Differentiability and Optimization of Multiparameter Persistent Homology.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\ZI59VEIG\\2406.html}
}

@online{SelfSupervisedLearningSeries2024,
  title = {Self-Supervised Learning Series},
  date = {2024-09-20},
  url = {http://www.youtube.com/playlist?list=PLderfcX9H9MoBP5iPKQ4btxfgzHjv7UJe},
  urldate = {2024-09-20},
  abstract = {Partagez vos vidéos avec vos amis, vos proches et le monde entier},
  langid = {french},
  organization = {YouTube},
  keywords = {Self-supervised Learning}
}

@inreference{Semicontinuity2025,
  title = {Semi-Continuity},
  booktitle = {Wikipedia},
  date = {2025-09-17T11:51:22Z},
  url = {https://en.wikipedia.org/w/index.php?title=Semi-continuity&oldid=1311877870#Lower_semicontinuity},
  urldate = {2025-11-12},
  abstract = {In mathematical analysis, semicontinuity (or semi-continuity) is a property of extended real-valued functions that is weaker than continuity. An extended real-valued function                         f                 \{\textbackslash displaystyle f\}     is upper (respectively, lower) semicontinuous at a point                                    x                        0                                     \{\textbackslash displaystyle x\_\{0\}\}     if, roughly speaking, the function values for arguments near                                    x                        0                                     \{\textbackslash displaystyle x\_\{0\}\}     are not much higher (respectively, lower) than                         f                    (                        x                            0                                   )                  .                 \{\textbackslash displaystyle f\textbackslash left(x\_\{0\}\textbackslash right).\}     Briefly, a function on a domain                         X                 \{\textbackslash displaystyle X\}     is lower semi-continuous if its epigraph                         \{         (         x         ,         t         )         ∈         X         ×                    R                  :         t         ≥         f         (         x         )         \}                 \{\textbackslash displaystyle \textbackslash\{(x,t)\textbackslash in X\textbackslash times \textbackslash mathbb \{R\} :t\textbackslash geq f(x)\textbackslash\}\}     is closed in                         X         ×                    R                          \{\textbackslash displaystyle X\textbackslash times \textbackslash mathbb \{R\} \}    , and upper semi-continuous if                         −         f                 \{\textbackslash displaystyle -f\}     is lower semi-continuous. A function is continuous if and only if it is both upper  and lower semicontinuous. If we take a continuous function and increase its value at a certain point                                    x                        0                                     \{\textbackslash displaystyle x\_\{0\}\}     to                         f                    (                        x                            0                                   )                  +         c                 \{\textbackslash displaystyle f\textbackslash left(x\_\{0\}\textbackslash right)+c\}     for some                         c         {$>$}         0                 \{\textbackslash displaystyle c{$>$}0\}    , then the result is upper semicontinuous; if we decrease its value to                         f                    (                        x                            0                                   )                  −         c                 \{\textbackslash displaystyle f\textbackslash left(x\_\{0\}\textbackslash right)-c\}     then the result is lower semicontinuous. The notion of upper and lower semicontinuous function was first introduced and studied by René Baire in his thesis in 1899.},
  langid = {english},
  annotation = {Page Version ID: 1311877870},
  file = {C:\Users\Aymen\Zotero\storage\DL2THP2J\index.html}
}

@inproceedings{senderaHyperShotFewShotLearning2023,
  title = {{{HyperShot}}: {{Few-Shot Learning}} by {{Kernel HyperNetworks}}},
  shorttitle = {{{HyperShot}}},
  booktitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Sendera, Marcin and Przewiezlikowski, Marcin and Karanowski, Konrad and Zieba, Maciej and Tabor, Jacek and Spurek, Przemyslaw},
  date = {2023-01},
  pages = {2468--2477},
  publisher = {IEEE},
  location = {Waikoloa, HI, USA},
  doi = {10.1109/WACV56688.2023.00250},
  url = {https://ieeexplore.ieee.org/document/10030925/},
  urldate = {2024-09-20},
  eventtitle = {2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-6654-9346-8},
  langid = {english}
}

@online{SeniorLecturer2024,
  title = {Senior {{Lecturer}}},
  date = {2024-11-05},
  url = {https://smcdonagh.github.io/},
  urldate = {2024-11-05},
  keywords = {Steven McDonagh portfolio}
}

@online{sept.2021TDAPersistentFunctions2025,
  title = {Beyond TDA - Persistent functions and its applications in data sciences, 2021},
  author = {family=2021, given=22 vidéos Dernière modification le 13, prefix=sept., useprefix=true},
  date = {2025-11-12},
  url = {http://www.youtube.com/playlist?list=PL4kY-dS_mSmKCxQ66ZnjMDkdigTT5FHcZ},
  urldate = {2025-11-12},
  abstract = {Conference webpage: http://personal.ntu.edu.sg/xiakelin/TDAconf.html The link for all the slides are available at: https://drive.google.com/drive/folders/1Al...},
  langid = {french},
  organization = {YouTube},
  file = {C:\Users\Aymen\Zotero\storage\HBK5RZF9\playlist.html}
}

@online{shinContinualLearningDeep2017,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  date = {2017-12-11},
  eprint = {1705.08690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.08690},
  url = {http://arxiv.org/abs/1705.08690},
  urldate = {2024-09-20},
  abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
  pubstate = {prepublished},
  keywords = {CL,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Replay paper},
  note = {Comment: NIPS 2017}
}

@online{SignGitLab2025,
  title = {Sign in · {{GitLab}}},
  date = {2025-11-13},
  url = {https://depot.padme-analytics.de/users/sign_in},
  urldate = {2025-11-13},
  abstract = {GitLab Community Edition},
  langid = {english},
  organization = {GitLab},
  file = {C:\Users\Aymen\Zotero\storage\2A6SFYMK\sign_in.html}
}

@online{SignPHTSingle2025,
  title = {Sign in to {{PHT Single Sign-On}}},
  date = {2025-11-13},
  url = {https://auth.padme-analytics.de/auth/realms/pht/protocol/openid-connect/auth?client_id=central-service&redirect_uri=https%3A%2F%2Frequester.padme-analytics.de%2F&state=0d7dbb9b-2a21-4e2b-8f01-f8e9c43b3910&response_mode=fragment&response_type=code&scope=openid&nonce=0285e471-d744-413c-8e25-14912d44bfa4&code_challenge=Klc1O7Mdg7rbGtCoslKxjvnzx5XIXuHte26LmrEJakw&code_challenge_method=S256},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\NHBRWPF3\auth.html}
}

@online{SignPHTSingle2025a,
  title = {Sign in to {{PHT Single Sign-On}}},
  date = {2025-11-13},
  url = {https://auth.padme-analytics.de/auth/realms/pht/protocol/openid-connect/auth?client_id=central-service&redirect_uri=https%3A%2F%2Frequester.padme-analytics.de%2F&state=5335321e-b8f4-4251-830f-8656fe466fa1&response_mode=fragment&response_type=code&scope=openid&nonce=6412bc55-889e-43ae-86b6-3077b5a18b59&code_challenge=dbxNxtueF9gN2LP2D0q05mxRCFdhlpxy6Yy9VidqNTM&code_challenge_method=S256},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\U7ZLDEVY\auth.html}
}

@online{singhHumanDataScaling2024,
  title = {Beyond {{Human Data}}: {{Scaling Self-Training}} for {{Problem-Solving}} with {{Language Models}}},
  shorttitle = {Beyond {{Human Data}}},
  author = {Singh, Avi and Co-Reyes, John D. and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J. and Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi, Aaron and Kumar, Abhishek and Alemi, Alex and Rizkowsky, Alex and Nova, Azade and Adlam, Ben and Bohnet, Bernd and Elsayed, Gamaleldin and Sedghi, Hanie and Mordatch, Igor and Simpson, Isabelle and Gur, Izzeddin and Snoek, Jasper and Pennington, Jeffrey and Hron, Jiri and Kenealy, Kathleen and Swersky, Kevin and Mahajan, Kshiteej and Culp, Laura and Xiao, Lechao and Bileschi, Maxwell L. and Constant, Noah and Novak, Roman and Liu, Rosanne and Warkentin, Tris and Qian, Yundi and Bansal, Yamini and Dyer, Ethan and Neyshabur, Behnam and Sohl-Dickstein, Jascha and Fiedel, Noah},
  date = {2024-04-19},
  eprint = {2312.06585},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.06585},
  url = {http://arxiv.org/abs/2312.06585},
  urldate = {2025-11-13},
  abstract = {Fine-tuning language models\textasciitilde (LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST\$\textasciicircum\{EM\}\$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST\$\textasciicircum\{EM\}\$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Accepted to TMLR. Camera-ready version. First three authors contributed equally},
  file = {C:\Users\Aymen\Zotero\storage\Y64Z5GYD\Singh et al. - 2024 - Beyond Human Data Scaling Self-Training for Problem-Solving with Language Models.pdf}
}

@online{snellLearningGenerateImages2017,
  title = {Learning to {{Generate Images}} with {{Perceptual Similarity Metrics}}},
  author = {Snell, Jake and Ridgeway, Karl and Liao, Renjie and Roads, Brett D. and Mozer, Michael C. and Zemel, Richard S.},
  date = {2017-01-23},
  eprint = {1511.06409},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06409},
  urldate = {2024-09-20},
  abstract = {Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions and reconstructing an input image from a compact representation. Supervised training of image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the mismatch between a generated image and its corresponding target image. We propose instead to use a loss function that is better calibrated to human perceptual judgments of image quality: the multiscale structural-similarity score (MS-SSIM) [34]. Because MS-SSIM is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using MS-SSIM versus PL loss on training deterministic and stochastic autoencoders. For three different architectures, we collected human judgments of the quality of image reconstructions. Observers reliably prefer images synthesized by MS-SSIM-optimized models over those synthesized by PL-optimized models, for two distinct PL measures (L1 and L2 distances). We also explore the effect of training objective on image encoding and analyze conditions under which perceptually-optimized representations yield better performance on image classification. Finally, we demonstrate the superiority of perceptually-optimized networks for super-resolution imaging. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Synthetic images}
}

@article{socherDynamicPoolingUnfolding2011,
  title = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},
  author = {Socher, Richard and Huang, Eric and Pennin, Jeffrey and Manning, Christopher D. and Ng, Andrew},
  date = {2011},
  journaltitle = {Advances in neural information processing systems},
  volume = {24},
  url = {https://proceedings.neurips.cc/paper/2011/hash/3335881e06d4d23091389226225e17c7-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\MXZS9YMM\Socher et al. - 2011 - Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.pdf}
}

@article{sommerConnectingDotsModeConnectedness2024,
  title = {Connecting the {{Dots}}: {{Is Mode-Connectedness}} the {{Key}} to {{Feasible Sample-Based Inference}} in {{Bayesian Neural Networks}}?},
  author = {Sommer, Emanuel and Wimmer, Lisa and Papamarkou, Theodore and Bothmann, Ludwig and Bischl, Bernd and Rügamer, David},
  date = {2024-02-01},
  pages = {arXiv:2402.01484},
  doi = {10.48550/arXiv.2402.01484},
  abstract = {A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a deep ensemble initialized approach as an effective solution with competitive performance and uncertainty quantification.},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning}
}

@online{soroDiffusionBasedNeuralNetwork2024,
  title = {Diffusion-{{Based Neural Network Weights Generation}}},
  author = {Soro, Bedionita and Andreis, Bruno and Lee, Hayeon and Jeong, Wonyong and Chong, Song and Hutter, Frank and Hwang, Sung Ju},
  date = {2024-10-28},
  eprint = {2402.18153},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.18153},
  url = {http://arxiv.org/abs/2402.18153},
  urldate = {2025-11-12},
  abstract = {Transfer learning has gained significant attention in recent deep learning research due to its ability to accelerate convergence and enhance performance on new tasks. However, its success is often contingent on the similarity between source and target data, and training on numerous datasets can be costly, leading to blind selection of pretrained models with limited insight into their effectiveness. To address these challenges, we introduce D2NWG, a diffusion-based neural network weights generation technique that efficiently produces high-performing weights for transfer learning, conditioned on the target dataset. Our method extends generative hyperrepresentation learning to recast the latent diffusion paradigm for neural network weights generation, learning the weight distributions of models pretrained on various datasets. This allows for automatic generation of weights that generalize well across both seen and unseen tasks, outperforming state-of-the-art meta-learning methods and pretrained models. Moreover, our approach is scalable to large architectures such as large language models (LLMs), overcoming the limitations of current parameter generation techniques that rely on task-specific model collections or access to original training data. By modeling the parameter distribution of LLMs, D2NWG enables task-specific parameter generation without requiring additional fine-tuning or large collections of model variants. Extensive experiments show that our method consistently enhances the performance of diverse base models, regardless of their size or complexity, positioning it as a robust solution for scalable transfer learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Weight generation},
  note = {Comment: 14 pages
\par
Comment: 32 pages
\par
Comment: 32 pages},
  file = {C:\Users\Aymen\Zotero\storage\VKKWU7WC\Soro et al. - 2024 - Diffusion-Based Neural Network Weights Generation.pdf}
}

@article{srivastavaImitationGameQuantifying2023,
  title = {Beyond the Imitation Game: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  shorttitle = {Beyond the Imitation Game},
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià},
  date = {2023},
  journaltitle = {Transactions on machine learning research},
  url = {https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\PL7MD3VI\Srivastava et al. - 2023 - Beyond the imitation game Quantifying and extrapolating the capabilities of language models.pdf}
}

@video{stanfordonlineStanfordCS224NNLP2021,
  entrysubtype = {video},
  title = {Stanford {{CS224N NLP}} with {{Deep Learning}} | {{Winter}} 2021 | {{Lecture}} 9 - {{Self- Attention}} and {{Transformers}}},
  editor = {{Stanford Online}},
  editortype = {director},
  date = {2021-10-29},
  url = {https://www.youtube.com/watch?v=ptuGllU5SQQ},
  urldate = {2024-09-20},
  abstract = {For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/3CvTOGY This lecture covers: 1. Impact of Transformers on NLP (and ML more broadly) 2. From Recurrence (RNNs) to Attention-Based NLP Models 3. Understanding the Transformer Model 4. Drawbacks and Variants of Transformers To learn more about this course visit: https://online.stanford.edu/courses/c...  To follow along with the course schedule and syllabus visit: http://web.stanford.edu/class/cs224n/  John Hewitt PhD student in Computer Science at Stanford University Professor Christopher Manning Thomas M. Siebel Professor in Machine Learning, Professor of Linguistics and of Computer Science Director, Stanford Artificial Intelligence Laboratory (SAIL) \#deeplearning  \#naturallanguageprocessing},
  keywords = {Transformer Ressources},
  annotation = {Directors: \_:n1100\\
Directors: \_:n1188}
}

@article{sunEfficiencyFreeIdeal2024,
  title = {Efficiency for {{Free}}: {{Ideal Data Are Transportable Representations}}},
  author = {Sun, Peng and Jiang, Yi and Lin, Tao},
  date = {2024-05-01},
  pages = {arXiv:2405.14669},
  doi = {10.48550/arXiv.2405.14669},
  abstract = {Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution. Existing paradigms tackle the issue of learning efficiency over massive datasets from the perspective of self-supervised learning and dataset distillation independently, while neglecting the untapped potential of accelerating representation learning from an intermediate standpoint. In this work, we delve into defining the ideal data properties from both optimization and generalization perspectives. We propose that model-generated representations, despite being trained on diverse tasks and architectures, converge to a shared linear space, facilitating effective linear transport between models. Furthermore, we demonstrate that these representations exhibit properties conducive to the formation of ideal data. The theoretical/empirical insights therein inspire us to propose a Representation Learning Accelerator (ReLA), which leverages a task- and architecture-agnostic, yet publicly available, free model to form a dynamic data subset and thus accelerate (self-)supervised learning. For instance, employing a CLIP ViT B/16 as a prior model for dynamic data generation, ReLA-aided BYOL can train a ResNet-50 from scratch with 50\% of ImageNet-1K, yielding performance surpassing that of training on the full dataset. Additionally, employing a ResNet-18 pre-trained on CIFAR-10 can enhance ResNet-50 training on 10\% of ImageNet-1K, resulting in a 7.7\% increase in accuracy.},
  keywords = {Computer Science - Artificial,Computer Science - Machine Learning,Intelligence},
  note = {Code: https://github.com/LINs-lab/ReLA}
}

@online{SuperforecastingGoogleSearch2025,
  title = {Superforecasting - {{Google Search}}},
  date = {2025-11-13},
  url = {https://www.google.com/search?gs_ssp=eJzj4tVP1zc0TCq3SDMvqSwyYPQSKC4tSC1Kyy9KTU4sLsnMSwcAtYoLmw&q=superforecasting&oq=superforcasting&gs_lcrp=EgZjaHJvbWUqCQgBEC4YChiABDIGCAAQRRg5MgkIARAuGAoYgAQyCQgCEAAYChiABDIJCAMQABgKGIAEMgkIBBAAGAoYgAQyCQgFEAAYChiABDIJCAYQABgKGIAEMgkIBxAAGAoYgAQyCQgIEAAYChiABDIJCAkQABgKGIAEMgkIChAAGAoYgAQyCQgLEAAYChiABDIJCAwQABgKGIAEMgkIDRAAGAoYgATSAQg5MDExajBqN6gCD7ACAfEF41I2Egbbk4XxBeNSNhIG25OF&client=ms-android-samsung-ss&sourceid=chrome-mobile&ie=UTF-8#ebo=0},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\S9H8TPXI\search.html}
}

@online{SupervisedGromovWassersteinOptimal2025,
  title = {Supervised {{Gromov-Wasserstein Optimal Transport}}},
  date = {2025-11-12},
  url = {https://arxiv.org/html/2401.06266v1},
  urldate = {2025-11-12}
}

@online{sutterNewMITStudy2025,
  title = {A {{New MIT Study Shows Reinforcement Learning Minimizes Catastrophic Forgetting Compared}} to {{Supervised Fine-Tuning}}},
  author = {Sutter, Michal},
  date = {2025-09-08T09:34:12+00:00},
  url = {https://www.marktechpost.com/2025/09/08/a-new-mit-study-shows-reinforcement-learning-minimizes-catastrophic-forgetting-compared-to-supervised-fine-tuning/},
  urldate = {2025-11-13},
  abstract = {reinforcement learning reduces catastrophic forgetting versus supervised fine-tuning, using KL divergence as predictive principle},
  langid = {american},
  organization = {MarkTechPost},
  file = {C:\Users\Aymen\Zotero\storage\IKJ7QLZP\a-new-mit-study-shows-reinforcement-learning-minimizes-catastrophic-forgetting-compared-to-supe.html}
}

@online{TDAToolsGoogle2025,
  title = {{{TDA}} Tools - {{Google~Sheets}}},
  date = {2025-11-13},
  url = {https://docs.google.com/spreadsheets/d/1w7TuNlChfx5lFE-HaDyccWmI-LOSdYc14YH3s4P61DM/edit?gid=0#gid=0},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\46CQPV7V\edit.html}
}

@online{ToolsTechniquesEffective2024,
  title = {Tools and Techniques for Effective Code Documentation},
  date = {2024-07-29},
  url = {https://github.com/resources/articles/tools-and-techniques-for-effective-code-documentation},
  urldate = {2025-11-13},
  abstract = {Learn about code documentation and why it’s essential for delivering quality software.},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\Aymen\Zotero\storage\EIRU8ZDB\tools-and-techniques-for-effective-code-documentation.html}
}

@online{TopoActVisuallyExploring,
  title = {{{TopoAct}}: {{Visually}} Exploring the Shape of Activations in Deep Learning ({{Rathore}} et al., 2021) - {{Google Search}}},
  url = {https://www.google.com/search?client=firefox-b-d&q=TopoAct%3A+Visually+exploring+the+shape+of+activations+in+deep+learning+%28Rathore+et+al.%2C+2021%29},
  urldate = {2025-11-16},
  file = {C:\Users\Aymen\Zotero\storage\LFMTGCEC\search.html}
}

@online{TopologicalDataAnalysis,
  title = {Topological Data Analysis of Decision Boundaries with Application to Model Selection ({{Ramamurthy}} et al., 2019) - {{Google Search}}},
  url = {https://www.google.com/search?client=firefox-b-d&q=Topological+data+analysis+of+decision+boundaries+with+application+to+model+selection+%28Ramamurthy+et+al.%2C+2019%29},
  urldate = {2025-11-16},
  file = {C:\Users\Aymen\Zotero\storage\SZNRZ5JV\search.html}
}

@online{TopologicalDynamicsFunctional,
  title = {Topological {{Dynamics}} of {{Functional Neural Network Graphs During}}...},
  url = {https://openreview.net/forum?id=4YThDIz3v5},
  urldate = {2025-11-16},
  abstract = {This study investigates the topological structures of neural network activation graphs, with a focus on detecting higher-order Betti numbers during reinforcement learning. The paper presents...},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\DJZ9BIC9\forum.html}
}

@video{torontomachinelearningseriestmlsMachineUnlearningAddressing2024,
  entrysubtype = {video},
  title = {Machine {{Unlearning}}: {{Addressing Bias}}, {{Privacy}}, and {{Regulation}} in {{LLMs}} and {{Multimodal Models}}},
  shorttitle = {Machine {{Unlearning}}},
  editor = {{Toronto Machine Learning Series (TMLS)}},
  editortype = {director},
  date = {2024-10-31},
  url = {https://www.youtube.com/watch?v=qrJGtMjA8PU},
  urldate = {2025-11-12},
  abstract = {Speaker: Marija Stanojevic,Lead Applied Machine Learning Scientist, EudAImonia Science Abstract: This talk discusses machine unlearning for large language models (LLMs) and multimodal models (MMs) handling sensitive data. As these AI models gain traction, ensuring adaptable and ethical practices is paramount, especially in domains handling healthcare, finance, and personal information. Here, we explore the intricacies of machine unlearning dynamics and their impact on bias mitigation, data privacy, legal compliance, and model robustness. The talk sheds light on recent advancements and seminal research in machine unlearning. Given the growing prevalence of AI regulations and concerns around test data leaks during massive training, machine unlearning emerges as an essential component for ensuring unbiased, compliant, and well-evaluated AI systems. We discuss techniques for identifying unwanted data within models and for removing it while preserving model performance. Additionally, the talk explores methods for evaluating the success of machine unlearning, guaranteeing that the model forgets the targeted data without compromising its overall behavior and performance on other data. Machine unlearning empowers stakeholders, including customers and data owners, with the ability to withdraw their data and fosters trust in the responsible development and deployment of LLMs and MM models.},
  annotation = {Directors: \_:n1437}
}

@online{TransformersAreGetting2025,
  title = {Transformers {{Are Getting Old}}: {{Variants}} and {{Alternatives Exist}}!},
  shorttitle = {Transformers {{Are Getting Old}}},
  date = {2025-07-07},
  url = {https://huggingface.co/blog/ProCreations/transformers-are-getting-old},
  urldate = {2025-11-13},
  abstract = {A Blog post by Pro Creations on Hugging Face},
  file = {C:\Users\Aymen\Zotero\storage\RTF5V3AU\transformers-are-getting-old.html}
}

@online{tronManifoldLearningFoliations2024,
  title = {Manifold {{Learning}} via {{Foliations}} and {{Knowledge Transfer}}},
  author = {Tron, E. and Fioresi, E.},
  date = {2024-09-11},
  eprint = {2409.07412},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2409.07412},
  urldate = {2024-11-05},
  abstract = {Understanding how real data is distributed in high dimensional spaces is the key to many tasks in machine learning. We want to provide a natural geometric structure on the space of data employing a deep ReLU neural network trained as a classifier. Through the data information matrix (DIM), a variation of the Fisher information matrix, the model will discern a singular foliation structure on the space of data. We show that the singular points of such foliation are contained in a measure zero set, and that a local regular foliation exists almost everywhere. Experiments show that the data is correlated with leaves of such foliation. Moreover we show the potential of our approach for knowledge transfer by analyzing the spectrum of the DIM to measure distances between datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{Tutorialathon2025Spring2025,
  title = {Tutorial-a-thon 2025 Spring},
  date = {2025-11-12},
  url = {http://www.youtube.com/playlist?list=PL4kY-dS_mSmIkOQ4lHtSMyxONiyTFqxMe},
  urldate = {2025-11-12},
  abstract = {These videos were created for the Spring 2025 Tutorial-a-thon hosted by AATRN and WinCompTop https://sites.google.com/view/aatrn-tutorial-a-thon.},
  langid = {french},
  organization = {YouTube},
  file = {C:\Users\Aymen\Zotero\storage\Q29K99A5\playlist.html}
}

@online{universityDiffusionBeatsAutoregressive2025,
  title = {Diffusion {{Beats Autoregressive}} in {{Data-Constrained Settings}}},
  author = {University, Carnegie Mellon, Machine Learning Department},
  date = {2025-09-22T09:11:00-04:00},
  url = {https://blog.ml.cmu.edu/2025/09/22/diffusion-beats-autoregressive-in-data-constrained-settings/},
  urldate = {2025-11-13},
  abstract = {Check out our new blog post on "Diffusion beats Autoregressive in Data-Constrained settings". The era of infinite internet data is ending. This research paper asks: ~What is the right generative modeling objective when data—not compute—is the bottleneck?},
  langid = {american},
  organization = {Machine Learning Blog | ML@CMU | Carnegie Mellon University},
  file = {C:\Users\Aymen\Zotero\storage\4WEJA7UY\diffusion-beats-autoregressive-in-data-constrained-settings.html}
}

@online{unterthinerPredictingNeuralNetwork2021,
  title = {Predicting {{Neural Network Accuracy}} from {{Weights}}},
  author = {Unterthiner, Thomas and Keysers, Daniel and Gelly, Sylvain and Bousquet, Olivier and Tolstikhin, Ilya},
  date = {2021-04-09},
  eprint = {2002.11448},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2002.11448},
  urldate = {2024-11-05},
  abstract = {We show experimentally that the accuracy of a trained neural network can be predicted surprisingly well by looking only at its weights, without evaluating it on input data. We motivate this task and introduce a formal setting for it. Even when using simple statistics of the weights, the predictors are able to rank neural networks by their performance with very high accuracy (R2 score more than 0.98). Furthermore, the predictors are able to rank networks trained on different, unobserved datasets and with different architectures. We release a collection of 120k convolutional neural networks trained on four different datasets to encourage further research in this area, with the goal of understanding network training and performance better.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{usaiDecryptingSpatialRelationship2023,
  title = {Decrypting the Spatial Relationship between Peripheral Chromatin and Nuclear Lamina in {{Hutchinson-Gilford Progeria Syndrome}} Using Super-Resolution Microscopy Techniques},
  author = {Usai, Chantal and Cainero, Isotta and Cuneo, Lisa and Mariangeli, Matteo and Baldini, Francesca and Bianchini, Paolo and Diaspro, Alberto},
  date = {2023-02},
  journaltitle = {Biophysical Journal},
  volume = {122},
  number = {3},
  pages = {491a},
  issn = {00063495},
  doi = {10.1016/j.bpj.2022.11.2625},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000634952203541X},
  urldate = {2024-11-05},
  langid = {english}
}

@online{UsePersistentHomology,
  title = {On the {{Use}} of {{Persistent Homology}} to {{Control}} the {{Generalization Capacity}} of a {{Neural Network}}},
  url = {https://ouci.dntb.gov.ua/en/works/4zn1VeE4/},
  urldate = {2025-11-16},
  file = {C:\Users\Aymen\Zotero\storage\5LQEVAJM\4zn1VeE4.html}
}

@online{UsePersistentHomologya,
  title = {On the {{Use}} of {{Persistent Homology}} to {{Control}} the {{Generalization Capacity}} of a {{Neural Network}}},
  url = {http://ouci.dntb.gov.ua/en/works/4zn1VeE4/},
  urldate = {2025-11-16},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\GCSL95DK\4zn1VeE4.html}
}

@inproceedings{vergariCompositionalAtlasTractable2021,
  title = {A {{Compositional Atlas}} of {{Tractable Circuit Operations}} for {{Probabilistic Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vergari, Antonio and Choi, YooJung and Liu, Anji and Teso, Stefano and Van den Broeck, Guy},
  date = {2021},
  volume = {34},
  pages = {13189--13201},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/6e01383fd96a17ae51cc3e15447e7533-Abstract.html},
  urldate = {2024-11-05},
  abstract = {Circuit representations are becoming the lingua franca to express and reason about tractable generative and discriminative models. In this paper, we show how complex inference scenarios for these models that commonly arise in machine learning---from  computing the expectations of decision tree ensembles to information-theoretic divergences of sum-product networks---can be represented in terms of tractable modular operations over circuits. Specifically, we characterize the tractability of simple transformations---sums, products, quotients, powers, logarithms, and exponentials---in terms of sufficient structural constraints of the circuits they operate on, and present novel hardness results for the cases in which these properties are not satisfied. Building on these operations, we derive a unified framework for reasoning about tractable models that generalizes several results in the literature and opens up novel tractable inference scenarios.}
}

@inreference{ViscositySolution2025,
  title = {Viscosity Solution},
  booktitle = {Wikipedia},
  date = {2025-08-18T09:13:08Z},
  url = {https://en.wikipedia.org/w/index.php?title=Viscosity_solution&oldid=1306542094},
  urldate = {2025-11-12},
  abstract = {In mathematics, the viscosity solution concept was introduced in the early 1980s by Pierre-Louis Lions and Michael G. Crandall as a generalization of the classical concept of what is meant by a 'solution' to a partial differential equation (PDE).  It has been found that the viscosity solution is the natural solution concept to use in many applications of PDE's, including for example first order equations arising in dynamic programming (the Hamilton–Jacobi–Bellman equation), differential games (the Hamilton–Jacobi–Isaacs equation) or front evolution problems, as well as second-order equations such as the ones arising in stochastic optimal control or stochastic differential games. The classical concept was that a PDE                        F         (         x         ,         u         ,         D         u         ,                    D                        2                             u         )         =         0                 \{\textbackslash displaystyle F(x,u,Du,D\textasciicircum\{2\}u)=0\}    over a domain                         x         ∈         Ω                 \{\textbackslash displaystyle x\textbackslash in \textbackslash Omega \}     has a solution if we can find a function u(x) continuous and differentiable over the entire domain such that                         x                 \{\textbackslash displaystyle x\}    ,                         u                 \{\textbackslash displaystyle u\}    ,                         D         u                 \{\textbackslash displaystyle Du\}    ,                                    D                        2                             u                 \{\textbackslash displaystyle D\textasciicircum\{2\}u\}     satisfy the above equation at every point. If a scalar equation is degenerate elliptic (defined below), one can define a type of weak solution called viscosity solution. Under the viscosity solution concept, u does not need to be everywhere differentiable. There may be points where either                         D         u                 \{\textbackslash displaystyle Du\}     or                                    D                        2                             u                 \{\textbackslash displaystyle D\textasciicircum\{2\}u\}     does not exist and yet u satisfies the equation in an appropriate generalized sense. The definition allows only for certain kind of singularities, so that existence, uniqueness, and stability under uniform limits, hold for a large class of equations.},
  langid = {english},
  annotation = {Page Version ID: 1306542094},
  file = {C:\Users\Aymen\Zotero\storage\LZAFG4ZJ\index.html}
}

@software{vivoRMTTheoryAndPracticeRMT2025,
  title = {{{RMT-TheoryAndPractice}}/{{RMT}}},
  author = {Vivo, Marcel Novaes, Pierpaolo, Giacomo Livan},
  date = {2025-11-10T09:59:29Z},
  origdate = {2017-07-17T15:56:27Z},
  url = {https://github.com/RMT-TheoryAndPractice/RMT},
  urldate = {2025-11-13},
  keywords = {random-matrix-theory},
  annotation = {Programmers: \_:n1661}
}

@online{vladymyrovContinualHyperTransformerMetaLearner2024,
  title = {Continual {{HyperTransformer}}: {{A Meta-Learner}} for {{Continual Few-Shot Learning}}},
  shorttitle = {Continual {{HyperTransformer}}},
  author = {Vladymyrov, Max and Zhmoginov, Andrey and Sandler, Mark},
  date = {2024-08-17},
  eprint = {2301.04584},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.04584},
  url = {http://arxiv.org/abs/2301.04584},
  urldate = {2024-09-20},
  abstract = {We focus on the problem of learning without forgetting from multiple tasks arriving sequentially, where each task is defined using a few-shot episode of novel or already seen classes. We approach this problem using the recently published HyperTransformer (HT), a Transformer-based hypernetwork that generates specialized task-specific CNN weights directly from the support set. In order to learn from a continual sequence of tasks, we propose to recursively re-use the generated weights as input to the HT for the next task. This way, the generated CNN weights themselves act as a representation of previously learned tasks, and the HT is trained to update these weights so that the new task can be learned without forgetting past tasks. This approach is different from most continual learning algorithms that typically rely on using replay buffers, weight regularization or task-dependent architectural changes. We demonstrate that our proposed Continual HyperTransformer method equipped with a prototypical loss is capable of learning and retaining knowledge about past tasks for a variety of scenarios, including learning from mini-batches, and task-incremental and class-incremental learning scenarios.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: TMLR
\par
Comment: TMLR}
}

@online{vyasLimitationsNTKUnderstanding2022,
  title = {Limitations of the {{NTK}} for {{Understanding Generalization}} in {{Deep Learning}}},
  author = {Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum},
  date = {2022-06-22},
  eprint = {2206.10012},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.10012},
  url = {http://arxiv.org/abs/2206.10012},
  urldate = {2025-11-13},
  abstract = {The ``Neural Tangent Kernel'' (NTK) (Jacot et al 2018), and its empirical variants have been proposed as a proxy to capture certain behaviors of real neural networks. In this work, we study NTKs through the lens of scaling laws, and demonstrate that they fall short of explaining important aspects of neural network generalization. In particular, we demonstrate realistic settings where finite-width neural networks have significantly better data scaling exponents as compared to their corresponding empirical and infinite NTKs at initialization. This reveals a more fundamental difference between the real networks and NTKs, beyond just a few percentage points of test accuracy. Further, we show that even if the empirical NTK is allowed to be pre-trained on a constant number of samples, the kernel scaling does not catch up to the neural network scaling. Finally, we show that the empirical NTK continues to evolve throughout most of the training, in contrast with prior work which suggests that it stabilizes after a few epochs of training. Altogether, our work establishes concrete limitations of the NTK approach in understanding generalization of real networks on natural datasets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\3PQU9N64\2206.html}
}

@article{wahidUnsupervisedFeatureSelection2022,
  title = {Unsupervised Feature Selection with Robust Data Reconstruction ({{UFS-RDR}}) and Outlier Detection},
  author = {Wahid, Abdul and Khan, Dost Muhammad and Hussain, Ijaz and Khan, Sajjad Ahmad and Khan, Zardad},
  date = {2022-09-01},
  journaltitle = {Expert Systems with Applications},
  volume = {201},
  pages = {117008},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.117008},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417422004262},
  urldate = {2024-09-20},
  abstract = {In unsupervised learning, the traditional feature selection methods are not always efficient and their feature selection performance can be severely affected in the presence of outliers and noise. To address this issue, we propose a novel robust unsupervised feature selection method, called Unsupervised Feature Selection with Robust Data Reconstruction (UFS-RDR), that minimizes the graph regularized weighted data reconstruction error function. For the detection of outliers, the well-known Mahalanobis distance is used and further determine the Huber-type weight function using these Mahalanobis distances. This weight function downweights the clustering observations that have large distance. Our experimental results on both synthetic and real-world datasets indicate that the proposed UFS-RDR approach has good feature selection performance and also outperforms the competitive non-robust unsupervised feature selection methods in the presence of contamination in the unlabeled data.},
  keywords = {Data reconstruction,Mahalanobis distance,Outliers,Robustness,Unsupervised feature selection}
}

@online{wangComprehensiveSurveyContinual2024,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  date = {2024-02-06},
  eprint = {2302.00487},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.00487},
  urldate = {2024-09-16},
  abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {CL,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,survey},
  note = {Comment: The concise version is in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}
}

@online{wangComprehensiveSurveyContinual2024a,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  date = {2024-02-06},
  eprint = {2302.00487},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.00487},
  url = {http://arxiv.org/abs/2302.00487},
  urldate = {2025-11-16},
  abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: The concise version is in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  file = {C:\Users\Aymen\Zotero\storage\LYDPPHCG\Wang et al. - 2024 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf}
}

@online{wangDatasetDistillation2020,
  title = {Dataset {{Distillation}}},
  author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  date = {2020-02-24},
  eprint = {1811.10959},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.10959},
  urldate = {2024-09-16},
  abstract = {Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60, 000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,DD,Statistics - Machine Learning}
}

@online{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  date = {2020-06-14},
  eprint = {2006.04768},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.04768},
  url = {http://arxiv.org/abs/2006.04768},
  urldate = {2025-11-16},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\textasciicircum 2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\textasciicircum 2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\WMV692FX\\Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\XC5EFFS3\\2006.html}
}

@software{wangLywang3081AwesomeContinualLearning2025,
  title = {Lywang3081/{{Awesome-Continual-Learning}}},
  author = {Wang, Liyuan},
  date = {2025-10-30T10:20:07Z},
  origdate = {2023-07-17T06:06:43Z},
  url = {https://github.com/lywang3081/Awesome-Continual-Learning},
  urldate = {2025-11-16},
  abstract = {A paper list of our recent survey on continual learning, and other useful resources in this field.}
}

@inproceedings{watanabeDeepNeuralNetwork2020,
  title = {Deep {{Neural Network Pruning Using Persistent Homology}}},
  booktitle = {2020 {{IEEE Third International Conference}} on {{Artificial Intelligence}} and {{Knowledge Engineering}} ({{AIKE}})},
  author = {Watanabe, Satoru and Yamana, Hayato},
  date = {2020-12},
  pages = {153--156},
  doi = {10.1109/AIKE48582.2020.00030},
  url = {https://ieeexplore.ieee.org/document/9355454},
  urldate = {2025-11-16},
  abstract = {Deep neural networks (DNNs) have improved the performance of artificial intelligence systems in various fields including image analysis, speech recognition, and text classification. However, the consumption of enormous computation resources prevents DNNs from operating on small computers such as edge sensors and handheld devices. Network pruning (NP), which removes parameters from trained DNNs, is one of the prominent methods of reducing the resource consumption of DNNs. In this paper, we propose a novel method of NP, hereafter referred to as PHPM, using persistent homology (PH). PH investigates the inner representation of knowledge in DNNs, and PHPM utilizes the investigation in NP to improve the efficiency of pruning. PHPM prunes DNNs in ascending order of magnitudes of the combinational effects among neurons, which are calculated using the one-dimensional PH, to prevent the deterioration of the accuracy. We compared PHPM with global magnitude pruning method (GMP), which is one of the common baselines to evaluate pruning methods. Evaluation results show that the classification accuracy of DNNs pruned by PHPM outperforms that pruned by GMP.},
  eventtitle = {2020 {{IEEE Third International Conference}} on {{Artificial Intelligence}} and {{Knowledge Engineering}} ({{AIKE}})},
  keywords = {Biological neural networks,deep neural network,Intelligent sensors,Knowledge engineering,network pruning,Neurons,Performance evaluation,persistent homology,Speech recognition,Text categorization,topological data analysis},
  file = {C:\Users\Aymen\Zotero\storage\JBZUI37J\9355454.html}
}

@online{watanabeTopologicalMeasurementDeep2021,
  title = {Topological {{Measurement}} of {{Deep Neural Networks Using Persistent Homology}}},
  author = {Watanabe, Satoru and Yamana, Hayato},
  date = {2021-06-06},
  eprint = {2106.03016},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.03016},
  url = {http://arxiv.org/abs/2106.03016},
  urldate = {2025-11-16},
  abstract = {The inner representation of deep neural networks (DNNs) is indecipherable, which makes it difficult to tune DNN models, control their training process, and interpret their outputs. In this paper, we propose a novel approach to investigate the inner representation of DNNs through topological data analysis (TDA). Persistent homology (PH), one of the outstanding methods in TDA, was employed for investigating the complexities of trained DNNs. We constructed clique complexes on trained DNNs and calculated the one-dimensional PH of DNNs. The PH reveals the combinational effects of multiple neurons in DNNs at different resolutions, which is difficult to be captured without using PH. Evaluations were conducted using fully connected networks (FCNs) and networks combining FCNs and convolutional neural networks (CNNs) trained on the MNIST and CIFAR-10 data sets. Evaluation results demonstrate that the PH of DNNs reflects both the excess of neurons and problem difficulty, making PH one of the prominent methods for investigating the inner representation of DNNs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: 17 pages, 7 figures},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\3BQUMUSE\\Watanabe et Yamana - 2021 - Topological Measurement of Deep Neural Networks Using Persistent Homology.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\GV3CJHGR\\2106.html}
}

@inreference{WaveletTransform2025,
  title = {Wavelet Transform},
  booktitle = {Wikipedia},
  date = {2025-11-11T07:38:44Z},
  url = {https://en.wikipedia.org/w/index.php?title=Wavelet_transform&oldid=1321563760},
  urldate = {2025-11-13},
  abstract = {In mathematics, a wavelet series is a representation of a square-integrable (real- or complex-valued) function by a certain orthonormal series generated by a wavelet. This article provides a formal, mathematical definition of an orthonormal wavelet and of the integral wavelet transform.},
  langid = {english},
  annotation = {Page Version ID: 1321563760},
  file = {C:\Users\Aymen\Zotero\storage\2IQ8VWQP\index.html}
}

@online{WaybackMachine2017,
  title = {Wayback {{Machine}}},
  date = {2017-08-20},
  url = {https://web.archive.org/web/20170820013659/http://www1.maths.leeds.ac.uk/~pmtwc/quivlecs.pdf},
  urldate = {2024-09-20},
  keywords = {Quivers,Representation theory}
}

@software{weiWindmapleAwesomeAutoML2025,
  title = {Windmaple/Awesome-{{AutoML}}},
  author = {Wei, Wei},
  date = {2025-11-15T23:48:21Z},
  origdate = {2018-11-05T06:04:40Z},
  url = {https://github.com/windmaple/awesome-AutoML},
  urldate = {2025-11-16},
  abstract = {Curating a list of AutoML-related research, tools, projects and other resources}
}

@online{WelcomeFLUTEDocumentation2024,
  title = {Welcome to {{FLUTE}} Documentation! — {{FLUTE}} Documentation},
  date = {2024-09-20},
  url = {https://microsoft.github.io/msrflute/},
  urldate = {2024-09-20}
}

@inproceedings{wheelerActivationLandscapesTopological2021,
  title = {Activation {{Landscapes}} as a {{Topological Summary}} of {{Neural Network Performance}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Wheeler, Matthew and Bouza, Jose and Bubenik, Peter},
  date = {2021-12-15},
  eprint = {2110.10136},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {3865--3870},
  doi = {10.1109/BigData52589.2021.9671368},
  url = {http://arxiv.org/abs/2110.10136},
  urldate = {2025-11-16},
  abstract = {We use topological data analysis (TDA) to study how data transforms as it passes through successive layers of a deep neural network (DNN). We compute the persistent homology of the activation data for each layer of the network and summarize this information using persistence landscapes. The resulting feature map provides both an informative visual- ization of the network and a kernel for statistical analysis and machine learning. We observe that the topological complexity often increases with training and that the topological complexity does not decrease with each layer.},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  note = {Comment: 4 pages, 5 figures},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\TUIVIMKW\\Wheeler et al. - 2021 - Activation Landscapes as a Topological Summary of Neural Network Performance.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\WSAXTMPD\\2110.html}
}

@article{whittingtonHowBuildCognitive2022,
  title = {How to Build a Cognitive Map},
  author = {Whittington, James C. R. and McCaffary, David and Bakermans, Jacob J. W. and Behrens, Timothy E. J.},
  date = {2022-10},
  journaltitle = {Nat Neurosci},
  volume = {25},
  number = {10},
  pages = {1257--1272},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01153-y},
  url = {https://www.nature.com/articles/s41593-022-01153-y},
  urldate = {2024-09-20},
  abstract = {Learning and interpreting the structure of the environment is an innate feature of biological systems, and is integral to guiding flexible behaviors for evolutionary viability. The concept of a cognitive map has emerged as one of the leading metaphors for these capacities, and unraveling the learning and neural representation of such a map has become a central focus of neuroscience. In recent years, many models have been developed to explain cellular responses in the hippocampus and other brain areas. Because it can be difficult to see how these models differ, how they relate and what each model can contribute, this Review aims to organize these models into a clear ontology. This ontology reveals parallels between existing empirical results, and implies new approaches to understand hippocampal–cortical interactions and beyond.},
  langid = {english},
  keywords = {Cognitive map,Cognitive neuroscience,Computational neuroscience}
}

@online{whittingtonRelatingTransformersModels2022,
  title = {Relating Transformers to Models and Neural Representations of the Hippocampal Formation},
  author = {Whittington, James C. R. and Warren, Joseph and Behrens, Timothy E. J.},
  date = {2022-03-15},
  eprint = {2112.04035},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2112.04035},
  urldate = {2024-09-20},
  abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,hippocompal formation,Quantitative Biology - Neurons and Cognition,Transformer}
}

@article{wignerCharacteristicVectorsBordered1955,
  title = {Characteristic {{Vectors}} of {{Bordered Matrices With Infinite Dimensions}}},
  author = {Wigner, Eugene P.},
  date = {1955},
  journaltitle = {Annals of Mathematics},
  volume = {62},
  number = {3},
  eprint = {1970079},
  eprinttype = {jstor},
  pages = {548--564},
  publisher = {[Annals of Mathematics, Trustees of Princeton University on Behalf of the Annals of Mathematics, Mathematics Department, Princeton University]},
  issn = {0003-486X},
  doi = {10.2307/1970079},
  url = {https://www.jstor.org/stable/1970079},
  urldate = {2025-11-13}
}

@online{williamsFastWitnessPersistence2025,
  title = {Fast {{Witness Persistence}} for {{MRI Volumes}} via {{Hybrid Landmarking}}},
  author = {Williams, Jorge Leonardo Ruiz},
  date = {2025-10-07},
  eprint = {2510.04553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.04553},
  url = {http://arxiv.org/abs/2510.04553},
  urldate = {2025-11-12},
  abstract = {We introduce a scalable witness-based persistent homology pipeline for full-brain MRI volumes that couples density-aware landmark selection with a GPU-ready witness filtration. Candidates are scored by a hybrid metric that balances geometric coverage against inverse kernel density, yielding landmark sets that shrink mean pairwise distances by 30–60\% over random or density-only baselines while preserving topological features. Benchmarks on BrainWeb, IXI, and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX 4090 GPU, avoiding the combinatorial blow-up of Čech, Vietoris–Rips, and alpha filtrations. The package is distributed on PyPI as whale-tda (installable via pip); source and issues are hosted at https: //github.com/jorgeLRW/whale. The release also exposes a fast preset (mri\_deep\_dive\_fast) for exploratory sweeps, and ships with reproducibility-focused scripts and artifacts for drop-in use in medical imaging workflows.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\JGNMWJID\Williams - 2025 - Fast Witness Persistence for MRI Volumes via Hybrid Landmarking.pdf}
}

@online{williamsFastWitnessPersistence2025a,
  title = {Fast {{Witness Persistence}} for {{MRI Volumes}} via {{Hybrid Landmarking}}},
  author = {Williams, Jorge Leonardo Ruiz},
  date = {2025-10-06},
  eprint = {2510.04553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.04553},
  url = {http://arxiv.org/abs/2510.04553},
  urldate = {2025-11-16},
  abstract = {We introduce a scalable witness-based persistent homology pipeline for full-brain MRI volumes that couples density-aware landmark selection with a GPU-ready witness filtration. Candidates are scored by a hybrid metric that balances geometric coverage against inverse kernel density, yielding landmark sets that shrink mean pairwise distances by 30–60\% over random or density-only baselines while preserving topological features. Benchmarks on BrainWeb, IXI, and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX 4090 GPU, avoiding the combinatorial blow-up of Čech, Vietoris–Rips, and alpha filtrations. The package is distributed on PyPI as whale-tda (installable via pip); source and issues are hosted at https: //github.com/jorgeLRW/whale. The release also exposes a fast preset (mri\_deep\_dive\_fast) for exploratory sweeps, and ships with reproducibility-focused scripts and artifacts for drop-in use in medical imaging workflows.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\Aymen\Zotero\storage\Q8GQIDFQ\Williams - 2025 - Fast Witness Persistence for MRI Volumes via Hybrid Landmarking.pdf}
}

@online{WorkshopTopologyData2025,
  title = {Workshop: Topology of Data in Rome},
  shorttitle = {Workshop},
  date = {2025-11-12},
  url = {http://www.youtube.com/playlist?list=PL4kY-dS_mSmLa8yICDrKnZidASbO9-rmZ},
  urldate = {2025-11-12},
  abstract = {This playlist contains talks given in the Workshop "Topology of Data in Rome" (15--16/09/2022) URL: https://www.mat.uniroma2.it/Eventi/2022/Topoldata/topolda...},
  langid = {french},
  organization = {YouTube},
  file = {C:\Users\Aymen\Zotero\storage\M3IMQ7LS\playlist.html}
}

@article{wuUnderstandingGeneralizationDeep2017,
  title = {Towards {{Understanding Generalization}} of {{Deep Learning}}: {{Perspective}} of {{Loss Landscapes}}},
  author = {Wu, Lei and Zhu, Zhanxing and E, Weinan},
  date = {2017-06-01},
  pages = {arXiv:1706.10239},
  doi = {10.48550/arXiv.1706.10239},
  abstract = {It is widely observed that deep learning models with learned parameters generalize well, even with much more model parameters than the number of training samples. We systematically investigate the underlying reasons why deep neural networks often generalize well, and reveal the difference between the minima (with the same training error) that generalize well and those they don't. We show that it is the characteristics the landscape of the loss function that explains the good generalization capability. For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima. We theoretically justify our findings through analyzing 2-layer neural networks; and show that the low-complexity solutions have a small norm of Hessian matrix with respect to model parameters. For deeper networks, extensive numerical evidence helps to support our arguments.},
  keywords = {Computer Science - Artificial,Computer Science - Machine Learning,Intelligence,Statistics - Machine Learning}
}

@inproceedings{xiaoDisentanglingTrainabilityGeneralization2020,
  title = {Disentangling Trainability and Generalization in Deep Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel},
  date = {2020},
  pages = {10462--10472},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v119/xiao20b.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\67LNFH9C\Xiao et al. - 2020 - Disentangling trainability and generalization in deep neural networks.pdf}
}

@inproceedings{xiaoDynamicalIsometryMean2018,
  title = {Dynamical Isometry and a Mean Field Theory of Cnns: {{How}} to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  shorttitle = {Dynamical Isometry and a Mean Field Theory of Cnns},
  booktitle = {International Conference on Machine Learning},
  author = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  date = {2018},
  pages = {5393--5402},
  publisher = {PMLR},
  url = {http://proceedings.mlr.press/v80/xiao18a},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\LLGTHY3G\Xiao et al. - 2018 - Dynamical isometry and a mean field theory of cnns How to train 10,000-layer vanilla convolutional.pdf}
}

@online{xiongRobustTrustworthyMachine2022,
  title = {Towards a {{Robust}} and {{Trustworthy Machine Learning System Development}}: {{An Engineering Perspective}}},
  shorttitle = {Towards a {{Robust}} and {{Trustworthy Machine Learning System Development}}},
  author = {Xiong, Pulei and Buffett, Scott and Iqbal, Shahrear and Lamontagne, Philippe and Mamun, Mohammad and Molyneaux, Heather},
  date = {2022-02-14},
  eprint = {2101.03042},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2101.03042},
  urldate = {2024-11-05},
  abstract = {While Machine Learning (ML) technologies are widely adopted in many mission critical fields to support intelligent decision-making, concerns remain about system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness from a security engineering perspective, focusing on the problems in system threat analysis, design and evaluation faced in developing practical machine learning applications, in terms of robustness and user trust. Accordingly, we organize the presentation of this survey intended to facilitate the convey of the body of knowledge from this angle. We then describe a metamodel we created that represents the body of knowledge in a standard and visualized way. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process which extends and scales up the classic process. Finally, we propose the future research directions motivated by our findings. Our work differs itself from the existing surveys by (i) exploring the fundamental principles and best practices to support robust and trustworthy ML system development, and (ii) studying the interplay of robustness and user trust in the context of ML systems. We expect this survey provides a big picture for machine learning security practitioners.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{xuMachineUnlearningSurvey2023,
  title = {Machine {{Unlearning}}: {{A Survey}}},
  shorttitle = {Machine {{Unlearning}}},
  author = {Xu, Heng and Zhu, Tianqing and Zhang, Lefeng and Zhou, Wanlei and Yu, Philip S.},
  date = {2023-06-06},
  eprint = {2306.03558},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.03558},
  url = {http://arxiv.org/abs/2306.03558},
  urldate = {2024-09-20},
  abstract = {Machine learning has attracted widespread attention and evolved into an enabling technology for a wide range of highly successful applications, such as intelligent computer vision, speech recognition, medical diagnosis, and more. Yet a special need has arisen where, due to privacy, usability, and/or the right to be forgotten, information about some specific samples needs to be removed from a model, called machine unlearning. This emerging technology has drawn significant interest from both academics and industry due to its innovation and practicality. At the same time, this ambitious problem has led to numerous research efforts aimed at confronting its challenges. To the best of our knowledge, no study has analyzed this complex topic or compared the feasibility of existing unlearning solutions in different kinds of scenarios. Accordingly, with this survey, we aim to capture the key concepts of unlearning techniques. The existing solutions are classified and summarized based on their characteristics within an up-to-date and comprehensive review of each category's advantages and limitations. The survey concludes by highlighting some of the outstanding issues with unlearning techniques, along with some feasible directions for new research opportunities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Knowledge removal,Machine Unlearning}
}

@online{yadavNeuralTangentKernels2024,
  title = {Neural {{Tangent Kernels}}: {{Bridging Neural Networks}} and {{Kernel Methods}}},
  shorttitle = {Neural {{Tangent Kernels}}},
  author = {Yadav, Amit},
  date = {2024-12-31T18:19:04},
  url = {https://medium.com/biased-algorithms/neural-tangent-kernels-bridging-neural-networks-and-kernel-methods-f558b2728268},
  urldate = {2025-11-13},
  abstract = {Before we dive into the technical depths of Neural Tangent Kernels (NTKs), it’s important to set the stage by recognizing a fundamental…},
  langid = {english},
  organization = {Biased-Algorithms},
  file = {C:\Users\Aymen\Zotero\storage\X4BMQC4B\neural-tangent-kernels-bridging-neural-networks-and-kernel-methods-f558b2728268.html}
}

@article{yangMeanFieldResidual2017,
  title = {Mean Field Residual Networks: {{On}} the Edge of Chaos},
  shorttitle = {Mean Field Residual Networks},
  author = {Yang, Ge and Schoenholz, Samuel},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  url = {https://proceedings.neurips.cc/paper/2017/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html},
  urldate = {2025-11-13},
  file = {C:\Users\Aymen\Zotero\storage\RKGVWF93\Yang et Schoenholz - 2017 - Mean field residual networks On the edge of chaos.pdf}
}

@online{yangMeanFieldTheory2019,
  title = {A {{Mean Field Theory}} of {{Batch Normalization}}},
  author = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
  date = {2019-03-07},
  eprint = {1902.08129},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.08129},
  url = {http://arxiv.org/abs/1902.08129},
  urldate = {2025-11-13},
  abstract = {We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Dynamical Systems},
  note = {Comment: To appear in ICLR 2019},
  file = {C:\Users\Aymen\Zotero\storage\2PQJC7RD\Yang et al. - 2019 - A Mean Field Theory of Batch Normalization.pdf}
}

@online{yaoADAHESSIANAdaptiveSecond2021,
  title = {{{ADAHESSIAN}}: {{An Adaptive Second Order Optimizer}} for {{Machine Learning}}},
  shorttitle = {{{ADAHESSIAN}}},
  author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael W.},
  date = {2021-04-29},
  eprint = {2006.00719},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2006.00719},
  url = {http://arxiv.org/abs/2006.00719},
  urldate = {2024-11-05},
  abstract = {We introduce ADAHESSIAN, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the HESSIAN. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and Adam. The main disadvantage of traditional second order methods is their heavier per iteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in ADAHESSIAN, including: (i) a fast Hutchinson based method to approximate the curvature matrix with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that ADAHESSIAN achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of Adam. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that ADAHESSIAN: (i) achieves 1.80\%/1.45\% higher accuracy on ResNets20/32 on Cifar10, and 5.55\% higher accuracy on ImageNet as compared to Adam; (ii) outperforms AdamW for transformers by 0.13/0.33 BLEU score on IWSLT14/WMT14 and 2.7/1.0 PPL on PTB/Wikitext-103; (iii) outperforms AdamW for SqueezeBert by 0.41 points on GLUE; and (iv) achieves 0.032\% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of ADAHESSIAN is comparable to first order methods, and that it exhibits robustness towards its hyperparameters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,pytorch optimizers,Statistics - Machine Learning}
}

@online{yoonFederatedContinualLearning2021,
  title = {Federated {{Continual Learning}} with {{Weighted Inter-client Transfer}}},
  author = {Yoon, Jaehong and Jeong, Wonyong and Lee, Giwoong and Yang, Eunho and Hwang, Sung Ju},
  date = {2021-06-14},
  eprint = {2003.03196},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.03196},
  url = {http://arxiv.org/abs/2003.03196},
  urldate = {2025-11-16},
  abstract = {There has been a surge of interest in continual learning and federated learning, both of which are important in deep neural networks in real-world scenarios. Yet little research has been done regarding the scenario where each client learns on a sequence of tasks from a private local data stream. This problem of federated continual learning poses new challenges to continual learning, such as utilizing knowledge from other clients, while preventing interference from irrelevant knowledge. To resolve these issues, we propose a novel federated continual learning framework, Federated Weighted Inter-client Transfer (FedWeIT), which decomposes the network weights into global federated parameters and sparse task-specific parameters, and each client receives selective knowledge from other clients by taking a weighted combination of their task-specific parameters. FedWeIT minimizes interference between incompatible tasks, and also allows positive knowledge transfer across clients during learning. We validate our FedWeIT against existing federated learning and continual learning methods under varying degrees of task similarity across clients, and our model significantly outperforms them with a large reduction in the communication cost. Code is available at https://github.com/wyjeong/FedWeIT},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICML 2021},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\PYKTRTAC\\Yoon et al. - 2021 - Federated Continual Learning with Weighted Inter-client Transfer.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\J5QDQ6EV\\2003.html}
}

@online{YouSearchedComputational2025,
  title = {You Searched for Computational Topology},
  date = {2025-11-13},
  url = {https://oceanofpdf.com/search/computational topology/},
  urldate = {2025-11-13},
  langid = {american},
  organization = {OceanofPDF},
  file = {C:\Users\Aymen\Zotero\storage\IVP2VT96\computational topology.html}
}

@online{zaheerBigBirdTransformers2021,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  shorttitle = {Big {{Bird}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  date = {2021-01-08},
  eprint = {2007.14062},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.14062},
  url = {http://arxiv.org/abs/2007.14062},
  urldate = {2025-11-16},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\GJAD6M69\\Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\9T8DZYZI\\2007.html}
}

@article{zeinabzeaiterDecipheringChromatinOrganization2023,
  title = {Deciphering the Chromatin Organization and Epigenomics Involved in Adipocyte Differentiation and Hypertrophy by Multimodal Nanoscopy},
  author = {Zeinab Zeaiter, Lama and Baldini, Francesca and Cuneo, Lisa and Diab, Farah and Bianchini, Paolo and Portincasa, Piero and Vergani, Laura and Diaspro, Alberto},
  date = {2023-02},
  journaltitle = {Biophysical Journal},
  volume = {122},
  number = {3},
  pages = {154a},
  issn = {00063495},
  doi = {10.1016/j.bpj.2022.11.994},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0006349522019105},
  urldate = {2024-11-05},
  langid = {english}
}

@online{zhangGraphHyperNetworksNeural2020,
  title = {Graph {{HyperNetworks}} for {{Neural Architecture Search}}},
  author = {Zhang, Chris and Ren, Mengye and Urtasun, Raquel},
  date = {2020-12-18},
  eprint = {1810.05749},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.05749},
  url = {http://arxiv.org/abs/1810.05749},
  urldate = {2024-09-20},
  abstract = {Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast -- they can search nearly 10 times faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Graph,HyperNetworks,Statistics - Machine Learning},
  note = {Comment: ICLR 2019}
}

@article{zhaoBridgingModeConnectivity2020,
  title = {Bridging {{Mode Connectivity}} in {{Loss Landscapes}} and {{Adversarial Robustness}}},
  author = {Zhao, Pu and Chen, Pin-Yu and Das, Payel and Natesan Ramamurthy, Karthikeyan and Lin, Xue},
  date = {2020-04-01},
  pages = {arXiv:2005.00060},
  doi = {10.48550/arXiv.2005.00060},
  abstract = {Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness.},
  keywords = {Computer Science - Computer,Computer Science - Machine Learning,Statistics - Machine Learning,Vision and Pattern Recognition},
  note = {accepted by ICLR 2020}
}

@online{zhaoDatasetCondensationDifferentiable2021,
  title = {Dataset {{Condensation}} with {{Differentiable Siamese Augmentation}}},
  author = {Zhao, Bo and Bilen, Hakan},
  date = {2021-06-10},
  eprint = {2102.08259},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.08259},
  urldate = {2024-09-16},
  abstract = {In many machine learning problems, large-scale datasets have become the de-facto standard to train state-of-the-art deep networks at the price of heavy computation load. In this paper, we focus on condensing large training sets into significantly smaller synthetic sets which can be used to train deep neural networks from scratch with minimum drop in performance. Inspired from the recent training set synthesis methods, we propose Differentiable Siamese Augmentation that enables effective use of data augmentation to synthesize more informative synthetic images and thus achieves better performance when training networks with augmentations. Experiments on multiple image classification benchmarks demonstrate that the proposed method obtains substantial gains over the state-of-the-art, 7\% improvements on CIFAR10 and CIFAR100 datasets. We show with only less than 1\% data that our method achieves 99.6\%, 94.9\%, 88.5\%, 71.5\% relative performance on MNIST, FashionMNIST, SVHN, CIFAR10 respectively. We also explore the use of our method in continual learning and neural architecture search, and show promising results.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,DD,DSA}
}

@online{zhaoDatasetCondensationDistribution2022,
  title = {Dataset {{Condensation}} with {{Distribution Matching}}},
  author = {Zhao, Bo and Bilen, Hakan},
  date = {2022-12-21},
  eprint = {2110.04181},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2110.04181},
  urldate = {2024-09-16},
  abstract = {Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and secondorder derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost1. We also show promising practical benefits of our method in continual learning and neural architecture search.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,DD,DM}
}

@online{zhengTopologicalDetectionTrojaned2021,
  title = {Topological {{Detection}} of {{Trojaned Neural Networks}}},
  author = {Zheng, Songzhu and Zhang, Yikai and Wagner, Hubert and Goswami, Mayank and Chen, Chao},
  date = {2021-06-11},
  eprint = {2106.06469},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.06469},
  url = {http://arxiv.org/abs/2106.06469},
  urldate = {2025-11-16},
  abstract = {Deep neural networks are known to have security issues. One particular threat is the Trojan attack. It occurs when the attackers stealthily manipulate the model's behavior through Trojaned training samples, which can later be exploited. Guided by basic neuroscientific principles we discover subtle -- yet critical -- structural deviation characterizing Trojaned models. In our analysis we use topological tools. They allow us to model high-order dependencies in the networks, robustly compare different networks, and localize structural abnormalities. One interesting observation is that Trojaned models develop short-cuts from input to output layers. Inspired by these observations, we devise a strategy for robust detection of Trojaned models. Compared to standard baselines it displays better performance on multiple benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\8G7IZ5DC\\Zheng et al. - 2021 - Topological Detection of Trojaned Neural Networks.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\JH9MZDXE\\2106.html}
}

@inproceedings{zhmoginovDecentralizedLearningMultiHeaded2023,
  title = {Decentralized {{Learning With Multi-Headed Distillation}}},
  author = {Zhmoginov, Andrey and Sandler, Mark and Miller, Nolan and Kristiansen, Gus and Vladymyrov, Max},
  date = {2023},
  pages = {8053--8063},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhmoginov_Decentralized_Learning_With_Multi-Headed_Distillation_CVPR_2023_paper.html},
  urldate = {2024-09-20},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english}
}

@inproceedings{zhmoginovHyperTransformerModelGeneration2022,
  title = {{{HyperTransformer}}: {{Model Generation}} for {{Supervised}} and {{Semi-Supervised Few-Shot Learning}}},
  shorttitle = {{{HyperTransformer}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Zhmoginov, Andrey and Sandler, Mark and Vladymyrov, Maksym},
  date = {2022-06-28},
  pages = {27075--27098},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/zhmoginov22a.html},
  urldate = {2024-09-20},
  abstract = {In this work we propose a HyperTransformer, a Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity Transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@online{zhouEvaluatingDisentanglementDeep2021,
  title = {Evaluating the {{Disentanglement}} of {{Deep Generative Models}} through {{Manifold Topology}}},
  author = {Zhou, Sharon and Zelikman, Eric and Lu, Fred and Ng, Andrew Y. and Carlsson, Gunnar and Ermon, Stefano},
  date = {2021-03-17},
  eprint = {2006.03680},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2006.03680},
  url = {http://arxiv.org/abs/2006.03680},
  urldate = {2025-11-16},
  abstract = {Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make ourcode publicly available at https://github.com/stanfordmlgroup/disentanglement.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published at ICLR 2021},
  file = {C\:\\Users\\Aymen\\Zotero\\storage\\JIVQR4SF\\Zhou et al. - 2021 - Evaluating the Disentanglement of Deep Generative Models through Manifold Topology.pdf;C\:\\Users\\Aymen\\Zotero\\storage\\VHWBEJP5\\2006.html}
}

@article{zhouVisualizingAnalyzingTopology,
  title = {Visualizing and {{Analyzing}} the {{Topology}} of {{Neuron Activations}} in  {{Deep Adversarial Training}}},
  author = {Zhou, Youjia and Zhou, Yi and Ding, Jie and Wang, Bei},
  abstract = {Deep models are known to be vulnerable to data adversarial attacks, and many adversarial training techniques have been developed to improve their adversarial robustness. While data adversaries attack model predictions through modifying data, little is known about their impact on the neuron activations produced by the model, which play a crucial role in determining the model’s predictions and interpretability. In this work, we aim to develop a topological understanding of adversarial training to enhance its interpretability. We analyze the topological structure—in particular, mapper graphs—of neuron activations of data samples produced by deep adversarial training. Each node of a mapper graph represents a cluster of activations, and two nodes are connected by an edge if their corresponding clusters have a nonempty intersection. We provide an interactive visualization tool that demonstrates the utility of our topological framework in exploring the activation space. We found that stronger attacks make the data samples more indistinguishable in the neuron activation space that leads to a lower accuracy. Our tool also provides a natural way to identify the vulnerable data samples that may be useful in improving model robustness.},
  langid = {english},
  file = {C:\Users\Aymen\Zotero\storage\BMZDQDLT\Zhou et al. - Visualizing and Analyzing the Topology of Neuron Activations in  Deep Adversarial Training.pdf}
}

@online{zirnbauerSymmetryClassesRandom2005,
  title = {Symmetry Classes in Random Matrix Theory},
  author = {Zirnbauer, Martin R.},
  date = {2005-09-17},
  eprint = {math-ph/0404058},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.math-ph/0404058},
  url = {http://arxiv.org/abs/math-ph/0404058},
  urldate = {2025-11-13},
  abstract = {Dyson's (1962) classification of matrix ensembles is reviewed from a modern perspective, and its recent extension to disordered fermion problems is motivated and described. It is explained in particular why symmetry classes are associated with large families of symmetric spaces.},
  pubstate = {prepublished},
  keywords = {Mathematical Physics},
  note = {Comment: Short review article (max. 4000 words) solicited by the editors of the Encylopedia of Mathematical Physics (Elsevier, 2005)},
  file = {C:\Users\Aymen\Zotero\storage\DQ98858Z\0404058.html}
}
