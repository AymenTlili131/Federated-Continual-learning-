{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Data Analysis of CNN Weight Spaces\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs comprehensive topological analysis on CNN weight spaces using persistent homology. We analyze 36,468 trained CNN models from the Merged zoo dataset to understand the geometric and topological structure of neural network weight spaces.\n",
    "\n",
    "### Dataset Information\n",
    "- **Total Models**: 36,468 trained CNNs\n",
    "- **Parameters per Model**: 2,464 (2,416 weights + 48 biases)\n",
    "- **Activation Functions**: 6 types (gelu, relu, silu, tanh, sigmoid, leakyrelu) - 6,078 models each\n",
    "- **Training Epochs**: 6 checkpoints (11, 16, 21, 26, 31, 36)\n",
    "- **MNIST Classes**: 10 digits (0-9), one-hot encoded\n",
    "\n",
    "### Analysis Strategy\n",
    "\n",
    "We employ **two complementary grouping strategies** to understand different aspects of the weight space:\n",
    "\n",
    "#### **Group A: Activation Function Analysis**\n",
    "Groups models by their activation function to understand how different non-linearities shape the weight space topology.\n",
    "- 6 groups: gelu, relu, silu, tanh, sigmoid, leakyrelu\n",
    "- Each group contains 6,078 models\n",
    "- **Goal**: Identify topological signatures unique to each activation function\n",
    "\n",
    "#### **Group B: MNIST Class Analysis**\n",
    "Groups models by which MNIST digit class appears in their training set (using one-hot encoding).\n",
    "- 10 groups: digits 0-9\n",
    "- Variable group sizes depending on label combinations\n",
    "- **Goal**: Detect class-specific topological patterns in weight spaces\n",
    "\n",
    "### Topological Methods\n",
    "\n",
    "1. **Vietoris-Rips Persistence**: Compute H0 (connected components) and H1 (loops) persistence diagrams\n",
    "2. **Distance Metrics**: Bottleneck and Wasserstein distances between persistence diagrams\n",
    "3. **Vectorized Representations**: Persistence landscapes, Betti curves, persistence images\n",
    "4. **Epoch Evolution**: Track topological changes across training epochs\n",
    "5. **Multi-parameter Persistence**: 2D filtration using distance and weight norm\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "- **Memory Management**: Models are subsampled to 100 per group to manage RAM usage\n",
    "- **Dimensionality Reduction**: PCA reduces 2,464-d weight vectors to 20-d before persistence computation\n",
    "- **CPU Optimization**: Parallel processing disabled for gudhi/multipers to avoid thread conflicts\n",
    "- **Figure Quality**: High-DPI (250) figures with large sizes for readability\n",
    "\n",
    "### Output Structure\n",
    "\n",
    "```\n",
    "figures/05_topology/\n",
    "├── group_A_activation/     # Activation function analysis results\n",
    "│   ├── pd_*.png           # Persistence diagrams\n",
    "│   ├── distances_*.png    # Pairwise distance heatmaps\n",
    "│   ├── landscapes_*.png   # Persistence landscapes\n",
    "│   └── *.csv             # Summary statistics\n",
    "└── group_B_class/         # MNIST class analysis results\n",
    "    ├── pd_class_*.png\n",
    "    ├── distances_*.png\n",
    "    └── *.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 - Environment Setup and Imports\n",
    "# =====================================\n",
    "\n",
    "# Set matplotlib for inline display\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import ast\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "# Numerical and scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Topological Data Analysis\n",
    "from gudhi import RipsComplex\n",
    "from gudhi.hera import bottleneck_distance, wasserstein_distance\n",
    "\n",
    "# Try to import persistence visualization, fallback to custom implementation\n",
    "try:\n",
    "    from giotto_tdaviz import PersistenceDiagram\n",
    "    HAS_GIOTTO_VIZ = True\n",
    "except ImportError:\n",
    "    HAS_GIOTTO_VIZ = False\n",
    "    print(\"Note: giotto_tdaviz not available, using custom persistence plotting\")\n",
    "\n",
    "# Try to import vectorized representations, fallback to custom implementation\n",
    "try:\n",
    "    from giotto_tda import PersistenceLandscape, BettiCurve, PersistenceImage\n",
    "    HAS_GIOTTO_TDA = True\n",
    "except ImportError:\n",
    "    HAS_GIOTTO_TDA = False\n",
    "    print(\"Note: giotto_tda not available, persistence landscapes will be skipped\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set CPU threads for memory efficiency\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '4'\n",
    "\n",
    "# Configure matplotlib for high-quality figures\n",
    "plt.rcParams['figure.dpi'] = 250\n",
    "plt.rcParams['savefig.dpi'] = 250\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "# Path configurations\n",
    "ROOT = Path(\"/home/aymen/Documents/GitHub/Federated-Continual-learning-/New\")\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MERGED_ZOO = DATA_DIR / \"Merged zoo.csv\"\n",
    "FIG_DIR = ROOT / \"notebooks_sandbox\" / \"figures\" / \"05_topology\"\n",
    "\n",
    "# Create figure directories\n",
    "FIG_DIR_A = FIG_DIR / \"group_A_activation\"\n",
    "FIG_DIR_B = FIG_DIR / \"group_B_class\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR_A.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR_B.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Analysis parameters\n",
    "RANDOM_SEED = 42\n",
    "N_SUBSAMPLE = 100\n",
    "PCA_DIM = 20\n",
    "MAX_DIM = 1  # Compute H0 and H1\n",
    "\n",
    "# Column definitions\n",
    "ACTIVATION_COLS = [\"gelu\", \"relu\", \"silu\", \"tanh\", \"sigmoid\", \"leakyrelu\"]\n",
    "DIGIT_COLS = [str(i) for i in range(10)]\n",
    "\n",
    "# Epoch list for analysis\n",
    "EPOCHS = [11, 16, 21, 26, 31, 36]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  TOPOLOGICAL DATA ANALYSIS - ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(f\"Working directory: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Figure directory: {FIG_DIR}\")\n",
    "print(f\"\\nAnalysis configuration:\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  Subsample size: {N_SUBSAMPLE} models per group\")\n",
    "print(f\"  PCA dimension: {PCA_DIM}\")\n",
    "print(f\"  Max homology dimension: {MAX_DIM}\")\n",
    "print(f\"  Epochs to analyze: {EPOCHS}\")\n",
    "print(f\"\\nPackage availability:\")\n",
    "print(f\"  giotto_tdaviz: {HAS_GIOTTO_VIZ}\")\n",
    "print(f\"  giotto_tda: {HAS_GIOTTO_TDA}\")\n",
    "print(f\"\\nOutput directories:\")\n",
    "print(f\"  Group A (Activations): {FIG_DIR_A}\")\n",
    "print(f\"  Group B (Classes): {FIG_DIR_B}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Environment configured successfully\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Load the Merged zoo CSV file and prepare it for topological analysis.\n",
    "\n",
    "**Data Validation Steps**:\n",
    "1. Load CSV and verify dimensions\n",
    "2. Extract weight and bias columns (2,464 parameters total)\n",
    "3. Parse activation function labels (one-hot encoded → string)\n",
    "4. Parse MNIST class labels (one-hot encoded)\n",
    "5. Convert epoch and accuracy to numeric types\n",
    "6. Display sample rows for verification\n",
    "\n",
    "**Expected Output**: DataFrame with 36,468 rows × 2,483 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Load and Validate Merged Zoo Data\n",
    "\n",
    "print(\"Loading Merged zoo.csv...\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(MERGED_ZOO)\n",
    "\n",
    "print(\"Dataset Dimensions:\")\n",
    "print(f\"  Rows:    {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns):,}\")\n",
    "\n",
    "# Extract parameter columns\n",
    "weight_cols = [c for c in df.columns if c.startswith(\"weight \")]\n",
    "bias_cols = [c for c in df.columns if c.startswith(\"bias \")]\n",
    "param_cols = weight_cols + bias_cols\n",
    "\n",
    "print(\"\")\n",
    "print(\"Parameter Breakdown:\")\n",
    "print(f\"  Weights: {len(weight_cols)}\")\n",
    "print(f\"  Biases:  {len(bias_cols)}\")\n",
    "print(f\"  Total:   {len(param_cols)}\")\n",
    "\n",
    "# Convert activation and digit columns to numeric (one-hot encoded)\n",
    "for c in ACTIVATION_COLS + DIGIT_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Convert metadata columns\n",
    "df[\"Accuracy\"] = pd.to_numeric(df[\"Accuracy\"], errors=\"coerce\")\n",
    "df[\"epoch\"] = pd.to_numeric(df[\"epoch\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Extract activation function name from one-hot encoding\n",
    "def get_activation(row):\n",
    "    \"\"\"Extract activation function name from one-hot encoded columns.\"\"\"\n",
    "    for act in ACTIVATION_COLS:\n",
    "        if row.get(act, 0) == 1:\n",
    "            return act\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"activation\"] = df.apply(get_activation, axis=1)\n",
    "\n",
    "# Parse label tuples (for reference, though we'll use one-hot encoding for class analysis)\n",
    "def parse_label(s):\n",
    "    \"\"\"Parse label string into sorted tuple of integers.\"\"\"\n",
    "    try:\n",
    "        return tuple(sorted(ast.literal_eval(s)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df[\"label_tuple\"] = df[\"label\"].apply(parse_label)\n",
    "\n",
    "# Data validation summary\n",
    "print(\"\")\n",
    "print(\"Data Validation:\")\n",
    "print(f\"  Unique epochs:  {sorted(df['epoch'].unique())}\")\n",
    "print(f\"  Unique labels:  {df['label_tuple'].nunique()}\")\n",
    "print(\"  Activation distribution:\")\n",
    "for act in ACTIVATION_COLS:\n",
    "    count = (df[\"activation\"] == act).sum()\n",
    "    print(f\"    {act:12s}: {count:5d} models\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"MNIST Class distribution (models containing each digit):\")\n",
    "for digit in DIGIT_COLS:\n",
    "    count = (df[digit] == 1).sum()\n",
    "    print(f\"    Digit {digit}: {count:5d} models\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Data loaded successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Sample Data (first 5 rows):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display sample with key columns\n",
    "display_cols = [\"label\", \"activation\", \"epoch\", \"Accuracy\"] + DIGIT_COLS\n",
    "display(df[display_cols].head())\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Persistence Analysis\n",
    "\n",
    "Define utility functions for topological data analysis with memory-efficient implementations.\n",
    "\n",
    "**Function Catalog**:\n",
    "\n",
    "1. **`subsample_group()`**: Randomly sample models from a group to manage memory\n",
    "2. **`compute_persistence_diagram()`**: Calculate Vietoris-Rips persistence using gudhi\n",
    "3. **`plot_persistence_diagrams()`**: Visualize persistence diagrams with high-quality formatting\n",
    "4. **`persistence_summary()`**: Extract statistical features from persistence diagrams\n",
    "5. **`smart_annotate_heatmap()`**: Conditionally annotate heatmap cells based on spacing\n",
    "\n",
    "**Memory Management**:\n",
    "- Automatic garbage collection after large computations\n",
    "- Figures closed after saving to free memory\n",
    "- PCA used for dimensionality reduction before persistence computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Helper Functions for Topological Analysis\n",
    "\n",
    "def subsample_group(df_group, param_cols, n=100, seed=42):\n",
    "    \"\"\"Randomly sample models from a group for memory efficiency.\"\"\"\n",
    "    if len(df_group) <= n:\n",
    "        return df_group[param_cols].values\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(len(df_group), n, replace=False)\n",
    "        return df_group.iloc[indices][param_cols].values\n",
    "\n",
    "def compute_persistence_diagram(X, max_dim=1, max_edge=1.0):\n",
    "    \"\"\"Compute Vietoris-Rips persistence diagram from point cloud.\"\"\"\n",
    "    if len(X) < 3:\n",
    "        return {0: np.array([]), 1: np.array([])}, None\n",
    "    \n",
    "    rips = RipsComplex(points=X, max_edge_length=max_edge)\n",
    "    simplex_tree = rips.create_simplex_tree(max_dimension=max_dim + 2)\n",
    "    \n",
    "    persistence = simplex_tree.persistence()\n",
    "    \n",
    "    diagrams = {}\n",
    "    for dim in range(max_dim + 1):\n",
    "        dim_pers = [(b, d) for b, d, p_dim in persistence if p_dim == dim and d > b]\n",
    "        if dim_pers:\n",
    "            diagrams[dim] = np.array(dim_pers)\n",
    "        else:\n",
    "            diagrams[dim] = np.array([])\n",
    "    \n",
    "    return diagrams, simplex_tree\n",
    "\n",
    "def plot_persistence_diagrams(diagrams, title=\"Persistence Diagrams\", \n",
    "                             save_path=None, figsize=(16, 7)):\n",
    "    \"\"\"Plot persistence diagrams with high-quality formatting.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(diagrams), figsize=figsize)\n",
    "    if len(diagrams) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = ['#e74c3c', '#3498db']\n",
    "    \n",
    "    for idx, (dim, dgm) in enumerate(diagrams.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if len(dgm) > 0:\n",
    "            ax.scatter(dgm[:, 0], dgm[:, 1], c=colors[dim], s=30, alpha=0.7, \n",
    "                      edgecolors='black', linewidth=0.5)\n",
    "            \n",
    "            # Plot diagonal\n",
    "            max_val = max(np.max(dgm), 1.0)\n",
    "            ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Set limits\n",
    "            ax.set_xlim(0, max_val * 1.1)\n",
    "            ax.set_ylim(0, max_val * 1.1)\n",
    "        else:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "        \n",
    "        ax.set_xlabel(\"Birth\", fontweight='bold')\n",
    "        ax.set_ylabel(\"Death\", fontweight='bold')\n",
    "        ax.set_title(f\"H{dim} Persistence\", fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=250, bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: {save_path.name}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def persistence_summary(diagrams):\n",
    "    \"\"\"Extract summary statistics from persistence diagrams.\"\"\"\n",
    "    summary = {}\n",
    "    \n",
    "    for dim, dgm in diagrams.items():\n",
    "        if len(dgm) > 0:\n",
    "            persistence = dgm[:, 1] - dgm[:, 0]\n",
    "            summary[f'H{dim}_n_features'] = len(dgm)\n",
    "            summary[f'H{dim}_mean_persistence'] = np.mean(persistence)\n",
    "            summary[f'H{dim}_std_persistence'] = np.std(persistence)\n",
    "            summary[f'H{dim}_max_persistence'] = np.max(persistence)\n",
    "            summary[f'H{dim}_total_persistence'] = np.sum(persistence)\n",
    "        else:\n",
    "            summary[f'H{dim}_n_features'] = 0\n",
    "            summary[f'H{dim}_mean_persistence'] = 0\n",
    "            summary[f'H{dim}_std_persistence'] = 0\n",
    "            summary[f'H{dim}_max_persistence'] = 0\n",
    "            summary[f'H{dim}_total_persistence'] = 0\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def smart_annotate_heatmap(data, ax, fmt=\".3f\", fontsize=9, threshold=0.1):\n",
    "    \"\"\"Conditionally annotate heatmap cells based on spacing.\"\"\"\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    if n <= 4:\n",
    "        # Small matrix - annotate all cells\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j and not np.isnan(data[i, j]):\n",
    "                    ax.text(j + 0.5, i + 0.5, fmt.format(data[i, j]),\n",
    "                           ha='center', va='center', fontsize=fontsize,\n",
    "                           fontweight='bold', color='white' if data[i, j] > threshold else 'black')\n",
    "    elif n <= 6:\n",
    "        # Medium matrix - annotate significant values\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j and not np.isnan(data[i, j]) and data[i, j] > np.nanpercentile(data, 75):\n",
    "                    ax.text(j + 0.5, i + 0.5, fmt.format(data[i, j]),\n",
    "                           ha='center', va='center', fontsize=fontsize,\n",
    "                           fontweight='bold')\n",
    "    # Large matrix - no annotations for readability\n",
    "\n",
    "print(\"✓ Helper functions defined successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
